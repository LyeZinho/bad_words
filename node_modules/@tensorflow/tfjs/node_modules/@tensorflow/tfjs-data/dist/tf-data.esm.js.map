{"version":3,"file":"tf-data.esm.js","sources":["../node_modules/seedrandom/lib/alea.js","../node_modules/seedrandom/lib/xor128.js","../node_modules/seedrandom/lib/xorwow.js","../node_modules/seedrandom/lib/xorshift7.js","../node_modules/seedrandom/lib/xor4096.js","../node_modules/seedrandom/lib/tychei.js","../node_modules/seedrandom/seedrandom.js","../node_modules/seedrandom/index.js","../src/util/deep_map.ts","../src/util/ring_buffer.ts","../src/util/growing_ring_buffer.ts","../src/iterators/lazy_iterator.ts","../src/dataset.ts","../src/datasets/text_line_dataset.ts","../src/datasets/csv_dataset.ts","../src/datasource.ts","../src/iterators/string_iterator.ts","../src/iterators/byte_chunk_iterator.ts","../src/iterators/file_chunk_iterator.ts","../src/iterators/url_chunk_iterator.ts","../src/util/source_util.ts","../src/sources/file_data_source.ts","../src/sources/url_data_source.ts","../src/readers.ts","../src/version.ts"],"sourcesContent":["// A port of an algorithm by Johannes Baagøe <baagoe@baagoe.com>, 2010\n// http://baagoe.com/en/RandomMusings/javascript/\n// https://github.com/nquinlan/better-random-numbers-for-javascript-mirror\n// Original work is under MIT license -\n\n// Copyright (C) 2010 by Johannes Baagøe <baagoe@baagoe.org>\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n// \n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n// \n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\n\n\n(function(global, module, define) {\n\nfunction Alea(seed) {\n  var me = this, mash = Mash();\n\n  me.next = function() {\n    var t = 2091639 * me.s0 + me.c * 2.3283064365386963e-10; // 2^-32\n    me.s0 = me.s1;\n    me.s1 = me.s2;\n    return me.s2 = t - (me.c = t | 0);\n  };\n\n  // Apply the seeding algorithm from Baagoe.\n  me.c = 1;\n  me.s0 = mash(' ');\n  me.s1 = mash(' ');\n  me.s2 = mash(' ');\n  me.s0 -= mash(seed);\n  if (me.s0 < 0) { me.s0 += 1; }\n  me.s1 -= mash(seed);\n  if (me.s1 < 0) { me.s1 += 1; }\n  me.s2 -= mash(seed);\n  if (me.s2 < 0) { me.s2 += 1; }\n  mash = null;\n}\n\nfunction copy(f, t) {\n  t.c = f.c;\n  t.s0 = f.s0;\n  t.s1 = f.s1;\n  t.s2 = f.s2;\n  return t;\n}\n\nfunction impl(seed, opts) {\n  var xg = new Alea(seed),\n      state = opts && opts.state,\n      prng = xg.next;\n  prng.int32 = function() { return (xg.next() * 0x100000000) | 0; }\n  prng.double = function() {\n    return prng() + (prng() * 0x200000 | 0) * 1.1102230246251565e-16; // 2^-53\n  };\n  prng.quick = prng;\n  if (state) {\n    if (typeof(state) == 'object') copy(state, xg);\n    prng.state = function() { return copy(xg, {}); }\n  }\n  return prng;\n}\n\nfunction Mash() {\n  var n = 0xefc8249d;\n\n  var mash = function(data) {\n    data = data.toString();\n    for (var i = 0; i < data.length; i++) {\n      n += data.charCodeAt(i);\n      var h = 0.02519603282416938 * n;\n      n = h >>> 0;\n      h -= n;\n      h *= n;\n      n = h >>> 0;\n      h -= n;\n      n += h * 0x100000000; // 2^32\n    }\n    return (n >>> 0) * 2.3283064365386963e-10; // 2^-32\n  };\n\n  return mash;\n}\n\n\nif (module && module.exports) {\n  module.exports = impl;\n} else if (define && define.amd) {\n  define(function() { return impl; });\n} else {\n  this.alea = impl;\n}\n\n})(\n  this,\n  (typeof module) == 'object' && module,    // present in node.js\n  (typeof define) == 'function' && define   // present with an AMD loader\n);\n\n\n","// A Javascript implementaion of the \"xor128\" prng algorithm by\n// George Marsaglia.  See http://www.jstatsoft.org/v08/i14/paper\n\n(function(global, module, define) {\n\nfunction XorGen(seed) {\n  var me = this, strseed = '';\n\n  me.x = 0;\n  me.y = 0;\n  me.z = 0;\n  me.w = 0;\n\n  // Set up generator function.\n  me.next = function() {\n    var t = me.x ^ (me.x << 11);\n    me.x = me.y;\n    me.y = me.z;\n    me.z = me.w;\n    return me.w ^= (me.w >>> 19) ^ t ^ (t >>> 8);\n  };\n\n  if (seed === (seed | 0)) {\n    // Integer seed.\n    me.x = seed;\n  } else {\n    // String seed.\n    strseed += seed;\n  }\n\n  // Mix in string seed, then discard an initial batch of 64 values.\n  for (var k = 0; k < strseed.length + 64; k++) {\n    me.x ^= strseed.charCodeAt(k) | 0;\n    me.next();\n  }\n}\n\nfunction copy(f, t) {\n  t.x = f.x;\n  t.y = f.y;\n  t.z = f.z;\n  t.w = f.w;\n  return t;\n}\n\nfunction impl(seed, opts) {\n  var xg = new XorGen(seed),\n      state = opts && opts.state,\n      prng = function() { return (xg.next() >>> 0) / 0x100000000; };\n  prng.double = function() {\n    do {\n      var top = xg.next() >>> 11,\n          bot = (xg.next() >>> 0) / 0x100000000,\n          result = (top + bot) / (1 << 21);\n    } while (result === 0);\n    return result;\n  };\n  prng.int32 = xg.next;\n  prng.quick = prng;\n  if (state) {\n    if (typeof(state) == 'object') copy(state, xg);\n    prng.state = function() { return copy(xg, {}); }\n  }\n  return prng;\n}\n\nif (module && module.exports) {\n  module.exports = impl;\n} else if (define && define.amd) {\n  define(function() { return impl; });\n} else {\n  this.xor128 = impl;\n}\n\n})(\n  this,\n  (typeof module) == 'object' && module,    // present in node.js\n  (typeof define) == 'function' && define   // present with an AMD loader\n);\n\n\n","// A Javascript implementaion of the \"xorwow\" prng algorithm by\n// George Marsaglia.  See http://www.jstatsoft.org/v08/i14/paper\n\n(function(global, module, define) {\n\nfunction XorGen(seed) {\n  var me = this, strseed = '';\n\n  // Set up generator function.\n  me.next = function() {\n    var t = (me.x ^ (me.x >>> 2));\n    me.x = me.y; me.y = me.z; me.z = me.w; me.w = me.v;\n    return (me.d = (me.d + 362437 | 0)) +\n       (me.v = (me.v ^ (me.v << 4)) ^ (t ^ (t << 1))) | 0;\n  };\n\n  me.x = 0;\n  me.y = 0;\n  me.z = 0;\n  me.w = 0;\n  me.v = 0;\n\n  if (seed === (seed | 0)) {\n    // Integer seed.\n    me.x = seed;\n  } else {\n    // String seed.\n    strseed += seed;\n  }\n\n  // Mix in string seed, then discard an initial batch of 64 values.\n  for (var k = 0; k < strseed.length + 64; k++) {\n    me.x ^= strseed.charCodeAt(k) | 0;\n    if (k == strseed.length) {\n      me.d = me.x << 10 ^ me.x >>> 4;\n    }\n    me.next();\n  }\n}\n\nfunction copy(f, t) {\n  t.x = f.x;\n  t.y = f.y;\n  t.z = f.z;\n  t.w = f.w;\n  t.v = f.v;\n  t.d = f.d;\n  return t;\n}\n\nfunction impl(seed, opts) {\n  var xg = new XorGen(seed),\n      state = opts && opts.state,\n      prng = function() { return (xg.next() >>> 0) / 0x100000000; };\n  prng.double = function() {\n    do {\n      var top = xg.next() >>> 11,\n          bot = (xg.next() >>> 0) / 0x100000000,\n          result = (top + bot) / (1 << 21);\n    } while (result === 0);\n    return result;\n  };\n  prng.int32 = xg.next;\n  prng.quick = prng;\n  if (state) {\n    if (typeof(state) == 'object') copy(state, xg);\n    prng.state = function() { return copy(xg, {}); }\n  }\n  return prng;\n}\n\nif (module && module.exports) {\n  module.exports = impl;\n} else if (define && define.amd) {\n  define(function() { return impl; });\n} else {\n  this.xorwow = impl;\n}\n\n})(\n  this,\n  (typeof module) == 'object' && module,    // present in node.js\n  (typeof define) == 'function' && define   // present with an AMD loader\n);\n\n\n","// A Javascript implementaion of the \"xorshift7\" algorithm by\n// François Panneton and Pierre L'ecuyer:\n// \"On the Xorgshift Random Number Generators\"\n// http://saluc.engr.uconn.edu/refs/crypto/rng/panneton05onthexorshift.pdf\n\n(function(global, module, define) {\n\nfunction XorGen(seed) {\n  var me = this;\n\n  // Set up generator function.\n  me.next = function() {\n    // Update xor generator.\n    var X = me.x, i = me.i, t, v, w;\n    t = X[i]; t ^= (t >>> 7); v = t ^ (t << 24);\n    t = X[(i + 1) & 7]; v ^= t ^ (t >>> 10);\n    t = X[(i + 3) & 7]; v ^= t ^ (t >>> 3);\n    t = X[(i + 4) & 7]; v ^= t ^ (t << 7);\n    t = X[(i + 7) & 7]; t = t ^ (t << 13); v ^= t ^ (t << 9);\n    X[i] = v;\n    me.i = (i + 1) & 7;\n    return v;\n  };\n\n  function init(me, seed) {\n    var j, w, X = [];\n\n    if (seed === (seed | 0)) {\n      // Seed state array using a 32-bit integer.\n      w = X[0] = seed;\n    } else {\n      // Seed state using a string.\n      seed = '' + seed;\n      for (j = 0; j < seed.length; ++j) {\n        X[j & 7] = (X[j & 7] << 15) ^\n            (seed.charCodeAt(j) + X[(j + 1) & 7] << 13);\n      }\n    }\n    // Enforce an array length of 8, not all zeroes.\n    while (X.length < 8) X.push(0);\n    for (j = 0; j < 8 && X[j] === 0; ++j);\n    if (j == 8) w = X[7] = -1; else w = X[j];\n\n    me.x = X;\n    me.i = 0;\n\n    // Discard an initial 256 values.\n    for (j = 256; j > 0; --j) {\n      me.next();\n    }\n  }\n\n  init(me, seed);\n}\n\nfunction copy(f, t) {\n  t.x = f.x.slice();\n  t.i = f.i;\n  return t;\n}\n\nfunction impl(seed, opts) {\n  if (seed == null) seed = +(new Date);\n  var xg = new XorGen(seed),\n      state = opts && opts.state,\n      prng = function() { return (xg.next() >>> 0) / 0x100000000; };\n  prng.double = function() {\n    do {\n      var top = xg.next() >>> 11,\n          bot = (xg.next() >>> 0) / 0x100000000,\n          result = (top + bot) / (1 << 21);\n    } while (result === 0);\n    return result;\n  };\n  prng.int32 = xg.next;\n  prng.quick = prng;\n  if (state) {\n    if (state.x) copy(state, xg);\n    prng.state = function() { return copy(xg, {}); }\n  }\n  return prng;\n}\n\nif (module && module.exports) {\n  module.exports = impl;\n} else if (define && define.amd) {\n  define(function() { return impl; });\n} else {\n  this.xorshift7 = impl;\n}\n\n})(\n  this,\n  (typeof module) == 'object' && module,    // present in node.js\n  (typeof define) == 'function' && define   // present with an AMD loader\n);\n\n","// A Javascript implementaion of Richard Brent's Xorgens xor4096 algorithm.\n//\n// This fast non-cryptographic random number generator is designed for\n// use in Monte-Carlo algorithms. It combines a long-period xorshift\n// generator with a Weyl generator, and it passes all common batteries\n// of stasticial tests for randomness while consuming only a few nanoseconds\n// for each prng generated.  For background on the generator, see Brent's\n// paper: \"Some long-period random number generators using shifts and xors.\"\n// http://arxiv.org/pdf/1004.3115v1.pdf\n//\n// Usage:\n//\n// var xor4096 = require('xor4096');\n// random = xor4096(1);                        // Seed with int32 or string.\n// assert.equal(random(), 0.1520436450538547); // (0, 1) range, 53 bits.\n// assert.equal(random.int32(), 1806534897);   // signed int32, 32 bits.\n//\n// For nonzero numeric keys, this impelementation provides a sequence\n// identical to that by Brent's xorgens 3 implementaion in C.  This\n// implementation also provides for initalizing the generator with\n// string seeds, or for saving and restoring the state of the generator.\n//\n// On Chrome, this prng benchmarks about 2.1 times slower than\n// Javascript's built-in Math.random().\n\n(function(global, module, define) {\n\nfunction XorGen(seed) {\n  var me = this;\n\n  // Set up generator function.\n  me.next = function() {\n    var w = me.w,\n        X = me.X, i = me.i, t, v;\n    // Update Weyl generator.\n    me.w = w = (w + 0x61c88647) | 0;\n    // Update xor generator.\n    v = X[(i + 34) & 127];\n    t = X[i = ((i + 1) & 127)];\n    v ^= v << 13;\n    t ^= t << 17;\n    v ^= v >>> 15;\n    t ^= t >>> 12;\n    // Update Xor generator array state.\n    v = X[i] = v ^ t;\n    me.i = i;\n    // Result is the combination.\n    return (v + (w ^ (w >>> 16))) | 0;\n  };\n\n  function init(me, seed) {\n    var t, v, i, j, w, X = [], limit = 128;\n    if (seed === (seed | 0)) {\n      // Numeric seeds initialize v, which is used to generates X.\n      v = seed;\n      seed = null;\n    } else {\n      // String seeds are mixed into v and X one character at a time.\n      seed = seed + '\\0';\n      v = 0;\n      limit = Math.max(limit, seed.length);\n    }\n    // Initialize circular array and weyl value.\n    for (i = 0, j = -32; j < limit; ++j) {\n      // Put the unicode characters into the array, and shuffle them.\n      if (seed) v ^= seed.charCodeAt((j + 32) % seed.length);\n      // After 32 shuffles, take v as the starting w value.\n      if (j === 0) w = v;\n      v ^= v << 10;\n      v ^= v >>> 15;\n      v ^= v << 4;\n      v ^= v >>> 13;\n      if (j >= 0) {\n        w = (w + 0x61c88647) | 0;     // Weyl.\n        t = (X[j & 127] ^= (v + w));  // Combine xor and weyl to init array.\n        i = (0 == t) ? i + 1 : 0;     // Count zeroes.\n      }\n    }\n    // We have detected all zeroes; make the key nonzero.\n    if (i >= 128) {\n      X[(seed && seed.length || 0) & 127] = -1;\n    }\n    // Run the generator 512 times to further mix the state before using it.\n    // Factoring this as a function slows the main generator, so it is just\n    // unrolled here.  The weyl generator is not advanced while warming up.\n    i = 127;\n    for (j = 4 * 128; j > 0; --j) {\n      v = X[(i + 34) & 127];\n      t = X[i = ((i + 1) & 127)];\n      v ^= v << 13;\n      t ^= t << 17;\n      v ^= v >>> 15;\n      t ^= t >>> 12;\n      X[i] = v ^ t;\n    }\n    // Storing state as object members is faster than using closure variables.\n    me.w = w;\n    me.X = X;\n    me.i = i;\n  }\n\n  init(me, seed);\n}\n\nfunction copy(f, t) {\n  t.i = f.i;\n  t.w = f.w;\n  t.X = f.X.slice();\n  return t;\n};\n\nfunction impl(seed, opts) {\n  if (seed == null) seed = +(new Date);\n  var xg = new XorGen(seed),\n      state = opts && opts.state,\n      prng = function() { return (xg.next() >>> 0) / 0x100000000; };\n  prng.double = function() {\n    do {\n      var top = xg.next() >>> 11,\n          bot = (xg.next() >>> 0) / 0x100000000,\n          result = (top + bot) / (1 << 21);\n    } while (result === 0);\n    return result;\n  };\n  prng.int32 = xg.next;\n  prng.quick = prng;\n  if (state) {\n    if (state.X) copy(state, xg);\n    prng.state = function() { return copy(xg, {}); }\n  }\n  return prng;\n}\n\nif (module && module.exports) {\n  module.exports = impl;\n} else if (define && define.amd) {\n  define(function() { return impl; });\n} else {\n  this.xor4096 = impl;\n}\n\n})(\n  this,                                     // window object or global\n  (typeof module) == 'object' && module,    // present in node.js\n  (typeof define) == 'function' && define   // present with an AMD loader\n);\n","// A Javascript implementaion of the \"Tyche-i\" prng algorithm by\n// Samuel Neves and Filipe Araujo.\n// See https://eden.dei.uc.pt/~sneves/pubs/2011-snfa2.pdf\n\n(function(global, module, define) {\n\nfunction XorGen(seed) {\n  var me = this, strseed = '';\n\n  // Set up generator function.\n  me.next = function() {\n    var b = me.b, c = me.c, d = me.d, a = me.a;\n    b = (b << 25) ^ (b >>> 7) ^ c;\n    c = (c - d) | 0;\n    d = (d << 24) ^ (d >>> 8) ^ a;\n    a = (a - b) | 0;\n    me.b = b = (b << 20) ^ (b >>> 12) ^ c;\n    me.c = c = (c - d) | 0;\n    me.d = (d << 16) ^ (c >>> 16) ^ a;\n    return me.a = (a - b) | 0;\n  };\n\n  /* The following is non-inverted tyche, which has better internal\n   * bit diffusion, but which is about 25% slower than tyche-i in JS.\n  me.next = function() {\n    var a = me.a, b = me.b, c = me.c, d = me.d;\n    a = (me.a + me.b | 0) >>> 0;\n    d = me.d ^ a; d = d << 16 ^ d >>> 16;\n    c = me.c + d | 0;\n    b = me.b ^ c; b = b << 12 ^ d >>> 20;\n    me.a = a = a + b | 0;\n    d = d ^ a; me.d = d = d << 8 ^ d >>> 24;\n    me.c = c = c + d | 0;\n    b = b ^ c;\n    return me.b = (b << 7 ^ b >>> 25);\n  }\n  */\n\n  me.a = 0;\n  me.b = 0;\n  me.c = 2654435769 | 0;\n  me.d = 1367130551;\n\n  if (seed === Math.floor(seed)) {\n    // Integer seed.\n    me.a = (seed / 0x100000000) | 0;\n    me.b = seed | 0;\n  } else {\n    // String seed.\n    strseed += seed;\n  }\n\n  // Mix in string seed, then discard an initial batch of 64 values.\n  for (var k = 0; k < strseed.length + 20; k++) {\n    me.b ^= strseed.charCodeAt(k) | 0;\n    me.next();\n  }\n}\n\nfunction copy(f, t) {\n  t.a = f.a;\n  t.b = f.b;\n  t.c = f.c;\n  t.d = f.d;\n  return t;\n};\n\nfunction impl(seed, opts) {\n  var xg = new XorGen(seed),\n      state = opts && opts.state,\n      prng = function() { return (xg.next() >>> 0) / 0x100000000; };\n  prng.double = function() {\n    do {\n      var top = xg.next() >>> 11,\n          bot = (xg.next() >>> 0) / 0x100000000,\n          result = (top + bot) / (1 << 21);\n    } while (result === 0);\n    return result;\n  };\n  prng.int32 = xg.next;\n  prng.quick = prng;\n  if (state) {\n    if (typeof(state) == 'object') copy(state, xg);\n    prng.state = function() { return copy(xg, {}); }\n  }\n  return prng;\n}\n\nif (module && module.exports) {\n  module.exports = impl;\n} else if (define && define.amd) {\n  define(function() { return impl; });\n} else {\n  this.tychei = impl;\n}\n\n})(\n  this,\n  (typeof module) == 'object' && module,    // present in node.js\n  (typeof define) == 'function' && define   // present with an AMD loader\n);\n\n\n","/*\nCopyright 2014 David Bau.\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n*/\n\n(function (pool, math) {\n//\n// The following constants are related to IEEE 754 limits.\n//\nvar global = this,\n    width = 256,        // each RC4 output is 0 <= x < 256\n    chunks = 6,         // at least six RC4 outputs for each double\n    digits = 52,        // there are 52 significant digits in a double\n    rngname = 'random', // rngname: name for Math.random and Math.seedrandom\n    startdenom = math.pow(width, chunks),\n    significance = math.pow(2, digits),\n    overflow = significance * 2,\n    mask = width - 1,\n    nodecrypto;         // node.js crypto module, initialized at the bottom.\n\n//\n// seedrandom()\n// This is the seedrandom function described above.\n//\nfunction seedrandom(seed, options, callback) {\n  var key = [];\n  options = (options == true) ? { entropy: true } : (options || {});\n\n  // Flatten the seed string or build one from local entropy if needed.\n  var shortseed = mixkey(flatten(\n    options.entropy ? [seed, tostring(pool)] :\n    (seed == null) ? autoseed() : seed, 3), key);\n\n  // Use the seed to initialize an ARC4 generator.\n  var arc4 = new ARC4(key);\n\n  // This function returns a random double in [0, 1) that contains\n  // randomness in every bit of the mantissa of the IEEE 754 value.\n  var prng = function() {\n    var n = arc4.g(chunks),             // Start with a numerator n < 2 ^ 48\n        d = startdenom,                 //   and denominator d = 2 ^ 48.\n        x = 0;                          //   and no 'extra last byte'.\n    while (n < significance) {          // Fill up all significant digits by\n      n = (n + x) * width;              //   shifting numerator and\n      d *= width;                       //   denominator and generating a\n      x = arc4.g(1);                    //   new least-significant-byte.\n    }\n    while (n >= overflow) {             // To avoid rounding up, before adding\n      n /= 2;                           //   last byte, shift everything\n      d /= 2;                           //   right using integer math until\n      x >>>= 1;                         //   we have exactly the desired bits.\n    }\n    return (n + x) / d;                 // Form the number within [0, 1).\n  };\n\n  prng.int32 = function() { return arc4.g(4) | 0; }\n  prng.quick = function() { return arc4.g(4) / 0x100000000; }\n  prng.double = prng;\n\n  // Mix the randomness into accumulated entropy.\n  mixkey(tostring(arc4.S), pool);\n\n  // Calling convention: what to return as a function of prng, seed, is_math.\n  return (options.pass || callback ||\n      function(prng, seed, is_math_call, state) {\n        if (state) {\n          // Load the arc4 state from the given state if it has an S array.\n          if (state.S) { copy(state, arc4); }\n          // Only provide the .state method if requested via options.state.\n          prng.state = function() { return copy(arc4, {}); }\n        }\n\n        // If called as a method of Math (Math.seedrandom()), mutate\n        // Math.random because that is how seedrandom.js has worked since v1.0.\n        if (is_math_call) { math[rngname] = prng; return seed; }\n\n        // Otherwise, it is a newer calling convention, so return the\n        // prng directly.\n        else return prng;\n      })(\n  prng,\n  shortseed,\n  'global' in options ? options.global : (this == math),\n  options.state);\n}\nmath['seed' + rngname] = seedrandom;\n\n//\n// ARC4\n//\n// An ARC4 implementation.  The constructor takes a key in the form of\n// an array of at most (width) integers that should be 0 <= x < (width).\n//\n// The g(count) method returns a pseudorandom integer that concatenates\n// the next (count) outputs from ARC4.  Its return value is a number x\n// that is in the range 0 <= x < (width ^ count).\n//\nfunction ARC4(key) {\n  var t, keylen = key.length,\n      me = this, i = 0, j = me.i = me.j = 0, s = me.S = [];\n\n  // The empty key [] is treated as [0].\n  if (!keylen) { key = [keylen++]; }\n\n  // Set up S using the standard key scheduling algorithm.\n  while (i < width) {\n    s[i] = i++;\n  }\n  for (i = 0; i < width; i++) {\n    s[i] = s[j = mask & (j + key[i % keylen] + (t = s[i]))];\n    s[j] = t;\n  }\n\n  // The \"g\" method returns the next (count) outputs as one number.\n  (me.g = function(count) {\n    // Using instance members instead of closure state nearly doubles speed.\n    var t, r = 0,\n        i = me.i, j = me.j, s = me.S;\n    while (count--) {\n      t = s[i = mask & (i + 1)];\n      r = r * width + s[mask & ((s[i] = s[j = mask & (j + t)]) + (s[j] = t))];\n    }\n    me.i = i; me.j = j;\n    return r;\n    // For robust unpredictability, the function call below automatically\n    // discards an initial batch of values.  This is called RC4-drop[256].\n    // See http://google.com/search?q=rsa+fluhrer+response&btnI\n  })(width);\n}\n\n//\n// copy()\n// Copies internal state of ARC4 to or from a plain object.\n//\nfunction copy(f, t) {\n  t.i = f.i;\n  t.j = f.j;\n  t.S = f.S.slice();\n  return t;\n};\n\n//\n// flatten()\n// Converts an object tree to nested arrays of strings.\n//\nfunction flatten(obj, depth) {\n  var result = [], typ = (typeof obj), prop;\n  if (depth && typ == 'object') {\n    for (prop in obj) {\n      try { result.push(flatten(obj[prop], depth - 1)); } catch (e) {}\n    }\n  }\n  return (result.length ? result : typ == 'string' ? obj : obj + '\\0');\n}\n\n//\n// mixkey()\n// Mixes a string seed into a key that is an array of integers, and\n// returns a shortened string seed that is equivalent to the result key.\n//\nfunction mixkey(seed, key) {\n  var stringseed = seed + '', smear, j = 0;\n  while (j < stringseed.length) {\n    key[mask & j] =\n      mask & ((smear ^= key[mask & j] * 19) + stringseed.charCodeAt(j++));\n  }\n  return tostring(key);\n}\n\n//\n// autoseed()\n// Returns an object for autoseeding, using window.crypto and Node crypto\n// module if available.\n//\nfunction autoseed() {\n  try {\n    var out;\n    if (nodecrypto && (out = nodecrypto.randomBytes)) {\n      // The use of 'out' to remember randomBytes makes tight minified code.\n      out = out(width);\n    } else {\n      out = new Uint8Array(width);\n      (global.crypto || global.msCrypto).getRandomValues(out);\n    }\n    return tostring(out);\n  } catch (e) {\n    var browser = global.navigator,\n        plugins = browser && browser.plugins;\n    return [+new Date, global, plugins, global.screen, tostring(pool)];\n  }\n}\n\n//\n// tostring()\n// Converts an array of charcodes to a string\n//\nfunction tostring(a) {\n  return String.fromCharCode.apply(0, a);\n}\n\n//\n// When seedrandom.js is loaded, we immediately mix a few bits\n// from the built-in RNG into the entropy pool.  Because we do\n// not want to interfere with deterministic PRNG state later,\n// seedrandom will not call math.random on its own again after\n// initialization.\n//\nmixkey(math.random(), pool);\n\n//\n// Nodejs and AMD support: export the implementation as a module using\n// either convention.\n//\nif ((typeof module) == 'object' && module.exports) {\n  module.exports = seedrandom;\n  // When in node.js, try using crypto package for autoseeding.\n  try {\n    nodecrypto = require('crypto');\n  } catch (ex) {}\n} else if ((typeof define) == 'function' && define.amd) {\n  define(function() { return seedrandom; });\n}\n\n// End anonymous scope, and pass initial values.\n})(\n  [],     // pool: entropy pool starts empty\n  Math    // math: package containing random, pow, and seedrandom\n);\n","// A library of seedable RNGs implemented in Javascript.\n//\n// Usage:\n//\n// var seedrandom = require('seedrandom');\n// var random = seedrandom(1); // or any seed.\n// var x = random();       // 0 <= x < 1.  Every bit is random.\n// var x = random.quick(); // 0 <= x < 1.  32 bits of randomness.\n\n// alea, a 53-bit multiply-with-carry generator by Johannes Baagøe.\n// Period: ~2^116\n// Reported to pass all BigCrush tests.\nvar alea = require('./lib/alea');\n\n// xor128, a pure xor-shift generator by George Marsaglia.\n// Period: 2^128-1.\n// Reported to fail: MatrixRank and LinearComp.\nvar xor128 = require('./lib/xor128');\n\n// xorwow, George Marsaglia's 160-bit xor-shift combined plus weyl.\n// Period: 2^192-2^32\n// Reported to fail: CollisionOver, SimpPoker, and LinearComp.\nvar xorwow = require('./lib/xorwow');\n\n// xorshift7, by François Panneton and Pierre L'ecuyer, takes\n// a different approach: it adds robustness by allowing more shifts\n// than Marsaglia's original three.  It is a 7-shift generator\n// with 256 bits, that passes BigCrush with no systmatic failures.\n// Period 2^256-1.\n// No systematic BigCrush failures reported.\nvar xorshift7 = require('./lib/xorshift7');\n\n// xor4096, by Richard Brent, is a 4096-bit xor-shift with a\n// very long period that also adds a Weyl generator. It also passes\n// BigCrush with no systematic failures.  Its long period may\n// be useful if you have many generators and need to avoid\n// collisions.\n// Period: 2^4128-2^32.\n// No systematic BigCrush failures reported.\nvar xor4096 = require('./lib/xor4096');\n\n// Tyche-i, by Samuel Neves and Filipe Araujo, is a bit-shifting random\n// number generator derived from ChaCha, a modern stream cipher.\n// https://eden.dei.uc.pt/~sneves/pubs/2011-snfa2.pdf\n// Period: ~2^127\n// No systematic BigCrush failures reported.\nvar tychei = require('./lib/tychei');\n\n// The original ARC4-based prng included in this library.\n// Period: ~2^1600\nvar sr = require('./seedrandom');\n\nsr.alea = alea;\nsr.xor128 = xor128;\nsr.xorwow = xorwow;\nsr.xorshift7 = xorshift7;\nsr.xor4096 = xor4096;\nsr.tychei = tychei;\n\nmodule.exports = sr;\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport * as tf from '@tensorflow/tfjs-core';\n\n// tslint:disable:no-any\n\n/**\n * A return value for a mapping function that can be applied via deepMap.\n *\n * If recurse is true, the value should be empty, and iteration will continue\n * into the object or array.\n */\nexport type DeepMapResult = {\n  value: any,\n  recurse: boolean\n};\n\n/**\n * Apply a mapping function to a nested structure in a recursive manner.\n *\n * The result of the mapping is an object with the same nested structure (i.e.,\n * of arrays and dicts) as the input, except that some subtrees are replaced,\n * according to the results of the mapping function.\n *\n * Mappings are memoized.  Thus, if the nested structure contains the same\n * object in multiple positions, the output will contain the same mapped object\n * in those positions.  Cycles are not supported, however.\n *\n * @param input: The object to which to apply the mapping function.\n * @param mapFn: A function that expects a single node of the object tree, and\n *   returns a `DeepMapResult`.  The `DeepMapResult` either provides a\n *   replacement value for that node (i.e., replacing the subtree), or indicates\n *   that the node should be processed recursively.\n */\nexport function deepMap(input: any, mapFn: (x: any) => DeepMapResult): any|\n    any[] {\n  return deepMapInternal(input, mapFn);\n}\n\n/**\n * @param seen: A Map of known object mappings (i.e., memoized results of\n *   `mapFn()`)\n * @param containedIn: An set containing objects on the reference path currently\n *   being processed (used to detect cycles).\n */\nfunction deepMapInternal(\n    input: any, mapFn: (x: any) => DeepMapResult,\n    seen: Map<any, any> = new Map(), containedIn: Set<{}> = new Set()): any|\n    any[] {\n  if (input == null) {\n    return null;\n  }\n  if (containedIn.has(input)) {\n    throw new Error('Circular references are not supported.');\n  }\n  if (seen.has(input)) {\n    return seen.get(input);\n  }\n  const result = mapFn(input);\n\n  if (result.recurse && result.value !== null) {\n    throw new Error(\n        'A deep map function may not return both a value and recurse=true.');\n  }\n\n  if (!result.recurse) {\n    seen.set(input, result.value);\n    return result.value;\n  } else if (isIterable(input)) {\n    // tslint:disable-next-line:no-any\n    const mappedIterable: any|any[] = Array.isArray(input) ? [] : {};\n    containedIn.add(input);\n    for (const k in input) {\n      const child = input[k];\n      const childResult = deepMapInternal(child, mapFn, seen, containedIn);\n      mappedIterable[k] = childResult;\n    }\n    containedIn.delete(input);\n    return mappedIterable;\n  } else {\n    throw new Error(`Can't recurse into non-iterable type: ${input}`);\n  }\n}\n\n// TODO(soergel, kangyizhang) Reconsider naming of deepZip() to avoid confusion\n// with zip()\n\n/**\n * Zip nested structures together in a recursive manner.\n *\n * This has the effect of transposing or pivoting data, e.g. converting it from\n * a row-major representation to a column-major representation.\n *\n * For example, `deepZip([{a: 1, b: 2}, {a: 3, b: 4}])` returns\n * `{a: [1, 3], b: [2, 4]}`.\n *\n * The inputs should all have the same nested structure (i.e., of arrays and\n * dicts).  The result is a single object with the same nested structure, where\n * the leaves are arrays collecting the values of the inputs at that location\n * (or, optionally, the result of a custom function applied to those arrays).\n *\n * @param inputs: An array of the objects to zip together.\n * @param zipFn: (optional) A function that expects an array of elements at a\n *   single node of the object tree, and returns a `DeepMapResult`.  The\n *   `DeepMapResult` either provides a result value for that node (i.e.,\n *   representing the subtree), or indicates that the node should be processed\n *   recursively.  The default zipFn recurses as far as possible and places\n *   arrays at the leaves.\n */\nexport function deepZip(\n    inputs: any[], zipFn: (xs: any[]) => DeepMapResult = zipToList): any|any[] {\n  return deepZipInternal(inputs, zipFn);\n}\n\n/**\n * @param containedIn: An set containing objects on the reference path currently\n *   being processed (used to detect cycles).\n */\nfunction deepZipInternal(\n    inputs: any[], zipFn: (xs: any[]) => DeepMapResult,\n    containedIn: Set<{}> = new Set()): any|any[] {\n  // The recursion follows the structure of input 0; it's assumed that all the\n  // other inputs have the same structure.\n  const input = inputs[0];\n  if (containedIn.has(input)) {\n    throw new Error('Circular references are not supported.');\n  }\n  const result = zipFn(inputs);\n\n  if (result.recurse && result.value !== null) {\n    throw new Error(\n        'A deep zip function may not return both a value and recurse=true.');\n  }\n\n  if (!result.recurse) {\n    return result.value;\n  } else if (isIterable(input)) {\n    // tslint:disable-next-line:no-any\n    const mappedIterable: any|any[] = Array.isArray(input) ? [] : {};\n    containedIn.add(input);\n    for (const k in input) {\n      const children = inputs.map(x => x[k]);\n      const childResult = deepZipInternal(children, zipFn, containedIn);\n      mappedIterable[k] = childResult;\n    }\n    containedIn.delete(input);\n    return mappedIterable;\n  } else {\n    throw new Error(`Can't recurse into non-iterable type: ${input}`);\n  }\n}\n\n// tslint:disable-next-line:no-any\nexport function zipToList(x: any[]): DeepMapResult {\n  if (x === null) {\n    return null;\n  }\n  // TODO(soergel): validate array type?\n\n  if (isIterable(x[0])) {\n    return {value: null, recurse: true};\n  } else {\n    return {value: x, recurse: false};\n  }\n}\n\n/**\n * A return value for an async map function for use with deepMapAndAwaitAll.\n *\n * If recurse is true, the value should be empty, and iteration will continue\n * into the object or array.\n */\nexport type DeepMapAsyncResult = {\n  value: Promise<any>,\n  recurse: boolean\n};\n\n/**\n * Apply an async mapping function to a nested structure in a recursive manner.\n *\n * This first creates a nested structure of Promises, and then awaits all of\n * those, resulting in a single Promise for a resolved nested structure.\n *\n * The result of the mapping is an object with the same nested structure (i.e.,\n * of arrays and dicts) as the input, except that some subtrees are replaced,\n * according to the results of the mapping function.\n *\n * Mappings are memoized.  Thus, if the nested structure contains the same\n * object in multiple positions, the output will contain the same mapped object\n * in those positions.  Cycles are not supported, however.\n *\n * @param input: The object to which to apply the mapping function.\n * @param mapFn: A function that expects a single node of the object tree, and\n *   returns a `DeepMapAsyncResult`.  The `DeepMapAsyncResult` either provides\n *   a `Promise` for a replacement value for that node (i.e., replacing the\n *   subtree), or indicates that the node should be processed recursively.  Note\n *   that the decision whether or not to recurse must be made immediately; only\n *   the mapped value may be promised.\n */\nexport async function deepMapAndAwaitAll(\n    input: any, mapFn: (x: any) => DeepMapAsyncResult): Promise<any|any[]> {\n  const seen: Map<any, Promise<any>> = new Map();\n\n  // First do a normal deepMap, collecting Promises in 'seen' as a side effect.\n  deepMapInternal(input, mapFn, seen);\n\n  // Replace the Promises in 'seen' in place.\n  // Note TypeScript provides no async map iteration, and regular map iteration\n  // is broken too, so sadly we have to do Array.from() to make it work.\n  // (There's no advantage to Promise.all(), and that would be tricky anyway.)\n  for (const key of Array.from(seen.keys())) {\n    const value = seen.get(key);\n    if (value instanceof Promise) {\n      const mappedValue = await value;\n      seen.set(key, mappedValue);\n    }\n  }\n\n  // Normal deepMap again, this time filling in the resolved values.\n  // It's unfortunate that we have to do two passes.\n  // TODO(soergel): test performance and think harder about a fast solution.\n  const result = deepMapInternal(input, mapFn, seen);\n  return result;\n}\n\n/**\n * Determine whether the argument is iterable.\n *\n * @returns true if the argument is an array or any non-Tensor object.\n */\n// tslint:disable-next-line:no-any\nexport function isIterable(obj: any): boolean {\n  return obj != null &&\n      (Array.isArray(obj) ||\n       (typeof obj === 'object' && !(obj instanceof tf.Tensor)));\n}\n\n/**\n * Determine whether the argument can be converted to Tensor.\n *\n * Tensors, primitives, arrays, and TypedArrays all qualify; anything else does\n * not.\n *\n * @returns true if the argument can be converted to Tensor.\n */\n// tslint:disable-next-line:no-any\nexport function canTensorify(obj: any): boolean {\n  return obj == null || isPrimitive(obj) || Array.isArray(obj) ||\n      (typeof obj === 'object' && (obj instanceof tf.Tensor)) ||\n      tf.util.isTypedArray(obj);\n}\n\n/**\n * Returns true if the given `value` is a primitive type. Otherwise returns\n * false. This is equivalant to node util.isPrimitive\n */\nfunction isPrimitive(value: any): boolean {\n  return (\n      value === null ||\n      (typeof value !== 'object' && typeof value !== 'function'));\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\n/**\n * A ring buffer, providing O(1) FIFO, LIFO, and related operations.\n */\nexport class RingBuffer<T> {\n  // Note we store the indices in the range 0 <= index < 2*capacity.\n  // This allows us to distinguish the full from the empty case.\n  // See https://www.snellman.net/blog/archive/2016-12-13-ring-buffers/\n  protected begin = 0;  // inclusive\n  protected end = 0;    // exclusive\n  protected doubledCapacity: number;\n\n  protected data: T[];\n\n  /**\n   * Constructs a `RingBuffer`.\n   * @param capacity The number of items that the buffer can accomodate.\n   */\n  constructor(public capacity: number) {\n    if (capacity == null) {\n      throw new RangeError('Can\\'t create a ring buffer of unknown capacity.');\n    }\n    if (capacity < 1) {\n      throw new RangeError('Can\\'t create ring buffer of capacity < 1.');\n    }\n    this.data = new Array<T>(capacity);\n    this.doubledCapacity = 2 * capacity;\n  }\n\n  /**\n   * Map any index into the range 0 <= index < 2*capacity.\n   */\n  protected wrap(index: number) {\n    // don't trust % on negative numbers\n    while (index < 0) {\n      index += this.doubledCapacity;\n    }\n    return index % this.doubledCapacity;\n  }\n\n  protected get(index: number) {\n    if (index < 0) {\n      throw new RangeError('Can\\'t get item at a negative index.');\n    }\n    return this.data[index % this.capacity];\n  }\n\n  protected set(index: number, value: T) {\n    if (index < 0) {\n      throw new RangeError('Can\\'t set item at a negative index.');\n    }\n    this.data[index % this.capacity] = value;\n  }\n\n  /**\n   * Returns the current number of items in the buffer.\n   */\n  length(): number {\n    let length = this.end - this.begin;\n    if (length < 0) {\n      length = this.doubledCapacity + length;\n    }\n    return length;\n  }\n\n  /**\n   * Reports whether the buffer is full.\n   * @returns true if the number of items in the buffer equals its capacity, and\n   *   false otherwise.\n   */\n  isFull() {\n    return this.length() === this.capacity;\n  }\n\n  /**\n   * Reports whether the buffer is empty.\n   * @returns true if the number of items in the buffer equals zero, and\n   *   false otherwise.\n   */\n  isEmpty() {\n    return this.length() === 0;\n  }\n\n  /**\n   * Adds an item to the end of the buffer.\n   */\n  push(value: T) {\n    if (this.isFull()) {\n      throw new RangeError('Ring buffer is full.');\n    }\n    this.set(this.end, value);\n    this.end = this.wrap(this.end + 1);\n  }\n\n  /**\n   * Adds many items to the end of the buffer, in order.\n   */\n  pushAll(values: T[]) {\n    for (const value of values) {\n      this.push(value);\n    }\n  }\n\n  /**\n   * Removes and returns the last item in the buffer.\n   */\n  pop(): T {\n    if (this.isEmpty()) {\n      throw new RangeError('Ring buffer is empty.');\n    }\n    this.end = this.wrap(this.end - 1);\n    const result = this.get(this.end);\n    this.set(this.end, undefined);\n    return result;\n  }\n\n  /**\n   * Adds an item to the beginning of the buffer.\n   */\n  unshift(value: T) {\n    if (this.isFull()) {\n      throw new RangeError('Ring buffer is full.');\n    }\n    this.begin = this.wrap(this.begin - 1);\n    this.set(this.begin, value);\n  }\n\n  /**\n   * Removes and returns the first item in the buffer.\n   */\n  shift(): T {\n    if (this.isEmpty()) {\n      throw new RangeError('Ring buffer is empty.');\n    }\n    const result = this.get(this.begin);\n    this.set(this.begin, undefined);\n    this.begin = this.wrap(this.begin + 1);\n    return result;\n  }\n\n  /**\n   * Removes and returns a specific item in the buffer, and moves the last item\n   * to the vacated slot.  This is useful for implementing a shuffling stream.\n   * Note that this operation necessarily scrambles the original order.\n   *\n   * @param relativeIndex: the index of the item to remove, relative to the\n   *   first item in the buffer (e.g., hiding the ring nature of the underlying\n   *   storage).\n   */\n  shuffleExcise(relativeIndex: number): T {\n    if (this.isEmpty()) {\n      throw new RangeError('Ring buffer is empty.');\n    }\n    const index = this.wrap(this.begin + relativeIndex);\n    const result = this.get(index);\n    this.set(index, this.pop());\n    return result;\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {RingBuffer} from './ring_buffer';\n\nexport class GrowingRingBuffer<T> extends RingBuffer<T> {\n  private static INITIAL_CAPACITY = 32;\n\n  /**\n   * Constructs a `GrowingRingBuffer`.\n   */\n  constructor() {\n    super(GrowingRingBuffer.INITIAL_CAPACITY);\n  }\n\n  isFull() {\n    return false;\n  }\n\n  push(value: T) {\n    if (super.isFull()) {\n      this.expand();\n    }\n    super.push(value);\n  }\n\n  unshift(value: T) {\n    if (super.isFull()) {\n      this.expand();\n    }\n    super.unshift(value);\n  }\n\n  /**\n   * Doubles the capacity of the buffer.\n   */\n  private expand() {\n    const newCapacity = this.capacity * 2;\n    const newData = new Array<T>(newCapacity);\n    const len = this.length();\n\n    // Rotate the buffer to start at index 0 again, since we can't just\n    // allocate more space at the end.\n    for (let i = 0; i < len; i++) {\n      newData[i] = this.get(this.wrap(this.begin + i));\n    }\n\n    this.data = newData;\n    this.capacity = newCapacity;\n    this.doubledCapacity = 2 * this.capacity;\n    this.begin = 0;\n    this.end = len;\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport {DataElement, IteratorContainer} from '../types';\nimport {deepMapAndAwaitAll, DeepMapAsyncResult, DeepMapResult, deepZip, zipToList} from '../util/deep_map';\nimport {GrowingRingBuffer} from '../util/growing_ring_buffer';\nimport {RingBuffer} from '../util/ring_buffer';\n\n// Here we implement a simple asynchronous iterator.\n// This lets us avoid using either third-party stream libraries or\n// recent TypeScript language support requiring polyfills.\n\n/**\n * Create a `LazyIterator` from an array of items.\n */\nexport function iteratorFromItems<T>(items: T[]): LazyIterator<T> {\n  return new ArrayIterator(items);\n}\n\n/**\n * Create a `LazyIterator` of incrementing integers.\n */\nexport function iteratorFromIncrementing(start: number): LazyIterator<number> {\n  let i = start;\n  return iteratorFromFunction(() => ({value: i++, done: false}));\n}\n\n/**\n * Create a `LazyIterator` from a function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * await iter.forEachAsync(e => console.log(e));\n * ```\n *\n * @param func A function that produces data on each call.\n */\nexport function iteratorFromFunction<T>(\n    func: () =>\n        IteratorResult<T>| Promise<IteratorResult<T>>): LazyIterator<T> {\n  return new FunctionCallIterator(func);\n}\n\n/**\n * Create a `LazyIterator` by concatenating underlying streams, which are\n * themselves provided as a stream.\n *\n * This can also be thought of as a \"stream flatten\" operation.\n *\n * @param baseIterators A stream of streams to be concatenated.\n * @param baseErrorHandler An optional function that can intercept `Error`s\n *   raised during a `next()` call on the base stream.  This function can decide\n *   whether the error should be propagated, whether the error should be\n *   ignored, or whether the base stream should be terminated.\n */\nexport function iteratorFromConcatenated<T>(\n    baseIterators: LazyIterator<LazyIterator<T>>,\n    baseErrorHandler?: (e: Error) => boolean): LazyIterator<T> {\n  return new ChainedIterator(baseIterators, baseErrorHandler);\n}\n\n/**\n * Create a `LazyIterator` by concatenating streams produced by calling a\n * stream-generating function a given number of times.\n *\n * Since a `LazyIterator` is read-once, it cannot be repeated, but this\n * function can be used to achieve a similar effect:\n *\n *   LazyIterator.ofConcatenatedFunction(() => new MyIterator(), 6);\n *\n * @param iteratorFunc: A function that produces a new stream on each call.\n * @param count: The number of times to call the function.\n * @param baseErrorHandler An optional function that can intercept `Error`s\n *   raised during a `next()` call on the base stream.  This function can decide\n *   whether the error should be propagated, whether the error should be\n *   ignored, or whether the base stream should be terminated.\n */\nexport function iteratorFromConcatenatedFunction<T>(\n    iteratorFunc: () => IteratorResult<LazyIterator<T>>, count: number,\n    baseErrorHandler?: (e: Error) => boolean): LazyIterator<T> {\n  return iteratorFromConcatenated(\n      iteratorFromFunction(iteratorFunc).take(count), baseErrorHandler);\n}\n\n/**\n * Create a `LazyIterator` by zipping together an array, dict, or nested\n * structure of `LazyIterator`s (and perhaps additional constants).\n *\n * The underlying streams must provide elements in a consistent order such\n * that they correspond.\n *\n * Typically, the underlying streams should have the same number of\n * elements. If they do not, the behavior is determined by the\n * `mismatchMode` argument.\n *\n * The nested structure of the `iterators` argument determines the\n * structure of elements in the resulting iterator.\n *\n * @param iterators: An array or object containing LazyIterators at the\n * leaves.\n * @param mismatchMode: Determines what to do when one underlying iterator\n * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)\n * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`\n * causes the zipped iterator to terminate with the furst underlying\n * streams, so elements remaining on the longer streams are ignored.\n * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling\n * in nulls for the exhausted streams, until all streams are exhausted.\n */\nexport function iteratorFromZipped<O extends DataElement>(\n    iterators: IteratorContainer,\n    mismatchMode: ZipMismatchMode = ZipMismatchMode.FAIL): LazyIterator<O> {\n  return new ZipIterator<O>(iterators, mismatchMode);\n}\n\n/**\n * An asynchronous iterator, providing lazy access to a potentially\n * unbounded stream of elements.\n *\n * Iterator can be obtained from a dataset:\n * `const iter = await dataset.iterator();`\n */\nexport abstract class LazyIterator<T> {\n  // This class implements AsyncIterator<T>, but we have not yet set the\n  // TypeScript --downlevelIteration flag to enable that.\n\n  abstract summary(): string;\n\n  /**\n   * Returns a `Promise` for the next element in the stream.\n   *\n   * When an item can be provided successfully, the return value is\n   * `{value:T, done:false}`.\n   *\n   * Calling next() on a closed stream returns `{value:null, done:true}`.\n   */\n  abstract async next(): Promise<IteratorResult<T>>;\n\n  /**\n   * Collect all remaining elements of a bounded stream into an array.\n   * Obviously this will succeed only for small streams that fit in memory.\n   * Useful for testing.\n   *\n   * @returns A Promise for an array of stream elements, which will resolve\n   *   when the stream is exhausted.\n   */\n  async toArray(): Promise<T[]> {\n    const result: T[] = [];\n    let x = await this.next();\n    while (!x.done) {\n      result.push(x.value);\n      x = await this.next();\n    }\n    return result;\n  }\n\n  /**\n   * Collect all elements of this dataset into an array with prefetching 100\n   * elements. This is useful for testing, because the prefetch changes the\n   * order in which the Promises are resolved along the processing pipeline.\n   * This may help expose bugs where results are dependent on the order of\n   * Promise resolution rather than on the logical order of the stream (i.e.,\n   * due to hidden mutable state).\n   *\n   * @returns A Promise for an array of stream elements, which will resolve\n   *   when the stream is exhausted.\n   */\n  async toArrayForTest(): Promise<T[]> {\n    const stream = this.prefetch(100);\n    const result: T[] = [];\n    let x = await stream.next();\n    while (!x.done) {\n      result.push(x.value);\n      x = await stream.next();\n    }\n    return result;\n  }\n\n  /**\n   * Draw items from the stream until it is exhausted.\n   *\n   * This can be useful when the stream has side effects but no output.  In\n   * that case, calling this function guarantees that the stream will be\n   * fully processed.\n   */\n  async resolveFully(): Promise<void> {\n    let x = await this.next();\n    while (!x.done) {\n      x = await this.next();\n    }\n  }\n\n  /**\n   * Draw items from the stream until it is exhausted, or a predicate fails.\n   *\n   * This can be useful when the stream has side effects but no output.  In\n   * that case, calling this function guarantees that the stream will be\n   * fully processed.\n   */\n  async resolveWhile(predicate: (r: T) => boolean): Promise<void> {\n    let x = await this.next();\n    let shouldContinue = predicate(x.value);\n    while ((!x.done) && shouldContinue) {\n      x = await this.next();\n      shouldContinue = predicate(x.value);\n    }\n  }\n\n  /**\n   * Handles errors thrown on this stream using a provided handler function.\n   *\n   * @param handler A function that handles any `Error` thrown during a `next()`\n   *   call and returns true if the stream should continue (dropping the failed\n   *   call) or false if the stream should quietly terminate.  If the handler\n   *   itself throws (or rethrows) an `Error`, that will be propagated.\n   *\n   * @returns A `LazyIterator` of elements passed through from upstream,\n   *   possibly filtering or terminating on upstream `next()` calls that\n   *   throw an `Error`.\n   */\n  handleErrors(handler: (error: Error) => boolean): LazyIterator<T> {\n    return new ErrorHandlingLazyIterator(this, handler);\n  }\n\n  // TODO(soergel): Implement reduce() etc.\n\n  /**\n   * Filters this stream according to `predicate`.\n   *\n   * @param predicate A function mapping a stream element to a boolean or a\n   * `Promise` for one.\n   *\n   * @returns A `LazyIterator` of elements for which the predicate was true.\n   */\n  filter(predicate: (value: T) => boolean): LazyIterator<T> {\n    return new FilterIterator(this, predicate);\n  }\n\n  /**\n   * Maps this stream through a 1-to-1 transform.\n   *\n   * @param transform A function mapping a stream element to a transformed\n   *   element.\n   *\n   * @returns A `LazyIterator` of transformed elements.\n   */\n  map<O>(transform: (value: T) => O): LazyIterator<O> {\n    return new MapIterator(this, transform);\n  }\n\n  /**\n   * Maps this stream through an async 1-to-1 transform.\n   *\n   * @param transform A function mapping a stream element to a `Promise` for a\n   *   transformed stream element.\n   *\n   * @returns A `LazyIterator` of transformed elements.\n   */\n  mapAsync<O>(transform: (value: T) => Promise<O>): LazyIterator<O> {\n    return new AsyncMapIterator(this, transform);\n  }\n\n  /**\n   * Maps this stream through a 1-to-1 transform, forcing serial execution.\n   *\n   * @param transform A function mapping a stream element to a transformed\n   *   element.\n   *\n   * @returns A `LazyIterator` of transformed elements.\n   */\n  serialMapAsync<O>(transform: (value: T) => Promise<O>): LazyIterator<O> {\n    return new AsyncMapIterator(this, transform).serial();\n  }\n\n  /**\n   * Maps this stream through a 1-to-many transform.\n   *\n   * @param transform A function mapping a stream element to an array of\n   *   transformed elements.\n   *\n   * @returns A `DataStream` of transformed elements.\n   */\n  flatmap<O>(transform: (value: T) => O[]): LazyIterator<O> {\n    return new FlatmapIterator(this, transform);\n  }\n\n  /**\n   * Apply a function to every element of the stream.\n   *\n   * @param f A function to apply to each stream element.\n   */\n  async forEachAsync(f: (value: T) => void): Promise<void> {\n    return this.map(f).resolveFully();\n  }\n\n  /**\n   * Apply a function to every element of the stream, forcing serial execution.\n   *\n   * @param f A function to apply to each stream element.  Should return 'true'\n   *   to indicate that the stream should continue, or 'false' to cause it to\n   *   terminate.\n   */\n  async serialForEach(f: (value: T) => Promise<boolean>): Promise<void> {\n    return this.serialMapAsync(f).resolveWhile(x => (x === true));\n  }\n\n  /**\n   * Groups elements into batches, represented as arrays of elements.\n   *\n   * We can think of the elements of this iterator as 'rows' (even if they are\n   * nested structures).  By the same token, consecutive values for a given\n   * key within the elements form a 'column'.  This matches the usual sense of\n   * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).\n   *\n   * Thus, \"Row-major\" means that the resulting batch is simply a collection of\n   * rows: `[row1, row2, row3, ...]`.  This is contrast to the column-major\n   * form, which is needed for vectorized computation.\n   *\n   * @param batchSize The number of elements desired per batch.\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\n   *   than batchSize elements. Default true.\n   * @returns A `LazyIterator` of batches of elements, represented as arrays\n   *   of the original element type.\n   */\n  rowMajorBatch(batchSize: number, smallLastBatch = true): LazyIterator<T[]> {\n    return new RowMajorBatchIterator(this, batchSize, smallLastBatch);\n  }\n\n  /**\n   * Groups elements into batches, represented in column-major form.\n   *\n   * We can think of the elements of this iterator as 'rows' (even if they are\n   * nested structures).  By the same token, consecutive values for a given\n   * key within the elements form a 'column'.  This matches the usual sense of\n   * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).\n   *\n   * Thus, \"column-major\" means that the resulting batch is a (potentially\n   * nested) structure representing the columns.  Each column entry, then,\n   * contains a collection of the values found in that column for a range of\n   * input elements.  This representation allows for vectorized computation, in\n   * contrast to the row-major form.\n   *\n   * The inputs should all have the same nested structure (i.e., of arrays and\n   * dicts).  The result is a single object with the same nested structure,\n   * where the leaves are arrays collecting the values of the inputs at that\n   * location (or, optionally, the result of a custom function applied to those\n   * arrays).\n   *\n   * @param batchSize The number of elements desired per batch.\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\n   *   than batchSize elements. Default true.\n   * @param zipFn: (optional) A function that expects an array of elements at a\n   *   single node of the object tree, and returns a `DeepMapResult`.  The\n   *   `DeepMapResult` either provides a result value for that node (i.e.,\n   *   representing the subtree), or indicates that the node should be processed\n   *   recursively.  The default zipFn recurses as far as possible and places\n   *   arrays at the leaves.\n   * @returns A `LazyIterator` of batches of elements, represented as an object\n   *   with collections at the leaves.\n   */\n  columnMajorBatch(\n      batchSize: number, smallLastBatch = true,\n      // tslint:disable-next-line:no-any\n      zipFn: (xs: any[]) => DeepMapResult = zipToList):\n      LazyIterator<DataElement> {\n    // First collect the desired number of input elements as a row-major batch.\n    const rowBatches = this.rowMajorBatch(batchSize, smallLastBatch);\n    // Now 'rotate' or 'pivot' the data, collecting all values from each column\n    // in the batch (i.e., for each key within the elements) into an array.\n    return rowBatches.map(x => deepZip(x, zipFn));\n  }\n\n  /**\n   * Concatenate this `LazyIterator` with another.\n   *\n   * @param iterator A `LazyIterator` to be concatenated onto this one.\n   * @param baseErrorHandler An optional function that can intercept `Error`s\n   *   raised during a `next()` call on the base stream.  This function can\n   *   decide whether the error should be propagated, whether the error should\n   *   be ignored, or whether the base stream should be terminated.\n   * @returns A `LazyIterator`.\n   */\n  concatenate(\n      iterator: LazyIterator<T>,\n      baseErrorHandler?: (e: Error) => boolean): LazyIterator<T> {\n    return new ChainedIterator(\n        iteratorFromItems([this, iterator]), baseErrorHandler);\n  }\n\n  /**\n   * Limits this stream to return at most `count` items.\n   *\n   * @param count The maximum number of items to provide from the stream. If\n   * a negative or undefined value is given, the entire stream is returned\n   *   unaltered.\n   */\n  take(count: number): LazyIterator<T> {\n    if (count < 0 || count == null) {\n      return this;\n    }\n    return new TakeIterator(this, count);\n  }\n\n  /**\n   * Skips the first `count` items in this stream.\n   *\n   * @param count The number of items to skip.  If a negative or undefined\n   * value is given, the entire stream is returned unaltered.\n   */\n  skip(count: number): LazyIterator<T> {\n    if (count < 0 || count == null) {\n      return this;\n    }\n    return new SkipIterator(this, count);\n  }\n\n  /**\n   * Prefetch the first `bufferSize` items in this stream.\n   *\n   * Note this prefetches Promises, but makes no guarantees about when those\n   * Promises resolve.\n   *\n   * @param bufferSize: An integer specifying the number of elements to be\n   *   prefetched.\n   */\n  prefetch(bufferSize: number): LazyIterator<T> {\n    return new PrefetchIterator(this, bufferSize);\n  }\n\n  // TODO(soergel): deep sharded shuffle, where supported\n\n  /**\n   * Randomly shuffles the elements of this stream.\n   *\n   * @param bufferSize: An integer specifying the number of elements from\n   * this stream from which the new stream will sample.\n   * @param seed: (Optional.) An integer specifying the random seed that\n   * will be used to create the distribution.\n   */\n  shuffle(windowSize: number, seed?: string): LazyIterator<T> {\n    return new ShuffleIterator(this, windowSize, seed);\n  }\n\n  /**\n   * Force an iterator to execute serially: each next() call will await the\n   * prior one, so that they cannot execute concurrently.\n   */\n  serial(): LazyIterator<T> {\n    return new SerialIterator(this);\n  }\n}\n\n// ============================================================================\n// The following private classes serve to implement the chainable methods\n// on LazyIterator.  Unfortunately they can't be placed in separate files,\n// due to resulting trouble with circular imports.\n// ============================================================================\n\n// Iterators that just extend LazyIterator directly\n// ============================================================================\n\nclass ArrayIterator<T> extends LazyIterator<T> {\n  private trav = 0;\n  constructor(protected items: T[]) {\n    super();\n  }\n\n  summary() {\n    return `Array of ${this.items.length} items`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    if (this.trav >= this.items.length) {\n      return {value: null, done: true};\n    }\n    const item = this.items[this.trav];\n    let result;\n    if (item instanceof tf.Tensor) {\n      result = tf.clone(item);\n    } else {\n      result = item;\n    }\n    this.trav++;\n    return {value: result, done: false};\n  }\n}\n\nclass FunctionCallIterator<T> extends LazyIterator<T> {\n  constructor(\n      protected nextFn: () => IteratorResult<T>| Promise<IteratorResult<T>>) {\n    super();\n  }\n\n  summary() {\n    return `Function call`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    try {\n      return this.nextFn();\n    } catch (e) {\n      // Modify the error message but leave the stack trace intact\n      e.message =\n          `Error thrown while iterating through a dataset: ${e.message}`;\n      throw e;\n    }\n  }\n}\n\nclass SerialIterator<T> extends LazyIterator<T> {\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>>;\n\n  constructor(protected upstream: LazyIterator<T>) {\n    super();\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Serial`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  private async serialNext(): Promise<IteratorResult<T>> {\n    return this.upstream.next();\n  }\n}\n\nclass SkipIterator<T> extends LazyIterator<T> {\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>>;\n\n  // Local state that should not be clobbered by out-of-order execution.\n  count = 0;\n\n  constructor(protected upstream: LazyIterator<T>, protected maxCount: number) {\n    super();\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Skip`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  private async serialNext(): Promise<IteratorResult<T>> {\n    // TODO(soergel): consider tradeoffs of reading in parallel, eg.\n    // collecting next() promises in an Array and then waiting for\n    // Promise.all() of those. Benefit: pseudo-parallel execution.  Drawback:\n    // maybe delayed GC.\n    while (this.count++ < this.maxCount) {\n      const skipped = await this.upstream.next();\n      // short-circuit if upstream is already empty\n      if (skipped.done) {\n        return skipped;\n      }\n      tf.dispose(skipped.value as {});\n    }\n    return this.upstream.next();\n  }\n}\n\nclass TakeIterator<T> extends LazyIterator<T> {\n  count = 0;\n  constructor(protected upstream: LazyIterator<T>, protected maxCount: number) {\n    super();\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Take`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    if (this.count++ >= this.maxCount) {\n      return {value: null, done: true};\n    }\n    return this.upstream.next();\n  }\n}\n\n// Note this batch just groups items into row-wise element arrays.\n// Rotating these to a column-wise representation happens only at the dataset\n// level.\nclass RowMajorBatchIterator<T> extends LazyIterator<T[]> {\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T[]>>;\n\n  constructor(\n      protected upstream: LazyIterator<T>, protected batchSize: number,\n      protected enableSmallLastBatch = true) {\n    super();\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> RowMajorBatch`;\n  }\n\n  async next(): Promise<IteratorResult<T[]>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  private async serialNext(): Promise<IteratorResult<T[]>> {\n    const batch: T[] = [];\n    while (batch.length < this.batchSize) {\n      const item = await this.upstream.next();\n      if (item.done) {\n        if (this.enableSmallLastBatch && batch.length > 0) {\n          return {value: batch, done: false};\n        }\n        return {value: null, done: true};\n      }\n      batch.push(item.value);\n    }\n    return {value: batch, done: false};\n  }\n}\n\nclass FilterIterator<T> extends LazyIterator<T> {\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>>;\n\n  constructor(\n      protected upstream: LazyIterator<T>,\n      protected predicate: (value: T) => boolean) {\n    super();\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Filter`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  private async serialNext(): Promise<IteratorResult<T>> {\n    while (true) {\n      const item = await this.upstream.next();\n      if (item.done || this.predicate(item.value)) {\n        return item;\n      }\n      tf.dispose(item.value as {});\n    }\n  }\n}\n\nclass MapIterator<I, O> extends LazyIterator<O> {\n  constructor(\n      protected upstream: LazyIterator<I>,\n      protected transform: (value: I) => O) {\n    super();\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Map`;\n  }\n\n  async next(): Promise<IteratorResult<O>> {\n    const item = await this.upstream.next();\n    if (item.done) {\n      return {value: null, done: true};\n    }\n    const inputTensors = tf.tensor_util.getTensorsInContainer(item.value as {});\n    // Careful: the transform may mutate the item in place.\n    // That's why we have to remember the input Tensors above, and then\n    // below dispose only those that were not passed through to the output.\n    // Note too that the transform function is responsible for tidying\n    // any intermediate Tensors.  Here we are concerned only about the\n    // inputs.\n    const mapped = this.transform(item.value);\n    const outputTensors = tf.tensor_util.getTensorsInContainer(mapped as {});\n\n    // TODO(soergel) faster intersection\n    // TODO(soergel) move to tf.disposeExcept(in, out)?\n    for (const t of inputTensors) {\n      if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n        t.dispose();\n      }\n    }\n    return {value: mapped, done: false};\n  }\n}\n\nclass ErrorHandlingLazyIterator<T> extends LazyIterator<T> {\n  count = 0;\n  constructor(\n      protected upstream: LazyIterator<T>,\n      protected handler: (error: Error) => boolean) {\n    super();\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> handleErrors`;\n  }\n\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>>;\n\n  async next(): Promise<IteratorResult<T>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  async serialNext(): Promise<IteratorResult<T>> {\n    while (true) {\n      try {\n        return await this.upstream.next();\n      } catch (e) {\n        if (!this.handler(e)) {\n          return {value: null, done: true};\n        }\n        // If the handler returns true, loop and fetch the next upstream item.\n\n        // If the upstream iterator throws an endless stream of errors, and if\n        // the handler says to ignore them, then we loop forever here.  That is\n        // the correct behavior-- it's up to the handler to decide when to stop.\n      }\n    }\n  }\n}\n\nclass AsyncMapIterator<I, O> extends LazyIterator<O> {\n  constructor(\n      protected upstream: LazyIterator<I>,\n      protected transform: (value: I) => Promise<O>) {\n    super();\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> AsyncMap`;\n  }\n\n  async next(): Promise<IteratorResult<O>> {\n    const item = await this.upstream.next();\n    if (item.done) {\n      return {value: null, done: true};\n    }\n    const inputTensors = tf.tensor_util.getTensorsInContainer(item.value as {});\n    // Careful: the transform may mutate the item in place.\n    // That's why we have to remember the input Tensors above, and then\n    // below dispose only those that were not passed through to the output.\n    // Note too that the transform function is responsible for tidying\n    // any intermediate Tensors.  Here we are concerned only about the\n    // inputs.\n    const mapped = await this.transform(item.value);\n    const outputTensors = tf.tensor_util.getTensorsInContainer(mapped as {});\n\n    // TODO(soergel) faster intersection\n    // TODO(soergel) move to tf.disposeExcept(in, out)?\n    for (const t of inputTensors) {\n      if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n        t.dispose();\n      }\n    }\n    return {value: mapped, done: false};\n  }\n}\n\n// Iterators that maintain a queue of pending items\n// ============================================================================\n\n/**\n * A base class for transforming streams that operate by maintaining an\n * output queue of elements that are ready to return via next().  This is\n * commonly required when the transformation is 1-to-many:  A call to next()\n * may trigger a call to the underlying stream, which will produce many\n * mapped elements of this stream-- of which we need to return only one, so\n * we have to queue the rest.\n */\nexport abstract class OneToManyIterator<T> extends LazyIterator<T> {\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>>;\n\n  // Local state that should not be clobbered by out-of-order execution.\n  protected outputQueue: RingBuffer<T>;\n\n  constructor() {\n    super();\n    this.outputQueue = new GrowingRingBuffer<T>();\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  /**\n   * Read one or more chunks from upstream and process them, possibly\n   * reading or writing a carryover, and adding processed items to the\n   * output queue.  Note it's possible that no items are added to the queue\n   * on a given pump() call, even if the upstream stream is not closed\n   * (e.g., because items are filtered).\n   *\n   * @return `true` if any action was taken, i.e. fetching items from the\n   *   upstream source OR adding items to the output queue.  `false` if the\n   *   upstream source is exhausted AND nothing was added to the queue\n   * (i.e., any remaining carryover).\n   */\n  protected abstract async pump(): Promise<boolean>;\n\n  async serialNext(): Promise<IteratorResult<T>> {\n    // Fetch so that the queue contains at least one item if possible.\n    // If the upstream source is exhausted, AND there are no items left in\n    // the output queue, then this stream is also exhausted.\n    while (this.outputQueue.length() === 0) {\n      // TODO(soergel): consider parallel reads.\n      if (!await this.pump()) {\n        return {value: null, done: true};\n      }\n    }\n    return {value: this.outputQueue.shift(), done: false};\n  }\n}\nclass FlatmapIterator<I, O> extends OneToManyIterator<O> {\n  constructor(\n      protected upstream: LazyIterator<I>,\n      protected transform: (value: I) => O[]) {\n    super();\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Flatmap`;\n  }\n\n  async pump(): Promise<boolean> {\n    const item = await this.upstream.next();\n    if (item.done) {\n      return false;\n    }\n    const inputTensors = tf.tensor_util.getTensorsInContainer(item.value as {});\n    // Careful: the transform may mutate the item in place.\n    // that's why we have to remember the input Tensors above, and then\n    // below dispose only those that were not passed through to the output.\n    // Note too that the transform function is responsible for tidying any\n    // intermediate Tensors.  Here we are concerned only about the inputs.\n    const mappedArray = this.transform(item.value);\n    const outputTensors =\n        tf.tensor_util.getTensorsInContainer(mappedArray as {});\n    this.outputQueue.pushAll(mappedArray);\n\n    // TODO(soergel) faster intersection, and deduplicate outputTensors\n    // TODO(soergel) move to tf.disposeExcept(in, out)?\n    for (const t of inputTensors) {\n      if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n        t.dispose();\n      }\n    }\n\n    return true;\n  }\n}\n\n/**\n * Provides a `LazyIterator` that concatenates a stream of underlying\n * streams.\n *\n * Doing this in a concurrency-safe way requires some trickery.  In\n * particular, we want this stream to return the elements from the\n * underlying streams in the correct order according to when next() was\n * called, even if the resulting Promises resolve in a different order.\n */\nexport class ChainedIterator<T> extends LazyIterator<T> {\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>> = null;\n\n  // Local state that should not be clobbered by out-of-order execution.\n  private iterator: LazyIterator<T> = null;\n  private moreIterators: LazyIterator<LazyIterator<T>>;\n\n  constructor(\n      iterators: LazyIterator<LazyIterator<T>>,\n      private readonly baseErrorHandler?: (e: Error) => boolean) {\n    super();\n    this.moreIterators = iterators;\n  }\n\n  summary() {\n    const upstreamSummaries = 'TODO: fill in upstream of chained summaries';\n    return `${upstreamSummaries} -> Chained`;\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    this.lastRead = this.readFromChain(this.lastRead);\n    return this.lastRead;\n  }\n\n  private async readFromChain(lastRead: Promise<IteratorResult<T>>):\n      Promise<IteratorResult<T>> {\n    // Must await on the previous read since the previous read may have advanced\n    // the stream of streams, from which we need to read.\n    // This is unfortunate since we can't parallelize reads. Which means\n    // prefetching of chained streams is a no-op.\n    // One solution is to prefetch immediately upstream of this.\n    await lastRead;\n    if (this.iterator == null) {\n      const iteratorResult = await this.moreIterators.next();\n      if (iteratorResult.done) {\n        // No more streams to stream from.\n        return {value: null, done: true};\n      }\n      this.iterator = iteratorResult.value;\n      if (this.baseErrorHandler != null) {\n        this.iterator = this.iterator.handleErrors(this.baseErrorHandler);\n      }\n    }\n    const itemResult = await this.iterator.next();\n    if (itemResult.done) {\n      this.iterator = null;\n      return this.readFromChain(lastRead);\n    }\n    return itemResult;\n  }\n}\n\nexport enum ZipMismatchMode {\n  FAIL,      // require zipped streams to have the same length\n  SHORTEST,  // terminate zip when the first stream is exhausted\n  LONGEST    // use nulls for exhausted streams; use up the longest stream.\n}\n\n/**\n * Provides a `LazyIterator` that zips together an array, dict, or nested\n * structure of `LazyIterator`s (and perhaps additional constants).\n *\n * The underlying streams must provide elements in a consistent order such\n * that they correspond.\n *\n * Typically, the underlying streams should have the same number of\n * elements. If they do not, the behavior is determined by the\n * `mismatchMode` argument.\n *\n * The nested structure of the `iterators` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Doing this in a concurrency-safe way requires some trickery.  In\n * particular, we want this stream to return the elements from the\n * underlying streams in the correct order according to when next() was\n * called, even if the resulting Promises resolve in a different order.\n *\n * @param iterators: An array or object containing LazyIterators at the\n * leaves.\n * @param mismatchMode: Determines what to do when one underlying iterator\n * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)\n * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`\n * causes the zipped iterator to terminate with the furst underlying\n * streams, so elements remaining on the longer streams are ignored.\n * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling\n * in nulls for the exhausted streams, until all streams are exhausted.\n */\nclass ZipIterator<O extends DataElement> extends LazyIterator<O> {\n  private count = 0;\n  private currentPromise: Promise<IteratorResult<O>> = null;\n\n  constructor(\n      protected readonly iterators: IteratorContainer,\n      protected readonly mismatchMode: ZipMismatchMode = ZipMismatchMode.FAIL) {\n    super();\n  }\n\n  summary() {\n    const upstreamSummaries = 'TODO: fill in upstream of zip summaries';\n    return `{${upstreamSummaries}} -> Zip`;\n  }\n\n  private async nextState(afterState: Promise<IteratorResult<O>>):\n      Promise<IteratorResult<O>> {\n    // This chaining ensures that the underlying next() are not even called\n    // before the previous ones have resolved.\n    await afterState;\n\n    // Collect underlying iterator \"done\" signals as a side effect in\n    // getNext()\n    let numIterators = 0;\n    let iteratorsDone = 0;\n\n    function getNext(container: IteratorContainer): DeepMapAsyncResult {\n      if (container instanceof LazyIterator) {\n        const result = container.next();\n        return {\n          value: result.then(x => {\n            numIterators++;\n            if (x.done) {\n              iteratorsDone++;\n            }\n            return x.value;\n          }),\n          recurse: false\n        };\n      } else {\n        return {value: null, recurse: true};\n      }\n    }\n\n    const mapped: O = await deepMapAndAwaitAll(this.iterators, getNext);\n\n    if (numIterators === iteratorsDone) {\n      // The streams have all ended.\n      return {value: null, done: true};\n    }\n    if (iteratorsDone > 0) {\n      switch (this.mismatchMode) {\n        case ZipMismatchMode.FAIL:\n          throw new Error(\n              'Zipped streams should have the same length. ' +\n              `Mismatched at element ${this.count}.`);\n        case ZipMismatchMode.SHORTEST:\n          return {value: null, done: true};\n        case ZipMismatchMode.LONGEST:\n        default:\n          // Continue.  The exhausted streams already produced value: null.\n      }\n    }\n\n    this.count++;\n    return {value: mapped, done: false};\n  }\n\n  async next(): Promise<IteratorResult<O>> {\n    this.currentPromise = this.nextState(this.currentPromise);\n    return (await this.currentPromise);\n  }\n}\n\n// Iterators that maintain a ring buffer of pending promises\n// ============================================================================\n\n/**\n * A stream that prefetches a given number of items from an upstream source,\n * returning them in FIFO order.\n *\n * Note this prefetches Promises, but makes no guarantees about when those\n * Promises resolve.\n */\nexport class PrefetchIterator<T> extends LazyIterator<T> {\n  protected buffer: RingBuffer<Promise<IteratorResult<T>>>;\n\n  constructor(\n      protected upstream: LazyIterator<T>, protected bufferSize: number) {\n    super();\n    this.buffer = new RingBuffer<Promise<IteratorResult<T>>>(bufferSize);\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Prefetch`;\n  }\n\n  /**\n   * Refill the prefetch buffer.  Returns only after the buffer is full, or\n   * the upstream source is exhausted.\n   */\n  protected refill() {\n    while (!this.buffer.isFull()) {\n      const v = this.upstream.next();\n      this.buffer.push(v);\n    }\n  }\n\n  next(): Promise<IteratorResult<T>> {\n    this.refill();\n    // This shift will never throw an error because the buffer is always\n    // full after a refill. If the stream is exhausted, the buffer will be\n    // full of Promises that will resolve to the end-of-stream signal.\n    return this.buffer.shift();\n  }\n}\n\n/**\n * A stream that performs a sliding-window random shuffle on an upstream\n * source. This is like a `PrefetchIterator` except that the items are\n * returned in randomized order.  Mixing naturally improves as the buffer\n * size increases.\n */\nexport class ShuffleIterator<T> extends PrefetchIterator<T> {\n  private readonly random: seedrandom.prng;\n\n  // Strict Promise execution order:\n  // a next() call may not even begin until the previous one completes.\n  private lastRead: Promise<IteratorResult<T>>;\n\n  // Local state that should not be clobbered by out-of-order execution.\n  private upstreamExhausted = false;\n\n  constructor(\n      protected upstream: LazyIterator<T>, protected windowSize: number,\n      seed?: string) {\n    super(upstream, windowSize);\n    this.random = seedrandom.alea(seed || tf.util.now().toString());\n    this.lastRead = Promise.resolve({value: null, done: false});\n  }\n\n  async next(): Promise<IteratorResult<T>> {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n\n  private randomInt(max: number) {\n    return Math.floor(this.random() * max);\n  }\n\n  protected chooseIndex(): number {\n    return this.randomInt(this.buffer.length());\n  }\n\n  async serialNext(): Promise<IteratorResult<T>> {\n    // TODO(soergel): consider performance\n    if (!this.upstreamExhausted) {\n      this.refill();\n    }\n    while (!this.buffer.isEmpty()) {\n      const chosenIndex = this.chooseIndex();\n      const result = await this.buffer.shuffleExcise(chosenIndex);\n      if (result.done) {\n        this.upstreamExhausted = true;\n      } else {\n        this.refill();\n        return result;\n      }\n    }\n    return {value: null, done: true};\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport * as tf from '@tensorflow/tfjs-core';\nimport {TensorLike} from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport {iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, LazyIterator, ZipMismatchMode} from './iterators/lazy_iterator';\nimport {DataElement, DatasetContainer} from './types';\nimport {canTensorify, deepMapAndAwaitAll, DeepMapResult, isIterable} from './util/deep_map';\n\n// TODO(soergel): consider vectorized operations within the pipeline.\n\n/**\n * Represents a potentially large list of independent data elements (typically\n * 'samples' or 'examples').\n *\n * A 'data example' may be a primitive, an array, a map from string keys to\n * values, or any nested structure of these.\n *\n * A `Dataset` represents an ordered collection of elements, together with a\n * chain of transformations to be performed on those elements. Each\n * transformation is a method of `Dataset` that returns another `Dataset`, so\n * these may be chained, e.g.\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n *\n * Data loading and transformation is done in a lazy, streaming fashion.  The\n * dataset may be iterated over multiple times; each iteration starts the data\n * loading anew and recapitulates the transformations.\n *\n * A `Dataset` is typically processed as a stream of unbatched examples --i.e.,\n * its transformations are applied one example at a time. Batching produces a\n * new `Dataset` where each element is a batch. Batching should usually come\n * last in a pipeline, because data transformations are easier to express on a\n * per-example basis than on a per-batch basis.\n *\n * The following code examples are calling `await dataset.forEachAsync(...)` to\n * iterate once over the entire dataset in order to print out the data.\n */\n/** @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'} */\nexport abstract class Dataset<T extends DataElement> {\n  /*\n   * Provide a new stream of elements.  Note this will also start new streams\n   * from any underlying `Dataset`s.\n   *\n   * CAUTION: Any Tensors contained within the elements returned from\n   * this stream *must* be manually disposed to avoid a GPU memory leak.\n   * The tf.tidy() approach cannot be used in an asynchronous context.\n   */\n  abstract async iterator(): Promise<LazyIterator<T>>;\n\n  readonly size: number = null;\n\n  // TODO(soergel): Make Datasets report whether repeated iterator() calls\n  // produce the same result (e.g., reading from a file) or different results\n  // (e.g., from the webcam).  Currently we don't make this distinction but it\n  // could be important for the user to know.\n  // abstract isDeterministic(): boolean;\n\n  /**\n   * Groups elements into batches.\n   *\n   * It is assumed that each of the incoming dataset elements has the same\n   * structure-- i.e. the same set of keys at each location in an object\n   * hierarchy.  For each key, the resulting `Dataset` provides a batched\n   * element collecting all of the incoming values for that key.\n   *\n   *  * Incoming primitives are grouped into a 1-D Tensor.\n   *  * Incoming Tensors are grouped into a new Tensor where the 0'th axis is\n   *    the batch dimension.\n   *  * Incoming arrays are converted to Tensor and then batched.\n   *  * A nested array is interpreted as an n-D Tensor, so the batched result\n   *    has n+1 dimensions.\n   *  * An array that cannot be converted to Tensor produces an error.\n   *\n   * If an array should not be batched as a unit, it should first be converted\n   * to an object with integer keys.\n   *\n   * Here are a few examples:\n   *\n   * Batch a dataset of numbers:\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\n   * await a.forEachAsync(e => e.print());\n   * ```\n   *\n   * Batch a dataset of arrays:\n   * ```js\n   * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\n   * await b.forEachAsync(e => e.print());\n   * ```\n   *\n   * Batch a dataset of objects:\n   * ```js\n   * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n   *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n   *   {a: 8, b: 18}]).batch(4);\n   * await c.forEachAsync(e => {\n   *   console.log('{');\n   *   for(var key in e) {\n   *     console.log(key+':');\n   *     e[key].print();\n   *   }\n   *   console.log('}');\n   * })\n   * ```\n   *\n   * @param batchSize The number of elements desired per batch.\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\n   *   than batchSize elements. Default true.\n   * @returns A `Dataset`, from which a stream of batches can be obtained.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  batch(batchSize: number, smallLastBatch = true): Dataset<DataElement> {\n    const base = this;\n    tf.util.assert(\n        batchSize > 0, () => `batchSize needs to be positive, but it is\n      ${batchSize}`);\n    let size;\n    if (this.size === Infinity || this.size == null) {\n      // If the size of this dataset is infinity or null, the new size keeps the\n      // same.\n      size = this.size;\n    } else if (smallLastBatch) {\n      // If the size of this dataset is known and include small last batch, the\n      // new size is full batch count plus last batch.\n      size = Math.ceil(this.size / batchSize);\n    } else {\n      // If the size of this dataset is known and not include small last batch,\n      // the new size is full batch count.\n      size = Math.floor(this.size / batchSize);\n    }\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator())\n          .columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);\n    }, size);\n  }\n\n  /**\n   * Concatenates this `Dataset` with another.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]);\n   * const b = tf.data.array([4, 5, 6]);\n   * const c = a.concatenate(b);\n   * await c.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param dataset A `Dataset` to be concatenated onto this one.\n   * @returns A `Dataset`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  concatenate(dataset: Dataset<T>): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size === Infinity || dataset.size === Infinity) {\n      // If the size of any of these two dataset is infinity, new size is\n      // infinity.\n      size = Infinity;\n    } else if (this.size != null && dataset.size != null) {\n      // If the size of both datasets are known and not infinity, new size is\n      // sum the size of these two datasets.\n      size = this.size + dataset.size;\n    } else {\n      // If neither of these two datasets has infinite size and any of these two\n      // datasets' size is null, the new size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(\n        async () =>\n            (await base.iterator()).concatenate(await dataset.iterator()),\n        size);\n  }\n\n  /**\n   * Filters this dataset according to `predicate`.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n   *   .filter(x => x%2 === 0);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param predicate A function mapping a dataset element to a boolean or a\n   * `Promise` for one.\n   *\n   * @returns A `Dataset` of elements for which the predicate was true.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  filter(predicate: (value: T) => boolean): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size === Infinity) {\n      // If the size of this dataset is infinity, new size is infinity\n      size = Infinity;\n    } else {\n      // If this dataset has limited elements, new size is null because it might\n      // exhausted randomly.\n      size = null;\n    }\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator()).filter(x => tf.tidy(() => predicate(x)));\n    }, size);\n  }\n\n  /**\n   * Apply a function to every element of the dataset.\n   *\n   * After the function is applied to a dataset element, any Tensors contained\n   * within that element are disposed.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param f A function to apply to each dataset element.\n   * @returns A `Promise` that resolves after all elements have been processed.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  async forEachAsync(f: (input: T) => void): Promise<void> {\n    return (await this.iterator()).forEachAsync(f);\n  }\n\n  /** @deprecated Please use `dataset.forEachAsync()` instead. */\n  async forEach(f: (input: T) => void): Promise<void> {\n    tf.deprecationWarn(\n        'dataset.forEach() is deprecated and will be removed. ' +\n        'Please use dataset.forEachAsync() instead');\n    return this.forEachAsync(f);\n  }\n\n  /**\n   * Maps this dataset through a 1-to-1 transform.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).map(x => x*x);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param transform A function mapping a dataset element to a transformed\n   *   dataset element.\n   *\n   * @returns A `Dataset` of transformed elements.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  map<O extends DataElement>(transform: (value: T) => O): Dataset<O> {\n    const base = this;\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator()).map(x => tf.tidy(() => transform(x)));\n    }, this.size);\n  }\n\n  /**\n   * Maps this dataset through an async 1-to-1 transform.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).map(x => new Promise(function(resolve){\n   *  resolve(x*x);\n   * }));\n   * await a.forEachAsync(e => e.then(function(value){\n   *  console.log(value);\n   * }));\n   * ```\n   *\n   * @param transform A function mapping a dataset element to a `Promise` for a\n   *   transformed dataset element.  This transform is responsible for disposing\n   *   any intermediate `Tensor`s, i.e. by wrapping its computation in\n   *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\n   *   `map()` case).\n   *\n   * @returns A `Dataset` of transformed elements.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  mapAsync<O extends DataElement>(transform: (value: T) => Promise<O>):\n      Dataset<O> {\n    const base = this;\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator()).mapAsync(transform);\n    }, this.size);\n  }\n\n  /**\n   *  Creates a `Dataset` that prefetches elements from this dataset.\n   *\n   * @param bufferSize: An integer specifying the number of elements to be\n   *   prefetched.\n   * @returns A `Dataset`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  prefetch(bufferSize: number): Dataset<T> {\n    if (bufferSize == null) {\n      throw new RangeError(\n          '`Dataset.prefetch()` requires bufferSize to be specified.');\n    }\n\n    const base = this;\n    return datasetFromIteratorFn(\n        async () => (await base.iterator()).prefetch(bufferSize), this.size);\n  }\n\n  /**\n   * Repeats this dataset `count` times.\n   *\n   * NOTE: If this dataset is a function of global state (e.g. a random number\n   * generator), then different repetitions may produce different elements.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).repeat(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: (Optional) An integer, representing the number of times\n   *   the dataset should be repeated. The default behavior (if `count` is\n   *   `undefined` or negative) is for the dataset be repeated indefinitely.\n   * @returns A `Dataset`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  repeat(count?: number): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size != null && count > 0) {\n      // If this dataset has size and count is positive, new size is current\n      // size multiply count. This also covers the case that current size is\n      // infinity.\n      size = this.size * count;\n    } else if (count === 0) {\n      // If count is 0, new size is 0.\n      size = 0;\n    } else if (this.size != null && (count === undefined || count < 0)) {\n      // If this dataset has size and count is undefined or negative, the\n      // dataset will be repeated indefinitely and new size is infinity.\n      size = Infinity;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(async () => {\n      const iteratorIterator = iteratorFromFunction(\n          async () => ({value: await base.iterator(), done: false}));\n      return iteratorFromConcatenated(iteratorIterator.take(count));\n    }, size);\n  }\n\n  /**\n   * Creates a `Dataset` that skips `count` initial elements from this dataset.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: The number of elements of this dataset that should be skipped\n   *   to form the new dataset.  If `count` is greater than the size of this\n   *   dataset, the new dataset will contain no elements.  If `count`\n   *   is `undefined` or negative, skips the entire dataset.\n   *\n   * @returns A `Dataset`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  skip(count: number): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size != null && count >= 0 && this.size >= count) {\n      // If the size of this dataset is greater than count, the new dataset's\n      // size is current size minus skipped size.This also covers the case that\n      // current size is infinity.\n      size = this.size - count;\n    } else if (\n        this.size != null &&\n        (this.size < count || count === undefined || count < 0)) {\n      // If the size of this dataset is smaller than count, or count is\n      // undefined or negative, skips the entire dataset and the new size is 0.\n      size = 0;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(\n        async () => (await base.iterator()).skip(count), size);\n  }\n\n  // TODO(soergel): deep sharded shuffle, where supported\n\n  static readonly MAX_BUFFER_SIZE = 10000;\n\n  /**\n   * Pseudorandomly shuffles the elements of this dataset. This is done in a\n   * streaming manner, by sampling from a given number of prefetched elements.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param bufferSize: An integer specifying the number of elements from this\n   *   dataset from which the new dataset will sample.\n   * @param seed: (Optional) An integer specifying the random seed that will\n   *   be used to create the distribution.\n   * @param reshuffleEachIteration: (Optional) A boolean, which if true\n   *   indicates that the dataset should be pseudorandomly reshuffled each time\n   *   it is iterated over. If false, elements will be returned in the same\n   *   shuffled order on each iteration. (Defaults to `true`.)\n   * @returns A `Dataset`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  shuffle(bufferSize: number, seed?: string, reshuffleEachIteration = true):\n      Dataset<T> {\n    if (bufferSize == null || bufferSize < 0) {\n      if (this.size == null) {\n        throw new RangeError(\n            '`Dataset.shuffle()` requires bufferSize to be specified.');\n      } else {\n        throw new RangeError(\n            '`Dataset.shuffle()` requires bufferSize to be specified.  ' +\n            'If your data fits in main memory (for regular JS objects), ' +\n            'and/or GPU memory (for `tf.Tensor`s), consider setting ' +\n            `bufferSize to the dataset size (${this.size} elements)`);\n      }\n    }\n    const base = this;\n    const random = seedrandom.alea(seed || tf.util.now().toString());\n    return datasetFromIteratorFn(async () => {\n      let seed2 = random.int32();\n      if (reshuffleEachIteration) {\n        seed2 += random.int32();\n      }\n      return (await base.iterator()).shuffle(bufferSize, seed2.toString());\n    }, this.size);\n  }\n\n  /**\n   * Creates a `Dataset` with at most `count` initial elements from this\n   * dataset.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: The number of elements of this dataset that should be taken\n   *   to form the new dataset.  If `count` is `undefined` or negative, or if\n   *   `count` is greater than the size of this dataset, the new dataset will\n   *   contain all elements of this dataset.\n   * @returns A `Dataset`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  take(count: number): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size != null && this.size > count) {\n      // If the size of this dataset is greater than count, the new dataset's\n      // size is count.\n      size = count;\n    } else if (this.size != null && this.size <= count) {\n      // If the size of this dataset is equal or smaller than count, the new\n      // dataset's size is the size of this dataset.\n      size = this.size;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(\n        async () => (await base.iterator()).take(count), size);\n  }\n\n  /**\n   * Collect all elements of this dataset into an array.\n   *\n   * Obviously this will succeed only for small datasets that fit in memory.\n   * Useful for testing and generally should be avoided if possible.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\n   * console.log(await a.toArray());\n   * ```\n   *\n   * @returns A Promise for an array of elements, which will resolve\n   *   when a new stream has been obtained and fully consumed.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  async toArray() {\n    if (this.size === Infinity) {\n      throw new Error('Can not convert infinite data stream to array.');\n    }\n    return (await this.iterator()).toArray();\n  }\n\n  /**\n   * Collect all elements of this dataset into an array with prefetching 100\n   * elements. This is useful for testing, because the prefetch changes the\n   * order in which the Promises are resolved along the processing pipeline.\n   * This may help expose bugs where results are dependent on the order of\n   * Promise resolution rather than on the logical order of the stream (i.e.,\n   * due to hidden mutable state).\n   *\n   * @returns A Promise for an array of elements, which will resolve\n   *   when a new stream has been obtained and fully consumed.\n   */\n  async toArrayForTest() {\n    if (this.size === Infinity) {\n      throw new Error('Can not convert infinite data stream to array.');\n    }\n    return (await this.iterator()).toArrayForTest();\n  }\n}\n\n/**\n * Create a `Dataset` defined by a provided iterator() function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * const ds = tf.data.datasetFromIteratorFn(iter);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n */\nexport function datasetFromIteratorFn<T extends DataElement>(\n    iteratorFn: () => Promise<LazyIterator<T>>,\n    size: number = null): Dataset<T> {\n  return new class extends Dataset<T> {\n    size = size;\n\n    /*\n     * Provide a new stream of elements.  Note this will also start new streams\n     * from any underlying `Dataset`s.\n     */\n    async iterator(): Promise<LazyIterator<T>> {\n      return iteratorFn();\n    }\n  }\n  ();\n}\n\n/**\n * Create a `Dataset` from an array of elements.\n *\n * Create a Dataset from an array of objects:\n * ```js\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n *\n * Create a Dataset from an array of numbers:\n * ```js\n * const a = tf.data.array([4, 5, 6]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n * @param items An array of elements that will be parsed as items in a dataset.\n */\n/** @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'} */\nexport function array<T extends DataElement>(items: T[]): Dataset<T> {\n  return datasetFromIteratorFn(\n      async () => iteratorFromItems(items), items.length);\n}\n\n/**\n * Create a `Dataset` by zipping together an array, dict, or nested\n * structure of `Dataset`s (and perhaps additional constants).\n * The underlying datasets must provide elements in a consistent order such that\n * they correspond.\n *\n * The number of elements in the resulting dataset is the same as the size of\n * the smallest dataset in datasets.\n *\n * The nested structure of the `datasets` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Note this means that, given an array of two datasets that produce dict\n * elements, the result is a dataset that produces elements that are arrays\n * of two dicts:\n *\n * Zip an array of datasets:\n * ```js\n * console.log('Zip two datasets of objects:');\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const ds3 = tf.data.zip([ds1, ds2]);\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n *\n * // If the goal is to merge the dicts in order to produce elements like\n * // {a: ..., b: ...}, this requires a second step such as:\n * console.log('Merge the objects:');\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\n * await ds4.forEachAsync(e => console.log(e));\n * ```\n *\n * Zip a dict of datasets:\n * ```js\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const c = tf.data.zip({c: a, d: b});\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\n * ```\n */\n/** @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'} */\nexport function zip<O extends DataElement>(datasets: DatasetContainer):\n    Dataset<O> {\n  // manually type-check the argument for JS users\n  if (!isIterable(datasets)) {\n    throw new Error('The argument to zip() must be an object or array.');\n  }\n  let size;\n  if (Array.isArray(datasets)) {\n    for (let i = 0; i < datasets.length; i++) {\n      size = size == null ? (datasets[i] as Dataset<O>).size :\n                            Math.min(size, (datasets[i] as Dataset<O>).size);\n    }\n  } else if (datasets instanceof Object) {\n    for (const ds in datasets) {\n      size = size == null ? (datasets[ds] as Dataset<O>).size :\n                            Math.min(size, (datasets[ds] as Dataset<O>).size);\n    }\n  }\n  return datasetFromIteratorFn<O>(async () => {\n    const streams = await deepMapAndAwaitAll(datasets, d => {\n      if (d instanceof Dataset) {\n        return {value: d.iterator(), recurse: false};\n      } else if (isIterable(d)) {\n        return {value: null, recurse: true};\n      } else {\n        throw new Error(\n            'Leaves of the structure passed to zip() must be Datasets, ' +\n            'not primitives.');\n      }\n    });\n    return iteratorFromZipped<O>(streams, ZipMismatchMode.SHORTEST);\n  }, size);\n}\n\n/**\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\n *\n * Accepts an array of identically-structured nested elements and either batches\n * them (if they are primitives, numeric arrays, or Tensors) or requests\n * recursion (if not).\n */\n// tslint:disable-next-line:no-any\nfunction deepBatchConcat(rows: any[]): DeepMapResult {\n  if (rows === null) {\n    return null;\n  }\n\n  // use the first item to decide whether to recurse or batch here.\n  const exampleRow = rows[0];\n\n  if (canTensorify(exampleRow)) {\n    // rows is an array of primitives, Tensors, or arrays.  Batch them.\n    const value = batchConcat(rows);\n    return {value, recurse: false};\n  }\n\n  // the example row is an object, so recurse into it.\n  return {value: null, recurse: true};\n}\n\n/**\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\n * into a single new Tensor where axis 0 is the batch dimension.\n */\nfunction batchConcat<T extends(TensorLike | tf.Tensor)>(arrays: T[]):\n    tf.Tensor {\n  if (arrays.length === 0) {\n    // We can't return an empty Tensor because we don't know the element shape.\n    throw new Error('Can\\'t make a batch of zero elements.');\n  }\n\n  if (arrays[0] instanceof tf.Tensor) {\n    // Input is an array of Tensors\n    return tf.stack(arrays as tf.Tensor[]);\n  } else {\n    // Input is a possibly-nested array of numbers.\n    return tf.tensor(arrays as TensorLike);\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {Dataset} from '../dataset';\nimport {DataSource} from '../datasource';\nimport {LazyIterator} from '../iterators/lazy_iterator';\n\n/**\n * Represents a potentially large collection of text lines.\n *\n * The results are not batched.\n */\nexport class TextLineDataset extends Dataset<string> {\n  /**\n   * Create a `TextLineDataset`.\n   *\n   * @param input A `DataSource` providing a chunked, UTF8-encoded byte stream.\n   */\n  constructor(protected readonly input: DataSource) {\n    super();\n  }\n\n  async iterator(): Promise<LazyIterator<string>> {\n    const inputIterator = await this.input.iterator();\n    const utf8Iterator = inputIterator.decodeUTF8();\n    const lineIterator = utf8Iterator.split('\\n');\n    return lineIterator;\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {util} from '@tensorflow/tfjs-core';\nimport {Dataset} from '../dataset';\nimport {DataSource} from '../datasource';\nimport {LazyIterator} from '../iterators/lazy_iterator';\nimport {ColumnConfig, CSVConfig, DataElement} from '../types';\nimport {TextLineDataset} from './text_line_dataset';\n\nconst CODE_QUOTE = '\"';\nconst STATE_OUT = Symbol('out');\nconst STATE_FIELD = Symbol('field');\nconst STATE_QUOTE = Symbol('quote');\nconst STATE_QUOTE_AFTER_QUOTE = Symbol('quoteafterquote');\nconst STATE_WITHIN_QUOTE_IN_QUOTE = Symbol('quoteinquote');\n\n/**\n * Represents a potentially large collection of delimited text records.\n *\n * The produced `DataElement`s each contain one key-value pair for\n * every column of the table.  When a field is empty in the incoming data, the\n * resulting value is `undefined`, or throw error if it is required.  Values\n * that can be parsed as numbers are emitted as type `number`, other values\n * are parsed as `string`.\n *\n * The results are not batched.\n */\n/** @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'} */\nexport class CSVDataset extends Dataset<DataElement> {\n  base: TextLineDataset;\n  private hasHeader = true;\n  private fullColumnNames: string[] = null;\n  private columnNamesValidated = false;\n  private columnConfigs: {[key: string]: ColumnConfig} = null;\n  private configuredColumnsOnly = false;\n  private delimiter = ',';\n\n  /**\n   * Returns column names of the csv dataset. If `configuredColumnsOnly` is\n   * true, return column names in `columnConfigs`. If `configuredColumnsOnly` is\n   * false and `columnNames` is provided, `columnNames`. If\n   * `configuredColumnsOnly` is false and `columnNames` is not provided, return\n   * all column names parsed from the csv file. For example usage please go to\n   * `tf.data.csv`.\n   */\n  /** @doc {heading: 'Data', subheading: 'Classes'} */\n  async columnNames() {\n    if (!this.columnNamesValidated) {\n      await this.setColumnNames();\n    }\n    return this.configuredColumnsOnly ? Object.keys(this.columnConfigs) :\n                                        this.fullColumnNames;\n  }\n\n  /* 1) If `columnNames` is provided as string[], use this string[] as output\n   * keys in corresponding order. The length must match the number of inferred\n   * columns if `hasHeader` is true .\n   * 2) If `columnNames` is not provided, parse header line as `columnNames` if\n   * hasHeader is true. If `hasHeader` is false, throw an error.\n   * 3) If `columnConfigs` is provided, all the keys in `columnConfigs` must\n   * exist in parsed `columnNames`.\n   */\n  private async setColumnNames() {\n    const columnNamesFromFile = await this.maybeReadHeaderLine();\n    if (!this.fullColumnNames && !columnNamesFromFile) {\n      // Throw an error if columnNames is not provided and no header line.\n      throw new Error(\n          'Column names must be provided if there is no header line.');\n    } else if (this.fullColumnNames && columnNamesFromFile) {\n      // Check provided columnNames match header line.\n      util.assert(\n          columnNamesFromFile.length === this.fullColumnNames.length,\n          () => 'The length of provided columnNames (' +\n              this.fullColumnNames.length.toString() +\n              ') does not match the length of the header line read from ' +\n              'file (' + columnNamesFromFile.length.toString() + ').');\n    }\n    if (!this.fullColumnNames) {\n      this.fullColumnNames = columnNamesFromFile;\n    }\n    // Check if there are duplicate column names.\n    const counts: {[key: string]: number} = this.fullColumnNames.reduce(\n        (countAcc: {[key: string]: number}, name) => {\n          countAcc[name] = (countAcc[name] + 1) || 1;\n          return countAcc;\n        },\n        {});\n    const duplicateNames =\n        Object.keys(counts).filter((name) => (counts[name] > 1));\n    util.assert(\n        duplicateNames.length === 0,\n        () => 'Duplicate column names found: ' + duplicateNames.toString());\n    // Check if keys in columnConfigs match columnNames.\n    if (this.columnConfigs) {\n      for (const key of Object.keys(this.columnConfigs)) {\n        const index = this.fullColumnNames.indexOf(key);\n        if (index === -1) {\n          throw new Error(\n              'The key \"' + key +\n              '\" provided in columnConfigs does not match any of the column ' +\n              'names (' + this.fullColumnNames.toString() + ').');\n        }\n      }\n    }\n    this.columnNamesValidated = true;\n  }\n\n  private async maybeReadHeaderLine() {\n    if (this.hasHeader) {\n      const iter = await this.base.iterator();\n      const firstElement = await iter.next();\n      if (firstElement.done) {\n        throw new Error('No data was found for CSV parsing.');\n      }\n      const firstLine: string = firstElement.value;\n      return firstLine.split(this.delimiter);\n    } else {\n      return null;\n    }\n  }\n\n  /**\n   * Create a `CSVDataset`.\n   *\n   * @param input A `DataSource` providing a chunked, UTF8-encoded byte stream.\n   * @param csvConfig (Optional) A CSVConfig object that contains configurations\n   *     of reading and decoding from CSV file(s).\n   *\n   *     hasHeader: (Optional) A boolean value that indicates whether the first\n   *     row of provided CSV file is a header line with column names, and should\n   *     not be included in the data. Defaults to `true`.\n   *\n   *     columnNames: (Optional) A list of strings that corresponds to\n   *     the CSV column names, in order. If provided, it ignores the column\n   *     names inferred from the header row. If not provided, infers the column\n   *     names from the first row of the records. If hasHeader is false and\n   *     columnNames is not provided, this method throws an error.\n   *\n   *     columnConfigs: (Optional) A dictionary whose key is column names, value\n   *     is an object stating if this column is required, column's data type,\n   *     default value, and if this column is label. If provided, keys must\n   *     correspond to names provided in columnNames or inferred from the file\n   *     header lines. If isLabel is true any column, returns an array of two\n   *     items: the first item is a dict of features key/value pairs, the second\n   *     item is a dict of labels key/value pairs. If no feature is marked as\n   *     label, returns a dict of features only.\n   *\n   *     configuredColumnsOnly (Optional) If true, only columns provided in\n   *     columnConfigs will be parsed and provided during iteration.\n   *\n   *     delimiter (Optional) The string used to parse each line of the input\n   *     file. Defaults to `,`.\n   */\n  constructor(protected readonly input: DataSource, csvConfig?: CSVConfig) {\n    super();\n    this.base = new TextLineDataset(input);\n    if (!csvConfig) {\n      csvConfig = {};\n    }\n    this.hasHeader = csvConfig.hasHeader === false ? false : true;\n    this.fullColumnNames = csvConfig.columnNames;\n    this.columnConfigs = csvConfig.columnConfigs;\n    this.configuredColumnsOnly = csvConfig.configuredColumnsOnly;\n    this.delimiter = csvConfig.delimiter ? csvConfig.delimiter : ',';\n  }\n\n  async iterator(): Promise<LazyIterator<DataElement>> {\n    if (!this.columnNamesValidated) {\n      await this.setColumnNames();\n    }\n    let lines = await this.base.iterator();\n    if (this.hasHeader) {\n      // We previously read the first line to get the columnNames.\n      // Now that we're providing data, skip it.\n      lines = lines.skip(1);\n    }\n    return lines.map(x => this.makeDataElement(x));\n  }\n\n  makeDataElement(line: string): DataElement {\n    const values = this.parseRow(line);\n    const features: {[key: string]: DataElement} = {};\n    const labels: {[key: string]: DataElement} = {};\n\n    for (let i = 0; i < this.fullColumnNames.length; i++) {\n      const key = this.fullColumnNames[i];\n      const config = this.columnConfigs ? this.columnConfigs[key] : null;\n      if (this.configuredColumnsOnly && !config) {\n        // This column is not selected.\n        continue;\n      } else {\n        const value = values[i];\n        let parsedValue = null;\n        if (value === '') {\n          // If default value is provided, use it. If default value is not\n          // provided, set as undefined.\n          if (config && config.default !== undefined) {\n            parsedValue = config.default;\n          } else if (config && (config.required || config.isLabel)) {\n            throw new Error(\n                `Required column ${key} is empty in this line: ${line}`);\n          } else {\n            parsedValue = undefined;\n          }\n        } else {\n          // A value is present, so parse it based on type\n          const valueAsNum = Number(value);\n          if (isNaN(valueAsNum)) {\n            // The value is a string and this column is declared as boolean\n            // in config, parse it as boolean.\n            if (config && config.dtype === 'bool') {\n              parsedValue = this.getBoolean(value);\n            } else {\n              // Set value as string\n              parsedValue = value as string;\n            }\n          } else if (!config || !config.dtype) {\n            // If this value is a number and no type config is provided, return\n            // it as number.\n            parsedValue = valueAsNum;\n          } else {\n            // If this value is a number and data type is provided, parse it\n            // according to provided data type.\n            switch (config.dtype) {\n              case 'float32':\n                parsedValue = valueAsNum;\n                break;\n              case 'int32':\n                parsedValue = Math.floor(valueAsNum);\n                break;\n              case 'bool':\n                parsedValue = this.getBoolean(value);\n                break;\n              default:\n                parsedValue = valueAsNum;\n            }\n          }\n        }\n        // Check if this column is label.\n        (config && config.isLabel) ? labels[key] = parsedValue :\n                                     features[key] = parsedValue;\n      }\n    }\n    // If label exists, return an object of features and labels as {xs:features,\n    // ys:labels}, otherwise return features only.\n    if (Object.keys(labels).length === 0) {\n      return features;\n\n    } else {\n      return {xs: features, ys: labels};\n    }\n  }\n\n  private getBoolean(value: string): number {\n    if (value === '1' || value.toLowerCase() === 'true') {\n      return 1;\n    } else {\n      return 0;\n    }\n  }\n\n  // adapted from https://beta.observablehq.com/@mbostock/streaming-csv\n  private parseRow(line: string): string[] {\n    const result: string[] = [];\n    let readOffset = 0;\n    const readLength = line.length;\n    let currentState = STATE_FIELD;\n    // Goes through the line to parse quote.\n    for (let i = 0; i < readLength; i++) {\n      switch (currentState) {\n        // Before enter a new field\n        case STATE_OUT:\n          switch (line.charAt(i)) {\n            // Enter a quoted field\n            case CODE_QUOTE:\n              readOffset = i + 1;\n              currentState = STATE_QUOTE;\n              break;\n            // Read an empty field\n            case this.delimiter:\n              result.push('');\n              currentState = STATE_OUT;\n              readOffset = i + 1;\n              break;\n            // Enter an unquoted field\n            default:\n              currentState = STATE_FIELD;\n              readOffset = i;\n              break;\n          }\n          break;\n        // In an unquoted field\n        case STATE_FIELD:\n          switch (line.charAt(i)) {\n            // Exit an unquoted field, add it to result\n            case this.delimiter:\n              result.push(line.substring(readOffset, i));\n              currentState = STATE_OUT;\n              readOffset = i + 1;\n              break;\n            default:\n          }\n          break;\n        // In a quoted field\n        case STATE_QUOTE:\n          switch (line.charAt(i)) {\n            // Read a quote after a quote\n            case CODE_QUOTE:\n              currentState = STATE_QUOTE_AFTER_QUOTE;\n              break;\n            default:\n          }\n          break;\n        // This state means it's right after a second quote in a field\n        case STATE_QUOTE_AFTER_QUOTE:\n          switch (line.charAt(i)) {\n            // Finished a quoted field\n            case this.delimiter:\n              result.push(line.substring(readOffset, i - 1));\n              currentState = STATE_OUT;\n              readOffset = i + 1;\n              break;\n            // Finished a quoted part in a quoted field\n            case CODE_QUOTE:\n              currentState = STATE_QUOTE;\n              break;\n            // In a quoted part in a quoted field\n            default:\n              currentState = STATE_WITHIN_QUOTE_IN_QUOTE;\n              break;\n          }\n          break;\n        case STATE_WITHIN_QUOTE_IN_QUOTE:\n          switch (line.charAt(i)) {\n            // Exit a quoted part in a quoted field\n            case CODE_QUOTE:\n              currentState = STATE_QUOTE;\n              break;\n            default:\n          }\n          break;\n        default:\n      }\n    }\n    // Adds last item based on if it is quoted.\n    if (currentState === STATE_QUOTE_AFTER_QUOTE) {\n      result.push(line.substring(readOffset, readLength - 1));\n    } else {\n      result.push(line.substring(readOffset));\n    }\n    return result;\n  }\n}\n\n// TODO(soergel): add more basic datasets for parity with tf.data\n// tf.data.FixedLengthRecordDataset()\n// tf.data.TFRecordDataset()\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {ByteChunkIterator} from './iterators/byte_chunk_iterator';\n\n/**\n * Represents a data source readable as a stream of binary data chunks.\n *\n * Because `Dataset`s can be read repeatedly (via `Dataset.iterator()`), this\n * provides a means to repeatedly create streams from the underlying data\n * sources.\n */\nexport abstract class DataSource {\n  /**\n   * Obtain a new stream of binary data chunks.\n   *\n   * Starts the new stream from the beginning of the data source, even if other\n   * streams have been obtained previously.\n   */\n  abstract async iterator(): Promise<ByteChunkIterator>;\n\n  // TODO(soergel): consider chainable Dataset construction here\n}\n\n// TODO(soergel): consider convenience factory functions here\n// in combination with chainable source->dataset above, e.g.:\n// tf.data.url(...).asCsvDataset().shuffle().batch()\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {LazyIterator, OneToManyIterator} from './lazy_iterator';\n\nexport abstract class StringIterator extends LazyIterator<string> {\n  /**\n   * Splits a string stream on a given separator.\n   *\n   * It is assumed that the incoming chunk boundaries have no semantic meaning,\n   * so conceptually the incoming stream is treated simply as the concatenation\n   * of its elements.\n   *\n   * The outgoing stream provides chunks corresponding to the results of the\n   * standard string split() operation (even if such a chunk spanned incoming\n   * chunks).  The separators are not included.\n   *\n   * A typical usage is to split a text file (represented as a stream with\n   * arbitrary chunk boundaries) into lines.\n   *\n   * @param upstream A readable stream of strings that can be treated as\n   *   concatenated.\n   * @param separator A character to split on.\n   */\n  split(separator: string): StringIterator {\n    return new SplitIterator(this, separator);\n  }\n}\n\n// ============================================================================\n// The following private classes serve to implement the chainable methods\n// on StringIterator.  Unfortunately they can't be placed in separate files, due\n// to resulting trouble with circular imports.\n// ============================================================================\n\n// We wanted multiple inheritance, e.g.\n//   class SplitIterator extends QueueIterator<string>, StringIterator\n// but the TypeScript mixin approach is a bit hacky, so we take this adapter\n// approach instead.\n\nclass SplitIterator extends StringIterator {\n  private impl: SplitIteratorImpl;\n\n  constructor(protected upstream: LazyIterator<string>, separator: string) {\n    super();\n    this.impl = new SplitIteratorImpl(upstream, separator);\n  }\n\n  summary() {\n    return this.impl.summary();\n  }\n\n  async next() {\n    return this.impl.next();\n  }\n}\n\nclass SplitIteratorImpl extends OneToManyIterator<string> {\n  // A partial string at the end of an upstream chunk\n  carryover = '';\n\n  constructor(\n      protected upstream: LazyIterator<string>, protected separator: string) {\n    super();\n  }\n\n  summary() {\n    return `${this.upstream.summary()} -> Split('${this.separator}')`;\n  }\n\n  async pump(): Promise<boolean> {\n    const chunkResult = await this.upstream.next();\n    if (chunkResult.done) {\n      if (this.carryover === '') {\n        return false;\n      }\n\n      // Pretend that the pump succeeded in order to emit the small last batch.\n      // The next pump() call will actually fail.\n      this.outputQueue.push(this.carryover);\n      this.carryover = '';\n      return true;\n    }\n    const lines = chunkResult.value.split(this.separator);\n    // Note the behavior: \" ab \".split(' ') === ['', 'ab', '']\n    // Thus the carryover may be '' if the separator falls on a chunk\n    // boundary; this produces the correct result.\n\n    lines[0] = this.carryover + lines[0];\n    for (const line of lines.slice(0, -1)) {\n      this.outputQueue.push(line);\n    }\n    this.carryover = lines[lines.length - 1];\n\n    return true;\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {ENV} from '@tensorflow/tfjs-core';\nimport {LazyIterator, OneToManyIterator} from './lazy_iterator';\nimport {StringIterator} from './string_iterator';\n\nexport abstract class ByteChunkIterator extends LazyIterator<Uint8Array> {\n  /**\n   * Decode a stream of UTF8-encoded byte arrays to a stream of strings.\n   *\n   * The byte arrays producetd from the ByteChunkIterator on which this is\n   * called will be interpreted as concatenated.  No assumptions are made about\n   * the boundaries of the incoming chunks, so a multi-byte UTF8 encoding of a\n   * character may span the boundary between chunks.  This naturally happens,\n   * for instance, when reading fixed-size byte arrays from a file.\n   */\n  decodeUTF8(): StringIterator {\n    return new Utf8Iterator(this);\n  }\n}\n\n// ============================================================================\n// The following private classes serve to implement the chainable methods\n// on ByteChunkIterator.  Unfortunately they can't be placed in separate files,\n// due to resulting trouble with circular imports.\n// ============================================================================\n\n// We wanted multiple inheritance, e.g.\n//   class Utf8Iterator extends QueueIterator<string>, StringIterator\n// but the TypeScript mixin approach is a bit hacky, so we take this adapter\n// approach instead.\n\nclass Utf8Iterator extends StringIterator {\n  private impl: Utf8IteratorImpl;\n\n  constructor(protected upstream: LazyIterator<Uint8Array>) {\n    super();\n    this.impl = new Utf8IteratorImpl(upstream);\n  }\n\n  summary() {\n    return this.impl.summary();\n  }\n\n  async next() {\n    return this.impl.next();\n  }\n}\n\n/**\n * Decode a stream of UTF8-encoded byte arrays to a stream of strings.\n *\n * This is tricky because the incoming byte array boundaries may disrupt a\n * multi-byte UTF8 character. Thus any incomplete character data at the end of\n * a chunk must be carried over and prepended to the next chunk before\n * decoding. Luckily with native decoder, TextDecoder in browser and\n * string_decoder in node, byte array boundaries are handled automatically.\n *\n * In the context of an input pipeline for machine learning, UTF8 decoding is\n * needed to parse text files containing training examples or prediction\n * requests (e.g., formatted as CSV or JSON). We cannot use the built-in\n * decoding provided by FileReader.readAsText() because here we are in a\n * streaming context, which FileReader does not support.\n *\n * @param upstream A `LazyIterator` of `Uint8Arrays` containing UTF8-encoded\n *   text, which should be interpreted as concatenated.  No assumptions are\n *   made about the boundaries of the incoming chunks, so a multi-byte UTF8\n *   encoding of a character may span the boundary between chunks.  This\n *   naturally happens, for instance, when reading fixed-size byte arrays from a\n *   file.\n */\nclass Utf8IteratorImpl extends OneToManyIterator<string> {\n  // `decoder` as `any` here to dynamically assign value based on ENV.\n  // tslint:disable-next-line:no-any\n  decoder:any;\n\n  constructor(protected readonly upstream: LazyIterator<Uint8Array>) {\n    super();\n    if (ENV.get('IS_BROWSER')) {\n      this.decoder = new TextDecoder('utf-8');\n    } else {\n      // tslint:disable-next-line:no-require-imports\n      const { StringDecoder } = require('string_decoder');\n      this.decoder = new StringDecoder('utf8');\n    }\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Utf8`;\n  }\n\n  async pump(): Promise<boolean> {\n    const chunkResult = await this.upstream.next();\n    let chunk;\n    if (chunkResult.done) {\n        return false;\n    } else {\n      chunk = chunkResult.value;\n    }\n\n    let text: string;\n    if (ENV.get('IS_BROWSER')) {\n      text = this.decoder.decode(chunk, {stream:true});\n    } else {\n      text = this.decoder.write(Buffer.from(chunk.buffer));\n    }\n    this.outputQueue.push(text);\n    return true;\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\n// inspired by https://github.com/maxogden/filereader-stream\nimport {ENV, util} from '@tensorflow/tfjs-core';\nimport {FileElement} from '../types';\nimport {ByteChunkIterator} from './byte_chunk_iterator';\n\nexport interface FileChunkIteratorOptions {\n  /** The byte offset at which to begin reading the File or Blob. Default 0. */\n  offset?: number;\n  /** The number of bytes to read at a time. Default 1MB. */\n  chunkSize?: number;\n}\n\n/**\n * Provide a stream of chunks from a File, Blob, or Uint8Array.\n * @param file The source File, Blob or Uint8Array.\n * @param options Optional settings controlling file reading.\n * @returns a lazy Iterator of Uint8Arrays containing sequential chunks of the\n *   input File, Blob or Uint8Array.\n */\nexport class FileChunkIterator extends ByteChunkIterator {\n  offset: number;\n  chunkSize: number;\n\n  constructor(\n      protected file: FileElement,\n      protected options: FileChunkIteratorOptions = {}) {\n    super();\n    util.assert(\n        (file instanceof Uint8Array) ||\n            (ENV.get('IS_BROWSER') ?\n                 (file instanceof File || file instanceof Blob) :\n                 false),\n        () => 'FileChunkIterator only supports File, Blob and Uint8Array ' +\n            'right now.');\n    this.offset = options.offset || 0;\n    // default 1MB chunk has tolerable perf on large files\n    this.chunkSize = options.chunkSize || 1024 * 1024;\n  }\n\n  summary() {\n    return `FileChunks ${this.file}`;\n  }\n\n  async next(): Promise<IteratorResult<Uint8Array>> {\n    if (this.offset >= ((this.file instanceof Uint8Array) ?\n                            this.file.byteLength :\n                            this.file.size)) {\n      return {value: null, done: true};\n    }\n    const chunk = new Promise<Uint8Array>((resolve, reject) => {\n      const end = this.offset + this.chunkSize;\n      if (this.file instanceof Uint8Array) {\n        // Note if end > this.uint8Array.byteLength, we just get a small last\n        // chunk.\n        resolve(new Uint8Array(this.file.slice(this.offset, end)));\n      } else {\n        // This branch assumes that this.file type is File or Blob, which\n        // means it is in the browser environment.\n\n        // TODO(soergel): is this a performance issue?\n        const fileReader = new FileReader();\n        fileReader.onload = (event) => {\n          let data: string|ArrayBuffer|Uint8Array = fileReader.result;\n          // Not sure we can trust the return type of\n          // FileReader.readAsArrayBuffer See e.g.\n          // https://github.com/node-file-api/FileReader/issues/2\n          if (data instanceof ArrayBuffer) {\n            data = new Uint8Array(data);\n          }\n          if (!(data instanceof Uint8Array)) {\n            return reject(new TypeError('FileReader returned unknown type.'));\n          }\n          resolve(data);\n        };\n        fileReader.onabort = (event) => {\n          return reject(new Error('Aborted'));\n        };\n        fileReader.onerror = (event) => {\n          return reject(new Error(event.type));\n        };\n        // TODO(soergel): better handle onabort, onerror\n        // Note if end > this.file.size, we just get a small last chunk.\n        const slice = this.file.slice(this.offset, end);\n        // We can't use readAsText here (even if we know the file is text)\n        // because the slice boundary may fall within a multi-byte character.\n        fileReader.readAsArrayBuffer(slice);\n      }\n      this.offset = end;\n    });\n    return {value: (await chunk), done: false};\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {ENV} from '@tensorflow/tfjs-core';\nimport {FileChunkIterator, FileChunkIteratorOptions} from './file_chunk_iterator';\n\n/**\n * Provide a stream of chunks from a URL.\n *\n * Note this class first downloads the entire file into memory before providing\n * the first element from the stream.  This is because the Fetch API does not\n * yet reliably provide a reader stream for the response body.\n */\nexport async function urlChunkIterator(\n    url: RequestInfo, options: FileChunkIteratorOptions = {}) {\n  let response;\n  if (ENV.get('IS_BROWSER')) {\n    response = await fetch(url);\n    if (response.ok) {\n      const blob = await response.blob();\n      return new FileChunkIterator(blob, options);\n    } else {\n      throw new Error(response.statusText);\n    }\n  } else {\n    // TODO(kangyizhang): Provide argument for users to use http.request with\n    // headers in node.\n    // tslint:disable-next-line:no-require-imports\n    const nodeFetch = require('node-fetch');\n    if (typeof url !== 'string') {\n      throw new Error(\n          'URL must be a string. Request objects are not supported ' +\n          'in the node.js environment yet.');\n    }\n    response = await nodeFetch(url);\n    if (response.ok) {\n      const unitArray = await response.buffer();\n      return new FileChunkIterator(unitArray, options);\n    } else {\n      throw new Error(response.statusText);\n    }\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\n// Skip tslint any type check cause this method is aiming to check type of\n// input.\n// tslint:disable-next-line:no-any\nexport function isLocalPath(source: any): boolean {\n  return (typeof source === 'string') && source.substr(0, 7) === 'file://';\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {ENV} from '@tensorflow/tfjs-core';\nimport {DataSource} from '../datasource';\nimport {ByteChunkIterator} from '../iterators/byte_chunk_iterator';\nimport {FileChunkIterator, FileChunkIteratorOptions} from '../iterators/file_chunk_iterator';\nimport {FileElement} from '../types';\nimport {isLocalPath} from '../util/source_util';\n\n/**\n * Represents a file, blob, or Uint8Array readable as a stream of binary data\n * chunks.\n */\nexport class FileDataSource extends DataSource {\n  /**\n   * Create a `FileDataSource`.\n   *\n   * @param input Local file path, or `File`/`Blob`/`Uint8Array` object to\n   *     read. Local file only works in node environment.\n   * @param options Options passed to the underlying `FileChunkIterator`s,\n   *   such as {chunksize: 1024}.\n   */\n  constructor(\n      protected input: FileElement|string,\n      protected readonly options: FileChunkIteratorOptions = {}) {\n    super();\n  }\n\n  async iterator(): Promise<ByteChunkIterator> {\n    if (isLocalPath(this.input) && ENV.get('IS_NODE')) {\n      // tslint:disable-next-line:no-require-imports\n      const fs = require('fs');\n      this.input = fs.readFileSync((this.input as string).substr(7));\n    }\n    // TODO(kangyizhang): Add LocalFileChunkIterator to split local streaming\n    // with file in browser.\n    return new FileChunkIterator(this.input as FileElement, this.options);\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {DataSource} from '../datasource';\nimport {ByteChunkIterator} from '../iterators/byte_chunk_iterator';\nimport {FileChunkIteratorOptions} from '../iterators/file_chunk_iterator';\nimport {urlChunkIterator} from '../iterators/url_chunk_iterator';\nimport {isLocalPath} from '../util/source_util';\nimport {FileDataSource} from './file_data_source';\n\n/*\n * Represents a URL readable as a stream of binary data chunks.\n */\nexport class URLDataSource extends DataSource {\n  /**\n   * Create a `URLDataSource`.\n   *\n   * @param url A source URL string, or a `Request` object.\n   * @param options Options passed to the underlying `FileChunkIterator`s,\n   *   such as {chunksize: 1024}.\n   */\n  constructor(\n      protected readonly url: RequestInfo,\n      protected readonly fileOptions: FileChunkIteratorOptions = {}) {\n    super();\n  }\n\n  // TODO(soergel): provide appropriate caching options.  Currently this\n  // will download the URL anew for each call to iterator().  Since we have\n  // to treat the downloaded file as a blob/buffer anyway, we may as well retain\n  // it-- but that raises GC issues.  Also we may want a persistent disk cache.\n  async iterator(): Promise<ByteChunkIterator> {\n    if (isLocalPath(this.url)) {\n      return (new FileDataSource(this.url as string, this.fileOptions))\n          .iterator();\n    } else {\n      return urlChunkIterator(this.url, this.fileOptions);\n    }\n  }\n}\n","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {Dataset, datasetFromIteratorFn} from './dataset';\nimport {CSVDataset} from './datasets/csv_dataset';\nimport {iteratorFromFunction} from './iterators/lazy_iterator';\nimport {URLDataSource} from './sources/url_data_source';\nimport {CSVConfig, DataElement} from './types';\n\n/**\n * Create a `CSVDataset` by reading and decoding CSV file(s) from provided URL\n * or local path if it's in Node environment.\n *\n * Note: If isLabel in columnConfigs is `true` for at least one column, the\n * element in returned `CSVDataset` will be an object of\n * `{xs:features, ys:labels}`: xs is a dict of features key/value pairs, ys\n * is a dict of labels key/value pairs. If no column is marked as label,\n * returns a dict of features only.\n *\n * ```js\n * const csvUrl =\n * 'https://storage.googleapis.com/tfjs-examples/multivariate-linear-regression/data/boston-housing-train.csv';\n *\n * async function run() {\n *   // We want to predict the column \"medv\", which represents a median value of\n *   // a home (in $1000s), so we mark it as a label.\n *   const csvDataset = tf.data.csv(\n *     csvUrl, {\n *       columnConfigs: {\n *         medv: {\n *           isLabel: true\n *         }\n *       }\n *     });\n *\n *   // Number of features is the number of column names minus one for the label\n *   // column.\n *   const numOfFeatures = (await csvDataset.columnNames()).length - 1;\n *\n *   // Prepare the Dataset for training.\n *   const flattenedDataset =\n *     csvDataset\n *     .map(({xs, ys}) =>\n *       {\n *         // Convert xs(features) and ys(labels) from object form (keyed by\n *         // column name) to array form.\n *         return {xs:Object.values(xs), ys:Object.values(ys)};\n *       })\n *     .batch(10);\n *\n *   // Define the model.\n *   const model = tf.sequential();\n *   model.add(tf.layers.dense({\n *     inputShape: [numOfFeatures],\n *     units: 1\n *   }));\n *   model.compile({\n *     optimizer: tf.train.sgd(0.000001),\n *     loss: 'meanSquaredError'\n *   });\n *\n *   // Fit the model using the prepared Dataset\n *   return model.fitDataset(flattenedDataset, {\n *     epochs: 10,\n *     callbacks: {\n *       onEpochEnd: async (epoch, logs) => {\n *         console.log(epoch + ':' + logs.loss);\n *       }\n *     }\n *   });\n * }\n *\n * await run();\n * ```\n *\n * @param source URL or local path to get CSV file. If it's a local path, it\n * must have prefix `file://` and it only works in node environment.\n * @param csvConfig (Optional) A CSVConfig object that contains configurations\n *     of reading and decoding from CSV file(s).\n */\n/**\n * @doc {\n *   heading: 'Data',\n *   subheading: 'Creation',\n *   namespace: 'data',\n *   configParamIndices: [1]\n *  }\n */\nexport function csv(\n    source: RequestInfo, csvConfig: CSVConfig = {}): CSVDataset {\n  return new CSVDataset(new URLDataSource(source), csvConfig);\n}\n\n/**\n * Create a `Dataset` that produces each element by calling a provided function.\n *\n * Note that repeated iterations over this `Dataset` may produce different\n * results, because the function will be called anew for each element of each\n * iteration.\n *\n * Also, beware that the sequence of calls to this function may be out of order\n * in time with respect to the logical order of the Dataset. This is due to the\n * asynchronous lazy nature of stream processing, and depends on downstream\n * transformations (e.g. .shuffle()). If the provided function is pure, this is\n * no problem, but if it is a closure over a mutable state (e.g., a traversal\n * pointer), then the order of the produced elements may be scrambled.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const ds = tf.data.func(func);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n *\n * @param f A function that produces one data element on each call.\n */\nexport function func<T extends DataElement>(\n    f: () => IteratorResult<T>| Promise<IteratorResult<T>>): Dataset<T> {\n  const iter = iteratorFromFunction(f);\n  return datasetFromIteratorFn(async () => iter);\n}\n\n/**\n * Create a `Dataset` that produces each element from provided JavaScript\n * generator, which is a function*\n * (https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators#Generator_functions),\n * or a function that returns an\n * iterator\n * (https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators#Generator_functions).\n *\n * The returned iterator should have `.next()` function that returns element in\n * format of `{value: DataElement, done:boolean}`.\n *\n * Example of creating a dataset from an iterator factory:\n * ```js\n * function makeIterator() {\n *   const numElements = 10;\n *   let index = 0;\n *\n *   const iterator = {\n *     next: () => {\n *       let result;\n *       if (index < numElements) {\n *         result = {value: index, done: false};\n *         index++;\n *         return result;\n *       }\n *       return {value: index, done: true};\n *     }\n *   };\n *   return iterator;\n * }\n * const ds = tf.data.generator(makeIterator);\n * ds.forEachAsync(e => console.log(e));\n * ```\n *\n * Example of creating a dataset from a generator:\n * ```js\n * function* dataGenerator() {\n *   const numElements = 10;\n *   let index = 0;\n *   while (index < numElements) {\n *     const x = index;\n *     index++;\n *     yield x;\n *   }\n * }\n *\n * const ds = tf.data.generator(dataGenerator);\n * ds.forEachAsync(e => console.log(e));\n * ```\n *\n * @param generator A Javascript generator function that returns a JavaScript\n *     iterator.\n */\n/**\n * @doc {\n *   heading: 'Data',\n *   subheading: 'Creation',\n *   namespace: 'data',\n *   configParamIndices: [1]\n *  }\n */\nexport function generator<T extends DataElement>(\n    generator: () => Iterator<T>| Promise<Iterator<T>>): Dataset<T> {\n  return datasetFromIteratorFn(async () => {\n    const gen = await generator();\n    return iteratorFromFunction(() => gen.next());\n  });\n}\n","/** @license See the LICENSE file. */\n// This code is auto-generated, do not modify this file!\nconst version = '1.0.4';\nexport {version};\n"],"names":["global","module","define","copy","f","t","c","s0","s1","s2","impl","seed","opts","xg","n","me","this","mash","data","toString","i","length","h","charCodeAt","next","state","prng","int32","double","quick","exports","amd","alea","x","y","z","w","strseed","k","result","xor128","v","d","xorwow","slice","Date","X","j","push","init","xorshift7","limit","Math","max","xor4096","a","b","floor","tychei","pool","math","nodecrypto","width","chunks","rngname","startdenom","pow","significance","overflow","mask","seedrandom","options","callback","key","shortseed","mixkey","flatten","obj","depth","prop","typ","e","entropy","tostring","out","randomBytes","Uint8Array","crypto","msCrypto","getRandomValues","browser","navigator","plugins","screen","autoseed","arc4","keylen","s","S","g","count","r","pass","is_math_call","smear","stringseed","String","fromCharCode","apply","random","require","ex","sr","deepMapInternal","input","mapFn","seen","containedIn","Map","Set","has","Error","get","recurse","value","isIterable","mappedIterable","Array","isArray","add","childResult","delete","set","deepZip","inputs","zipFn","deepZipInternal","map","zipToList","deepMapAndAwaitAll","_a","from","keys","_i","Promise","mappedValue","_b","tf.Tensor","canTensorify","isPrimitive","tf.util","isTypedArray","capacity","RangeError","doubledCapacity","RingBuffer","index","end","begin","isFull","wrap","values","values_1","isEmpty","undefined","relativeIndex","pop","_super","GrowingRingBuffer","INITIAL_CAPACITY","tslib_1.__extends","expand","unshift","newCapacity","newData","len","iteratorFromItems","items","ArrayIterator","iteratorFromFunction","func","FunctionCallIterator","iteratorFromConcatenated","baseIterators","baseErrorHandler","ChainedIterator","iteratorFromZipped","iterators","mismatchMode","ZipMismatchMode","FAIL","ZipIterator","LazyIterator","done","stream","prefetch","predicate","shouldContinue","handler","ErrorHandlingLazyIterator","FilterIterator","transform","MapIterator","AsyncMapIterator","serial","FlatmapIterator","resolveFully","serialMapAsync","resolveWhile","batchSize","smallLastBatch","RowMajorBatchIterator","rowMajorBatch","iterator","TakeIterator","SkipIterator","bufferSize","PrefetchIterator","windowSize","ShuffleIterator","SerialIterator","_this","trav","item","tf.clone","nextFn","message","upstream","lastRead","resolve","summary","then","serialNext","maxCount","skipped","tf.dispose","enableSmallLastBatch","batch","inputTensors","tf.tensor_util","getTensorsInContainer","mapped","outputTensors","inputTensors_1","isTensorInList","dispose","e_1","inputTensors_2","outputQueue","OneToManyIterator","pump","shift","mappedArray","pushAll","inputTensors_3","moreIterators","upstreamSummaries","readFromChain","iteratorResult","handleErrors","itemResult","afterState","getNext","container","numIterators","iteratorsDone","SHORTEST","LONGEST","currentPromise","nextState","buffer","refill","seedrandom.alea","now","randomInt","upstreamExhausted","chosenIndex","chooseIndex","shuffleExcise","Dataset","base","assert","datasetFromIteratorFn","columnMajorBatch","deepBatchConcat","size","Infinity","ceil","dataset","_c","concatenate","filter","tf.tidy","forEachAsync","tf.deprecationWarn","mapAsync","take","skip","reshuffleEachIteration","seed2","shuffle","toArray","toArrayForTest","iteratorFn","class_1","array","zip","datasets","min","Object","ds","rows","batchConcat","arrays","tf.stack","tf.tensor","TextLineDataset","inputIterator","utf8Iterator","decodeUTF8","split","CODE_QUOTE","STATE_OUT","Symbol","STATE_FIELD","STATE_QUOTE","STATE_QUOTE_AFTER_QUOTE","STATE_WITHIN_QUOTE_IN_QUOTE","csvConfig","hasHeader","fullColumnNames","columnNames","columnConfigs","configuredColumnsOnly","delimiter","CSVDataset","columnNamesValidated","setColumnNames","maybeReadHeaderLine","columnNamesFromFile","util","counts","reduce","countAcc","name","duplicateNames","indexOf","firstElement","lines","makeDataElement","line","parseRow","features","labels","config","parsedValue","default","required","isLabel","valueAsNum","Number","isNaN","dtype","getBoolean","xs","ys","toLowerCase","readOffset","readLength","currentState","charAt","substring","StringIterator","separator","SplitIterator","SplitIteratorImpl","chunkResult","carryover","ByteChunkIterator","Utf8Iterator","Utf8IteratorImpl","ENV","decoder","TextDecoder","StringDecoder","chunk","text","decode","write","Buffer","file","File","Blob","offset","chunkSize","FileChunkIterator","byteLength","reject","fileReader_1","FileReader","onload","event","ArrayBuffer","TypeError","onabort","onerror","type","readAsArrayBuffer","urlChunkIterator","url","fetch","response","ok","blob","statusText","nodeFetch","unitArray","isLocalPath","source","substr","FileDataSource","fs","readFileSync","DataSource","fileOptions","URLDataSource","csv","iter","generator","gen","version"],"mappings":";;;;;;;;;;;;;;;;+/DA2BA,SAAUA,EAAQC,EAAQC,GA0B1B,SAASC,EAAKC,EAAGC,GAKf,OAJAA,EAAEC,EAAIF,EAAEE,EACRD,EAAEE,GAAKH,EAAEG,GACTF,EAAEG,GAAKJ,EAAEI,GACTH,EAAEI,GAAKL,EAAEK,GACFJ,EAGT,SAASK,EAAKC,EAAMC,GAClB,IAAIC,EAAK,IAjCX,SAAcF,GACZ,IAgDIG,EAhDAC,EAAKC,KAAMC,GAgDXH,EAAI,WAEG,SAASI,GAClBA,EAAOA,EAAKC,WACZ,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAKG,OAAQD,IAAK,CAEpC,IAAIE,EAAI,oBADRR,GAAKI,EAAKK,WAAWH,IAGrBE,GADAR,EAAIQ,IAAM,EAGVR,GADAQ,GAAKR,KACK,EAEVA,GAAS,YADTQ,GAAKR,GAGP,OAAmB,wBAAXA,IAAM,KA5DhBC,EAAGS,KAAO,WACR,IAAInB,EAAI,QAAUU,EAAGR,GAAY,uBAAPQ,EAAGT,EAG7B,OAFAS,EAAGR,GAAKQ,EAAGP,GACXO,EAAGP,GAAKO,EAAGN,GACJM,EAAGN,GAAKJ,GAAKU,EAAGT,EAAQ,EAAJD,IAI7BU,EAAGT,EAAI,EACPS,EAAGR,GAAKU,EAAK,KACbF,EAAGP,GAAKS,EAAK,KACbF,EAAGN,GAAKQ,EAAK,KACbF,EAAGR,IAAMU,EAAKN,GACVI,EAAGR,GAAK,IAAKQ,EAAGR,IAAM,GAC1BQ,EAAGP,IAAMS,EAAKN,GACVI,EAAGP,GAAK,IAAKO,EAAGP,IAAM,GAC1BO,EAAGN,IAAMQ,EAAKN,GACVI,EAAGN,GAAK,IAAKM,EAAGN,IAAM,GAC1BQ,EAAO,KAYE,CAASN,GACdc,EAAQb,GAAQA,EAAKa,MACrBC,EAAOb,EAAGW,KAUd,OATAE,EAAKC,MAAQ,WAAa,OAAoB,WAAZd,EAAGW,OAAwB,GAC7DE,EAAKE,OAAS,WACZ,OAAOF,IAAmC,wBAAhB,QAATA,IAAoB,IAEvCA,EAAKG,MAAQH,EACTD,IACmB,oBAAUtB,EAAKsB,EAAOZ,GAC3Ca,EAAKD,MAAQ,WAAa,OAAOtB,EAAKU,QAEjCa,EAyBLzB,GAAUA,EAAO6B,QACnB7B,EAAO6B,QAAUpB,EACRR,GAAUA,EAAO6B,IAC1B7B,EAAO,WAAa,OAAOQ,IAE3BM,KAAKgB,KAAOtB,EA7Ed,CAiFEM,EACAf,GACA,8CC3GF,SAAUD,EAAQC,EAAQC,GAkC1B,SAASC,EAAKC,EAAGC,GAKf,OAJAA,EAAE4B,EAAI7B,EAAE6B,EACR5B,EAAE6B,EAAI9B,EAAE8B,EACR7B,EAAE8B,EAAI/B,EAAE+B,EACR9B,EAAE+B,EAAIhC,EAAEgC,EACD/B,EAGT,SAASK,EAAKC,EAAMC,GAClB,IAAIC,EAAK,IAzCX,SAAgBF,GACd,IAAII,EAAKC,KAAMqB,EAAU,GAEzBtB,EAAGkB,EAAI,EACPlB,EAAGmB,EAAI,EACPnB,EAAGoB,EAAI,EACPpB,EAAGqB,EAAI,EAGPrB,EAAGS,KAAO,WACR,IAAInB,EAAIU,EAAGkB,EAAKlB,EAAGkB,GAAK,GAIxB,OAHAlB,EAAGkB,EAAIlB,EAAGmB,EACVnB,EAAGmB,EAAInB,EAAGoB,EACVpB,EAAGoB,EAAIpB,EAAGqB,EACHrB,EAAGqB,GAAMrB,EAAGqB,IAAM,GAAM/B,EAAKA,IAAM,GAGxCM,KAAiB,EAAPA,GAEZI,EAAGkB,EAAItB,EAGP0B,GAAW1B,EAIb,IAAK,IAAI2B,EAAI,EAAGA,EAAID,EAAQhB,OAAS,GAAIiB,IACvCvB,EAAGkB,GAA6B,EAAxBI,EAAQd,WAAWe,GAC3BvB,EAAGS,OAaI,CAAWb,GAChBc,EAAQb,GAAQA,EAAKa,MACrBC,EAAO,WAAa,OAAQb,EAAGW,SAAW,GAAK,YAenD,OAdAE,EAAKE,OAAS,WACZ,GACE,IAEIW,IAFM1B,EAAGW,SAAW,KACbX,EAAGW,SAAW,GAAK,aACF,GAAK,UACf,IAAXe,GACT,OAAOA,GAETb,EAAKC,MAAQd,EAAGW,KAChBE,EAAKG,MAAQH,EACTD,IACmB,oBAAUtB,EAAKsB,EAAOZ,GAC3Ca,EAAKD,MAAQ,WAAa,OAAOtB,EAAKU,QAEjCa,EAGLzB,GAAUA,EAAO6B,QACnB7B,EAAO6B,QAAUpB,EACRR,GAAUA,EAAO6B,IAC1B7B,EAAO,WAAa,OAAOQ,IAE3BM,KAAKwB,OAAS9B,EApEhB,CAwEEM,EACAf,GACA,8CC1EF,SAAUD,EAAQC,EAAQC,GAqC1B,SAASC,EAAKC,EAAGC,GAOf,OANAA,EAAE4B,EAAI7B,EAAE6B,EACR5B,EAAE6B,EAAI9B,EAAE8B,EACR7B,EAAE8B,EAAI/B,EAAE+B,EACR9B,EAAE+B,EAAIhC,EAAEgC,EACR/B,EAAEoC,EAAIrC,EAAEqC,EACRpC,EAAEqC,EAAItC,EAAEsC,EACDrC,EAGT,SAASK,EAAKC,EAAMC,GAClB,IAAIC,EAAK,IA9CX,SAAgBF,GACd,IAAII,EAAKC,KAAMqB,EAAU,GAGzBtB,EAAGS,KAAO,WACR,IAAInB,EAAKU,EAAGkB,EAAKlB,EAAGkB,IAAM,EAE1B,OADAlB,EAAGkB,EAAIlB,EAAGmB,EAAGnB,EAAGmB,EAAInB,EAAGoB,EAAGpB,EAAGoB,EAAIpB,EAAGqB,EAAGrB,EAAGqB,EAAIrB,EAAG0B,GACzC1B,EAAG2B,EAAK3B,EAAG2B,EAAI,OAAS,IAC5B3B,EAAG0B,EAAK1B,EAAG0B,EAAK1B,EAAG0B,GAAK,EAAOpC,EAAKA,GAAK,GAAO,GAGtDU,EAAGkB,EAAI,EACPlB,EAAGmB,EAAI,EACPnB,EAAGoB,EAAI,EACPpB,EAAGqB,EAAI,EACPrB,EAAG0B,EAAI,EAEH9B,KAAiB,EAAPA,GAEZI,EAAGkB,EAAItB,EAGP0B,GAAW1B,EAIb,IAAK,IAAI2B,EAAI,EAAGA,EAAID,EAAQhB,OAAS,GAAIiB,IACvCvB,EAAGkB,GAA6B,EAAxBI,EAAQd,WAAWe,GACvBA,GAAKD,EAAQhB,SACfN,EAAG2B,EAAI3B,EAAGkB,GAAK,GAAKlB,EAAGkB,IAAM,GAE/BlB,EAAGS,OAeI,CAAWb,GAChBc,EAAQb,GAAQA,EAAKa,MACrBC,EAAO,WAAa,OAAQb,EAAGW,SAAW,GAAK,YAenD,OAdAE,EAAKE,OAAS,WACZ,GACE,IAEIW,IAFM1B,EAAGW,SAAW,KACbX,EAAGW,SAAW,GAAK,aACF,GAAK,UACf,IAAXe,GACT,OAAOA,GAETb,EAAKC,MAAQd,EAAGW,KAChBE,EAAKG,MAAQH,EACTD,IACmB,oBAAUtB,EAAKsB,EAAOZ,GAC3Ca,EAAKD,MAAQ,WAAa,OAAOtB,EAAKU,QAEjCa,EAGLzB,GAAUA,EAAO6B,QACnB7B,EAAO6B,QAAUpB,EACRR,GAAUA,EAAO6B,IAC1B7B,EAAO,WAAa,OAAOQ,IAE3BM,KAAK2B,OAASjC,EAzEhB,CA6EEM,EACAf,GACA,iDC7EF,SAAUD,EAAQC,EAAQC,GAkD1B,SAASC,EAAKC,EAAGC,GAGf,OAFAA,EAAE4B,EAAI7B,EAAE6B,EAAEW,QACVvC,EAAEe,EAAIhB,EAAEgB,EACDf,EAGT,SAASK,EAAKC,EAAMC,GACN,MAARD,IAAcA,OAAakC,MAC/B,IAAIhC,EAAK,IAxDX,SAAgBF,GACd,IAAII,EAAKC,KAGTD,EAAGS,KAAO,WAER,IAAwBnB,EAAGoC,EAAvBK,EAAI/B,EAAGkB,EAAGb,EAAIL,EAAGK,EAQrB,OAPAf,EAAIyC,EAAE1B,GAAoBqB,GAAhBpC,GAAMA,IAAM,GAAaA,GAAK,GACpBoC,IAApBpC,EAAIyC,EAAG1B,EAAI,EAAK,IAAcf,IAAM,GAChBoC,IAApBpC,EAAIyC,EAAG1B,EAAI,EAAK,IAAcf,IAAM,EAChBoC,IAApBpC,EAAIyC,EAAG1B,EAAI,EAAK,IAAcf,GAAK,EACnCA,EAAIyC,EAAG1B,EAAI,EAAK,GAAuBqB,IAAnBpC,GAASA,GAAK,IAAeA,GAAK,EACtDyC,EAAE1B,GAAKqB,EACP1B,EAAGK,EAAKA,EAAI,EAAK,EACVqB,GAGT,SAAc1B,EAAIJ,GAChB,IAAIoC,EAAMD,KAEV,GAAInC,KAAiB,EAAPA,GAERmC,EAAE,GAAKnC,OAIX,IADAA,EAAO,GAAKA,EACPoC,EAAI,EAAGA,EAAIpC,EAAKU,SAAU0B,EAC7BD,EAAM,EAAJC,GAAUD,EAAM,EAAJC,IAAU,GACnBpC,EAAKY,WAAWwB,GAAKD,EAAGC,EAAI,EAAK,IAAM,GAIhD,KAAOD,EAAEzB,OAAS,GAAGyB,EAAEE,KAAK,GAC5B,IAAKD,EAAI,EAAGA,EAAI,GAAc,IAATD,EAAEC,KAAYA,GAOnC,IANS,GAALA,EAAYD,EAAE,IAAM,EAAYA,EAAEC,GAEtChC,EAAGkB,EAAIa,EACP/B,EAAGK,EAAI,EAGF2B,EAAI,IAAKA,EAAI,IAAKA,EACrBhC,EAAGS,OAIPyB,CAAKlC,EAAIJ,GAWA,CAAWA,GAChBc,EAAQb,GAAQA,EAAKa,MACrBC,EAAO,WAAa,OAAQb,EAAGW,SAAW,GAAK,YAenD,OAdAE,EAAKE,OAAS,WACZ,GACE,IAEIW,IAFM1B,EAAGW,SAAW,KACbX,EAAGW,SAAW,GAAK,aACF,GAAK,UACf,IAAXe,GACT,OAAOA,GAETb,EAAKC,MAAQd,EAAGW,KAChBE,EAAKG,MAAQH,EACTD,IACEA,EAAMQ,GAAG9B,EAAKsB,EAAOZ,GACzBa,EAAKD,MAAQ,WAAa,OAAOtB,EAAKU,QAEjCa,EAGLzB,GAAUA,EAAO6B,QACnB7B,EAAO6B,QAAUpB,EACRR,GAAUA,EAAO6B,IAC1B7B,EAAO,WAAa,OAAOQ,IAE3BM,KAAKkC,UAAYxC,EAnFnB,CAuFEM,EACAf,GACA,+CCrEF,SAAUD,EAAQC,EAAQC,GA+E1B,SAASC,EAAKC,EAAGC,GAIf,OAHAA,EAAEe,EAAIhB,EAAEgB,EACRf,EAAE+B,EAAIhC,EAAEgC,EACR/B,EAAEyC,EAAI1C,EAAE0C,EAAEF,QACHvC,EAGT,SAASK,EAAKC,EAAMC,GACN,MAARD,IAAcA,OAAakC,MAC/B,IAAIhC,EAAK,IAtFX,SAAgBF,GACd,IAAII,EAAKC,KAGTD,EAAGS,KAAO,WACR,IACwBnB,EAAGoC,EADvBL,EAAIrB,EAAGqB,EACPU,EAAI/B,EAAG+B,EAAG1B,EAAIL,EAAGK,EAcrB,OAZAL,EAAGqB,EAAIA,EAAKA,EAAI,WAAc,EAE9BK,EAAIK,EAAG1B,EAAI,GAAM,KACjBf,EAAIyC,EAAE1B,EAAMA,EAAI,EAAK,KACrBqB,GAAKA,GAAK,GACVpC,GAAKA,GAAK,GACVoC,GAAKA,IAAM,GACXpC,GAAKA,IAAM,GAEXoC,EAAIK,EAAE1B,GAAKqB,EAAIpC,EACfU,EAAGK,EAAIA,EAECqB,GAAKL,EAAKA,IAAM,IAAQ,GAGlC,SAAcrB,EAAIJ,GAChB,IAAIN,EAAGoC,EAAGrB,EAAG2B,EAAGX,EAAGU,KAAQK,EAAQ,IAYnC,IAXIxC,KAAiB,EAAPA,IAEZ8B,EAAI9B,EACJA,EAAO,OAGPA,GAAc,KACd8B,EAAI,EACJU,EAAQC,KAAKC,IAAIF,EAAOxC,EAAKU,SAG1BD,EAAI,EAAG2B,GAAK,GAAIA,EAAII,IAASJ,EAE5BpC,IAAM8B,GAAK9B,EAAKY,YAAYwB,EAAI,IAAMpC,EAAKU,SAErC,IAAN0B,IAASX,EAAIK,GACjBA,GAAKA,GAAK,GACVA,GAAKA,IAAM,GACXA,GAAKA,GAAK,EACVA,GAAKA,IAAM,GACPM,GAAK,IACPX,EAAKA,EAAI,WAAc,EAEvBhB,EAAK,IADLf,EAAKyC,EAAM,IAAJC,IAAaN,EAAIL,GACThB,EAAI,EAAI,GAW3B,IAPIA,GAAK,MACP0B,EAA+B,KAA5BnC,GAAQA,EAAKU,QAAU,KAAa,GAKzCD,EAAI,IACC2B,EAAI,IAASA,EAAI,IAAKA,EACzBN,EAAIK,EAAG1B,EAAI,GAAM,KACjBf,EAAIyC,EAAE1B,EAAMA,EAAI,EAAK,KACrBqB,GAAKA,GAAK,GACVpC,GAAKA,GAAK,GACVoC,GAAKA,IAAM,GACXpC,GAAKA,IAAM,GACXyC,EAAE1B,GAAKqB,EAAIpC,EAGbU,EAAGqB,EAAIA,EACPrB,EAAG+B,EAAIA,EACP/B,EAAGK,EAAIA,EAGT6B,CAAKlC,EAAIJ,GAYA,CAAWA,GAChBc,EAAQb,GAAQA,EAAKa,MACrBC,EAAO,WAAa,OAAQb,EAAGW,SAAW,GAAK,YAenD,OAdAE,EAAKE,OAAS,WACZ,GACE,IAEIW,IAFM1B,EAAGW,SAAW,KACbX,EAAGW,SAAW,GAAK,aACF,GAAK,UACf,IAAXe,GACT,OAAOA,GAETb,EAAKC,MAAQd,EAAGW,KAChBE,EAAKG,MAAQH,EACTD,IACEA,EAAMqB,GAAG3C,EAAKsB,EAAOZ,GACzBa,EAAKD,MAAQ,WAAa,OAAOtB,EAAKU,QAEjCa,EAGLzB,GAAUA,EAAO6B,QACnB7B,EAAO6B,QAAUpB,EACRR,GAAUA,EAAO6B,IAC1B7B,EAAO,WAAa,OAAOQ,IAE3BM,KAAKsC,QAAU5C,EAjHjB,CAqHEM,EACAf,GACA,8CC5IF,SAAUD,EAAQC,EAAQC,GAuD1B,SAASC,EAAKC,EAAGC,GAKf,OAJAA,EAAEkD,EAAInD,EAAEmD,EACRlD,EAAEmD,EAAIpD,EAAEoD,EACRnD,EAAEC,EAAIF,EAAEE,EACRD,EAAEqC,EAAItC,EAAEsC,EACDrC,EAGT,SAASK,EAAKC,EAAMC,GAClB,IAAIC,EAAK,IA9DX,SAAgBF,GACd,IAAII,EAAKC,KAAMqB,EAAU,GAGzBtB,EAAGS,KAAO,WACR,IAAIgC,EAAIzC,EAAGyC,EAAGlD,EAAIS,EAAGT,EAAGoC,EAAI3B,EAAG2B,EAAGa,EAAIxC,EAAGwC,EAQzC,OAPAC,EAAKA,GAAK,GAAOA,IAAM,EAAKlD,EAC5BA,EAAKA,EAAIoC,EAAK,EACdA,EAAKA,GAAK,GAAOA,IAAM,EAAKa,EAC5BA,EAAKA,EAAIC,EAAK,EACdzC,EAAGyC,EAAIA,EAAKA,GAAK,GAAOA,IAAM,GAAMlD,EACpCS,EAAGT,EAAIA,EAAKA,EAAIoC,EAAK,EACrB3B,EAAG2B,EAAKA,GAAK,GAAOpC,IAAM,GAAMiD,EACzBxC,EAAGwC,EAAKA,EAAIC,EAAK,GAmB1BzC,EAAGwC,EAAI,EACPxC,EAAGyC,EAAI,EACPzC,EAAGT,GAAI,WACPS,EAAG2B,EAAI,WAEH/B,IAASyC,KAAKK,MAAM9C,IAEtBI,EAAGwC,EAAK5C,EAAO,WAAe,EAC9BI,EAAGyC,EAAW,EAAP7C,GAGP0B,GAAW1B,EAIb,IAAK,IAAI2B,EAAI,EAAGA,EAAID,EAAQhB,OAAS,GAAIiB,IACvCvB,EAAGyC,GAA6B,EAAxBnB,EAAQd,WAAWe,GAC3BvB,EAAGS,OAaI,CAAWb,GAChBc,EAAQb,GAAQA,EAAKa,MACrBC,EAAO,WAAa,OAAQb,EAAGW,SAAW,GAAK,YAenD,OAdAE,EAAKE,OAAS,WACZ,GACE,IAEIW,IAFM1B,EAAGW,SAAW,KACbX,EAAGW,SAAW,GAAK,aACF,GAAK,UACf,IAAXe,GACT,OAAOA,GAETb,EAAKC,MAAQd,EAAGW,KAChBE,EAAKG,MAAQH,EACTD,IACmB,oBAAUtB,EAAKsB,EAAOZ,GAC3Ca,EAAKD,MAAQ,WAAa,OAAOtB,EAAKU,QAEjCa,EAGLzB,GAAUA,EAAO6B,QACnB7B,EAAO6B,QAAUpB,EACRR,GAAUA,EAAO6B,IAC1B7B,EAAO,WAAa,OAAOQ,IAE3BM,KAAK0C,OAAShD,EAzFhB,CA6FEM,EACAf,GACA,kDC3EF,SAAW0D,EAAMC,GAIjB,IASIC,EATA7D,EAASgB,KACT8C,EAAQ,IACRC,EAAS,EAETC,EAAU,SACVC,EAAaL,EAAKM,IAAIJ,EAAOC,GAC7BI,EAAeP,EAAKM,IAAI,EAHf,IAITE,EAA0B,EAAfD,EACXE,EAAOP,EAAQ,EAOnB,SAASQ,EAAW3D,EAAM4D,EAASC,GACjC,IAAIC,KAIAC,EAAYC,EAoHlB,SAASC,EAAQC,EAAKC,GACpB,IAAqCC,EAAjCxC,KAAayC,SAAcH,EAC/B,GAAIC,GAAgB,UAAPE,EACX,IAAKD,KAAQF,EACX,IAAMtC,EAAOS,KAAK4B,EAAQC,EAAIE,GAAOD,EAAQ,IAAO,MAAOG,IAG/D,OAAQ1C,EAAOlB,OAASkB,EAAgB,UAAPyC,EAAkBH,EAAMA,EAAM,KA3HxCD,EAHvBL,EAAsB,GAAXA,GAAqBW,SAAS,GAAUX,OAIzCW,SAAWvE,EAAMwE,EAASxB,IACzB,MAARhD,EA+IL,WACE,IACE,IAAIyE,EAQJ,OAPIvB,IAAeuB,EAAMvB,EAAWwB,aAElCD,EAAMA,EAAItB,IAEVsB,EAAM,IAAIE,WAAWxB,IACpB9D,EAAOuF,QAAUvF,EAAOwF,UAAUC,gBAAgBL,IAE9CD,EAASC,GAChB,MAAOH,GACP,IAAIS,EAAU1F,EAAO2F,UACjBC,EAAUF,GAAWA,EAAQE,QACjC,QAAS,IAAI/C,KAAM7C,EAAQ4F,EAAS5F,EAAO6F,OAAQV,EAASxB,KA7J3CmC,GAAanF,EAAM,GAAI8D,GAGtCsB,EAAO,IA+Db,SAActB,GACZ,IAAIpE,EAAG2F,EAASvB,EAAIpD,OAChBN,EAAKC,KAAMI,EAAI,EAAG2B,EAAIhC,EAAGK,EAAIL,EAAGgC,EAAI,EAAGkD,EAAIlF,EAAGmF,KAG7CF,IAAUvB,GAAOuB,MAGtB,KAAO5E,EAAI0C,GACTmC,EAAE7E,GAAKA,IAET,IAAKA,EAAI,EAAGA,EAAI0C,EAAO1C,IACrB6E,EAAE7E,GAAK6E,EAAElD,EAAIsB,EAAQtB,EAAI0B,EAAIrD,EAAI4E,IAAW3F,EAAI4F,EAAE7E,KAClD6E,EAAElD,GAAK1C,GAIRU,EAAGoF,EAAI,SAASC,GAIf,IAFA,IAAI/F,EAAGgG,EAAI,EACPjF,EAAIL,EAAGK,EAAG2B,EAAIhC,EAAGgC,EAAGkD,EAAIlF,EAAGmF,EACxBE,KACL/F,EAAI4F,EAAE7E,EAAIiD,EAAQjD,EAAI,GACtBiF,EAAIA,EAAIvC,EAAQmC,EAAE5B,GAAS4B,EAAE7E,GAAK6E,EAAElD,EAAIsB,EAAQtB,EAAI1C,KAAQ4F,EAAElD,GAAK1C,IAGrE,OADAU,EAAGK,EAAIA,EAAGL,EAAGgC,EAAIA,EACVsD,IAINvC,GA7FQ,CAASW,GAIhB/C,EAAO,WAIT,IAHA,IAAIZ,EAAIiF,EAAKI,EAAEpC,GACXrB,EAAIuB,EACJhC,EAAI,EACDnB,EAAIqD,GACTrD,GAAKA,EAAImB,GAAK6B,EACdpB,GAAKoB,EACL7B,EAAI8D,EAAKI,EAAE,GAEb,KAAOrF,GAAKsD,GACVtD,GAAK,EACL4B,GAAK,EACLT,KAAO,EAET,OAAQnB,EAAImB,GAAKS,GAWnB,OARAhB,EAAKC,MAAQ,WAAa,OAAmB,EAAZoE,EAAKI,EAAE,IACxCzE,EAAKG,MAAQ,WAAa,OAAOkE,EAAKI,EAAE,GAAK,YAC7CzE,EAAKE,OAASF,EAGdiD,EAAOQ,EAASY,EAAKG,GAAIvC,IAGjBY,EAAQ+B,MAAQ9B,GACpB,SAAS9C,EAAMf,EAAM4F,EAAc9E,GAUjC,OATIA,IAEEA,EAAMyE,GAAK/F,EAAKsB,EAAOsE,GAE3BrE,EAAKD,MAAQ,WAAa,OAAOtB,EAAK4F,QAKpCQ,GAAgB3C,EAAKI,GAAWtC,EAAaf,GAIrCe,IAElBA,EACAgD,EACA,WAAYH,EAAUA,EAAQvE,OAAUgB,MAAQ4C,EAChDW,EAAQ9C,OAmDV,SAAStB,EAAKC,EAAGC,GAIf,OAHAA,EAAEe,EAAIhB,EAAEgB,EACRf,EAAE0C,EAAI3C,EAAE2C,EACR1C,EAAE6F,EAAI9F,EAAE8F,EAAEtD,QACHvC,EAsBT,SAASsE,EAAOhE,EAAM8D,GAEpB,IADA,IAA4B+B,EAAxBC,EAAa9F,EAAO,GAAWoC,EAAI,EAChCA,EAAI0D,EAAWpF,QACpBoD,EAAIJ,EAAOtB,GACTsB,GAASmC,GAAyB,GAAhB/B,EAAIJ,EAAOtB,IAAW0D,EAAWlF,WAAWwB,KAElE,OAAOoC,EAASV,GA8BlB,SAASU,EAAS5B,GAChB,OAAOmD,OAAOC,aAAaC,MAAM,EAAGrD,GAgBtC,GAhIAK,EAAK,OAASI,GAAWM,EA0HzBK,EAAOf,EAAKiD,SAAUlD,GAMlB1D,EAAsC6B,QAAS,CACjD7B,UAAiBqE,EAEjB,IACET,EAAaiD,QAAQ,UACrB,MAAOC,UACA,EAtNX,IA6NE3D,QCjMF4D,WAAGhF,KAAOA,KACVgF,WAAGxE,OAASA,OACZwE,WAAGrE,OAASA,OACZqE,WAAG9D,UAAYA,UACf8D,WAAG1D,QAAUA,QACb0D,WAAGtD,OAASA,OAEZ,iBAAiBsD,0CCEjB,SAASC,gBACLC,EAAYC,EACZC,EAAiCC,GAEnC,gBAFED,MAA0BE,kBAAOD,MAA2BE,KAEjD,MAATL,EACF,OAAO,KAET,GAAIG,EAAYG,IAAIN,GAClB,MAAM,IAAIO,MAAM,0CAElB,GAAIL,EAAKI,IAAIN,GACX,OAAOE,EAAKM,IAAIR,GAElB,IAAM3E,EAAS4E,EAAMD,GAErB,GAAI3E,EAAOoF,SAA4B,OAAjBpF,EAAOqF,MAC3B,MAAM,IAAIH,MACN,qEAGN,GAAKlF,EAAOoF,QAGL,CAAA,GAAIE,WAAWX,GAAQ,CAE5B,IAAMY,EAA4BC,MAAMC,QAAQd,SAEhD,IAAK,IAAM5E,KADX+E,EAAYY,IAAIf,GACAA,EAAO,CACrB,IACMgB,EAAcjB,gBADNC,EAAM5E,GACuB6E,EAAOC,EAAMC,GACxDS,EAAexF,GAAK4F,EAGtB,OADAb,EAAYc,OAAOjB,GACZY,EAEP,MAAM,IAAIL,MAAM,yCAAyCP,GAbzD,OADAE,EAAKgB,IAAIlB,EAAO3E,EAAOqF,OAChBrF,EAAOqF,MA0ClB,SAAgBS,QACZC,EAAeC,GACjB,oBADiBA,aACVC,gBAAgBF,EAAQC,GAOjC,SAASC,gBACLF,EAAeC,EACflB,gBAAAA,MAA2BE,KAG7B,IAAML,EAAQoB,EAAO,GACrB,GAAIjB,EAAYG,IAAIN,GAClB,MAAM,IAAIO,MAAM,0CAElB,IAAMlF,EAASgG,EAAMD,GAErB,GAAI/F,EAAOoF,SAA4B,OAAjBpF,EAAOqF,MAC3B,MAAM,IAAIH,MACN,qEAGN,GAAKlF,EAAOoF,QAEL,CAAA,GAAIE,WAAWX,GAAQ,CAE5B,IAAMY,EAA4BC,MAAMC,QAAQd,SAChDG,EAAYY,IAAIf,kBACL5E,GACT,IACM4F,EAAcM,gBADHF,EAAOG,IAAI,SAAAxG,GAAK,OAAAA,EAAEK,KACWiG,EAAOlB,GACrDS,EAAexF,GAAK4F,GAHtB,IAAK,IAAM5F,KAAK4E,IAAL5E,GAMX,OADA+E,EAAYc,OAAOjB,GACZY,EAEP,MAAM,IAAIL,MAAM,yCAAyCP,GAbzD,OAAO3E,EAAOqF,MAkBlB,SAAgBc,UAAUzG,GACxB,OAAU,OAANA,EACK,KAIL4F,WAAW5F,EAAE,KACP2F,MAAO,KAAMD,SAAS,IAEtBC,MAAO3F,EAAG0F,SAAS,GAqC/B,SAAsBgB,mBAClBzB,EAAYC,6HACRC,EAA+B,IAAIE,IAGzCL,gBAAgBC,EAAOC,EAAOC,OAMZwB,EAAAb,MAAMc,KAAKzB,EAAK0B,gCAAhBC,YAAPtE,QACHmD,EAAQR,EAAKM,IAAIjD,cACFuE,WACOpB,uBAApBqB,EAAcC,SACpB9B,EAAKgB,IAAI3D,EAAKwE,2BAJAF,iBAYlB,SADe9B,gBAAgBC,EAAOC,EAAOC,SAU/C,SAAgBS,WAAWhD,GACzB,OAAc,MAAPA,IACFkD,MAAMC,QAAQnD,IACE,iBAARA,KAAsBA,aAAesE,SAYpD,SAAgBC,aAAavE,GAC3B,OAAc,MAAPA,GAAewE,YAAYxE,IAAQkD,MAAMC,QAAQnD,IACpC,iBAARA,GAAqBA,aAAesE,QAC5CG,KAAQC,aAAa1E,GAO3B,SAASwE,YAAYzB,GACnB,OACc,OAAVA,GACkB,iBAAVA,GAAuC,mBAAVA,EC9P3C,0BAcE,WAAmB4B,GACjB,GADiBxI,cAAAwI,EAVTxI,WAAQ,EACRA,SAAM,EAUE,MAAZwI,EACF,MAAM,IAAIC,WAAW,mDAEvB,GAAID,EAAW,EACb,MAAM,IAAIC,WAAW,6CAEvBzI,KAAKE,KAAO,IAAI6G,MAASyB,GACzBxI,KAAK0I,gBAAkB,EAAIF,EAoI/B,OA9HYG,iBAAV,SAAeC,GAEb,KAAOA,EAAQ,GACbA,GAAS5I,KAAK0I,gBAEhB,OAAOE,EAAQ5I,KAAK0I,iBAGZC,gBAAV,SAAcC,GACZ,GAAIA,EAAQ,EACV,MAAM,IAAIH,WAAW,uCAEvB,OAAOzI,KAAKE,KAAK0I,EAAQ5I,KAAKwI,WAGtBG,gBAAV,SAAcC,EAAehC,GAC3B,GAAIgC,EAAQ,EACV,MAAM,IAAIH,WAAW,uCAEvBzI,KAAKE,KAAK0I,EAAQ5I,KAAKwI,UAAY5B,GAMrC+B,mBAAA,WACE,IAAItI,EAASL,KAAK6I,IAAM7I,KAAK8I,MAI7B,OAHIzI,EAAS,IACXA,EAASL,KAAK0I,gBAAkBrI,GAE3BA,GAQTsI,mBAAA,WACE,OAAO3I,KAAKK,WAAaL,KAAKwI,UAQhCG,oBAAA,WACE,OAAyB,IAAlB3I,KAAKK,UAMdsI,iBAAA,SAAK/B,GACH,GAAI5G,KAAK+I,SACP,MAAM,IAAIN,WAAW,wBAEvBzI,KAAKoH,IAAIpH,KAAK6I,IAAKjC,GACnB5G,KAAK6I,IAAM7I,KAAKgJ,KAAKhJ,KAAK6I,IAAM,IAMlCF,oBAAA,SAAQM,GACN,IAAoB,QAAAC,IAAAnB,WAAAA,IAAQ,CAAvB,IAAMnB,OACT5G,KAAKgC,KAAK4E,KAOd+B,gBAAA,WACE,GAAI3I,KAAKmJ,UACP,MAAM,IAAIV,WAAW,yBAEvBzI,KAAK6I,IAAM7I,KAAKgJ,KAAKhJ,KAAK6I,IAAM,GAChC,IAAMtH,EAASvB,KAAK0G,IAAI1G,KAAK6I,KAE7B,OADA7I,KAAKoH,IAAIpH,KAAK6I,SAAKO,GACZ7H,GAMToH,oBAAA,SAAQ/B,GACN,GAAI5G,KAAK+I,SACP,MAAM,IAAIN,WAAW,wBAEvBzI,KAAK8I,MAAQ9I,KAAKgJ,KAAKhJ,KAAK8I,MAAQ,GACpC9I,KAAKoH,IAAIpH,KAAK8I,MAAOlC,IAMvB+B,kBAAA,WACE,GAAI3I,KAAKmJ,UACP,MAAM,IAAIV,WAAW,yBAEvB,IAAMlH,EAASvB,KAAK0G,IAAI1G,KAAK8I,OAG7B,OAFA9I,KAAKoH,IAAIpH,KAAK8I,WAAOM,GACrBpJ,KAAK8I,MAAQ9I,KAAKgJ,KAAKhJ,KAAK8I,MAAQ,GAC7BvH,GAYToH,0BAAA,SAAcU,GACZ,GAAIrJ,KAAKmJ,UACP,MAAM,IAAIV,WAAW,yBAEvB,IAAMG,EAAQ5I,KAAKgJ,KAAKhJ,KAAK8I,MAAQO,GAC/B9H,EAASvB,KAAK0G,IAAIkC,GAExB,OADA5I,KAAKoH,IAAIwB,EAAO5I,KAAKsJ,OACd/H,sCCnJT,oBACEgI,YAAMC,EAAkBC,wBAyC5B,OAhD0CC,eAUxCF,mBAAA,WACE,OAAO,GAGTA,iBAAA,SAAK5C,GACC2C,YAAMR,mBACR/I,KAAK2J,SAEPJ,YAAMvH,eAAK4E,IAGb4C,oBAAA,SAAQ5C,GACF2C,YAAMR,mBACR/I,KAAK2J,SAEPJ,YAAMK,kBAAQhD,IAMR4C,mBAAR,WAOE,IANA,IAAMK,EAA8B,EAAhB7J,KAAKwI,SACnBsB,EAAU,IAAI/C,MAAS8C,GACvBE,EAAM/J,KAAKK,SAIRD,EAAI,EAAGA,EAAI2J,EAAK3J,IACvB0J,EAAQ1J,GAAKJ,KAAK0G,IAAI1G,KAAKgJ,KAAKhJ,KAAK8I,MAAQ1I,IAG/CJ,KAAKE,KAAO4J,EACZ9J,KAAKwI,SAAWqB,EAChB7J,KAAK0I,gBAAkB,EAAI1I,KAAKwI,SAChCxI,KAAK8I,MAAQ,EACb9I,KAAK6I,IAAMkB,GA7CEP,mBAAmB,MADMb,YCY1C,SAAgBqB,kBAAqBC,GACnC,OAAO,IAAIC,cAAcD,GAwB3B,SAAgBE,qBACZC,GAEF,OAAO,IAAIC,qBAAqBD,GAelC,SAAgBE,yBACZC,EACAC,GACF,OAAO,IAAIC,gBAAgBF,EAAeC,GAkD5C,SAAgBE,mBACZC,EACAC,GACF,oBADEA,EAAgCC,gBAAgBC,MAC3C,IAAIC,YAAeJ,EAAWC,GAUvC,IAq0BYC,wCAr0BZ,cAwUA,OAhTQG,oBAAN,6HAEU,OADFzJ,QACQvB,KAAKQ,eAAfS,EAAI2G,iCACA3G,EAAEgK,YACR1J,EAAOS,KAAKf,EAAE2F,UACJ5G,KAAKQ,uBAAfS,EAAI2G,sBAEN,SAAOrG,SAcHyJ,2BAAN,+HAGU,OAFFE,EAASlL,KAAKmL,SAAS,KACvB5J,QACQ2J,EAAO1K,eAAjBS,EAAI2G,iCACA3G,EAAEgK,YACR1J,EAAOS,KAAKf,EAAE2F,UACJsE,EAAO1K,uBAAjBS,EAAI2G,sBAEN,SAAOrG,SAUHyJ,yBAAN,2HACU,SAAMhL,KAAKQ,eAAfS,EAAI2G,iCACA3G,EAAEgK,cACEjL,KAAKQ,sBAAfS,EAAI2G,sCAWFoD,yBAAN,SAAmBI,qHACT,SAAMpL,KAAKQ,eAAfS,EAAI2G,SACJyD,EAAiBD,EAAUnK,EAAE2F,+BACxB3F,EAAEgK,OAASI,WACRrL,KAAKQ,sBAAfS,EAAI2G,SACJyD,EAAiBD,EAAUnK,EAAE2F,oCAgBjCoE,yBAAA,SAAaM,GACX,OAAO,IAAIC,0BAA0BvL,KAAMsL,IAa7CN,mBAAA,SAAOI,GACL,OAAO,IAAII,eAAexL,KAAMoL,IAWlCJ,gBAAA,SAAOS,GACL,OAAO,IAAIC,YAAY1L,KAAMyL,IAW/BT,qBAAA,SAAYS,GACV,OAAO,IAAIE,iBAAiB3L,KAAMyL,IAWpCT,2BAAA,SAAkBS,GAChB,OAAO,IAAIE,iBAAiB3L,KAAMyL,GAAWG,UAW/CZ,oBAAA,SAAWS,GACT,OAAO,IAAII,gBAAgB7L,KAAMyL,IAQ7BT,yBAAN,SAAmB5L,sFACjB,SAAOY,KAAKyH,IAAIrI,GAAG0M,qBAUfd,0BAAN,SAAoB5L,sFAClB,SAAOY,KAAK+L,eAAe3M,GAAG4M,aAAa,SAAA/K,GAAK,OAAO,IAANA,UAqBnD+J,0BAAA,SAAciB,EAAmBC,GAC/B,oBAD+BA,MACxB,IAAIC,sBAAsBnM,KAAMiM,EAAWC,IAmCpDlB,6BAAA,SACIiB,EAAmBC,EAEnB3E,GAMF,oBARqB2E,mBAEnB3E,aAGiBvH,KAAKoM,cAAcH,EAAWC,GAG/BzE,IAAI,SAAAxG,GAAK,OAAAoG,QAAQpG,EAAGsG,MAaxCyD,wBAAA,SACIqB,EACA7B,GACF,OAAO,IAAIC,gBACPT,mBAAmBhK,KAAMqM,IAAY7B,IAU3CQ,iBAAA,SAAK5F,GACH,OAAIA,EAAQ,GAAc,MAATA,EACRpF,KAEF,IAAIsM,aAAatM,KAAMoF,IAShC4F,iBAAA,SAAK5F,GACH,OAAIA,EAAQ,GAAc,MAATA,EACRpF,KAEF,IAAIuM,aAAavM,KAAMoF,IAYhC4F,qBAAA,SAASwB,GACP,OAAO,IAAIC,iBAAiBzM,KAAMwM,IAapCxB,oBAAA,SAAQ0B,EAAoB/M,GAC1B,OAAO,IAAIgN,gBAAgB3M,KAAM0M,EAAY/M,IAO/CqL,mBAAA,WACE,OAAO,IAAI4B,eAAe5M,sCAe5B,WAAsBiK,GAAtB,MACEV,0BADoBsD,QAAA5C,EADd4C,OAAO,IAuBjB,OAxB+BnD,eAM7BQ,oBAAA,WACE,MAAO,YAAYlK,KAAKiK,MAAM5J,iBAG1B6J,iBAAN,sGACE,OAAIlK,KAAK8M,MAAQ9M,KAAKiK,MAAM5J,WAClBuG,MAAO,KAAMqE,MAAM,KAEvB8B,EAAO/M,KAAKiK,MAAMjK,KAAK8M,MAG3BvL,EADEwL,aAAgB5E,OACT6E,MAASD,GAETA,EAEX/M,KAAK8M,WACGlG,MAAOrF,EAAQ0J,MAAM,aAtBFD,+CA2B7B,WACciC,GADd,MAEE1D,0BADYsD,SAAAI,IAkBhB,OApBsCvD,eAMpCW,oBAAA,WACE,MAAO,iBAGHA,iBAAN,8FACE,IACE,SAAOrK,KAAKiN,UACZ,MAAOhJ,GAIP,MAFAA,EAAEiJ,QACE,mDAAmDjJ,EAAEiJ,QACnDjJ,oBAjB0B+G,yCA2BpC,WAAsBmC,GAAtB,MACE5D,0BADoBsD,WAAAM,EAEpBN,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MAmBxD,OA1BgCvB,eAU9BkD,oBAAA,WACE,OAAU5M,KAAKmN,SAASG,wBAGpBV,iBAAN,yGAME,OADA5M,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAGAR,uBAAd,8FACE,SAAO5M,KAAKmN,SAAS3M,gBAxBOwK,uCAoC9B,WAAsBmC,EAAqCM,GAA3D,MACElE,0BADoBsD,WAAAM,EAAqCN,WAAAY,EAF3DZ,QAAQ,EAINA,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MA+BxD,OAzC8BvB,eAa5B6C,oBAAA,WACE,OAAUvM,KAAKmN,SAASG,sBAGpBf,iBAAN,yGAME,OADAvM,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAGAb,uBAAd,kIAKSvM,KAAKoF,QAAUpF,KAAKyN,YACHzN,KAAKmN,SAAS3M,qBAEpC,OAFMkN,EAAU9F,UAEJqD,QACHyC,IAETC,QAAWD,EAAQ9G,qBAErB,SAAO5G,KAAKmN,SAAS3M,iBAvCKwK,uCA6C5B,WAAsBmC,EAAqCM,GAA3D,MACElE,0BADoBsD,WAAAM,EAAqCN,WAAAY,EAD3DZ,QAAQ,IAeV,OAhB8BnD,eAM5B4C,oBAAA,WACE,OAAUtM,KAAKmN,SAASG,sBAGpBhB,iBAAN,8FACE,OAAItM,KAAKoF,SAAWpF,KAAKyN,aACf7G,MAAO,KAAMqE,MAAM,OAEtBjL,KAAKmN,SAAS3M,gBAdKwK,gDA0B5B,WACcmC,EAAqClB,EACrC2B,gBAAAA,MAFd,MAGErE,0BAFYsD,WAAAM,EAAqCN,YAAAZ,EACrCY,uBAAAe,EAEZf,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MA8BxD,OAvCuCvB,eAYrCyC,oBAAA,WACE,OAAUnM,KAAKmN,SAASG,+BAGpBnB,iBAAN,yGAME,OADAnM,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAGAjB,uBAAd,6HACQ0B,6BACCA,EAAMxN,OAASL,KAAKiM,aACNjM,KAAKmN,SAAS3M,qBACjC,OADMuM,EAAOnF,UACJqD,KACHjL,KAAK4N,sBAAwBC,EAAMxN,OAAS,MACtCuG,MAAOiH,EAAO5C,MAAM,QAEtBrE,MAAO,KAAMqE,MAAM,KAE7B4C,EAAM7L,KAAK+K,EAAKnG,qBAElB,UAAQA,MAAOiH,EAAO5C,MAAM,aArCOD,yCA8CrC,WACcmC,EACA/B,GAFd,MAGE7B,0BAFYsD,WAAAM,EACAN,YAAAzB,EAEZyB,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MAyBxD,OAlCgCvB,eAY9B8B,oBAAA,WACE,OAAUxL,KAAKmN,SAASG,wBAGpB9B,iBAAN,yGAME,OADAxL,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAGA5B,uBAAd,2HAEiB,SAAMxL,KAAKmN,SAAS3M,eACjC,OADMuM,EAAOnF,UACJqD,MAAQjL,KAAKoL,UAAU2B,EAAKnG,UAC5BmG,IAETY,QAAWZ,EAAKnG,wCA/BUoE,sCAqC9B,WACcmC,EACA1B,GAFd,MAGElC,0BAFYsD,WAAAM,EACAN,YAAApB,IAgChB,OAnCgC/B,eAO9BgC,oBAAA,WACE,OAAU1L,KAAKmN,SAASG,qBAGpB5B,iBAAN,uIACe,SAAM1L,KAAKmN,SAAS3M,eACjC,IADMuM,EAAOnF,UACJqD,KACP,UAAQrE,MAAO,KAAMqE,MAAM,IAc7B,IAZM6C,EAAeC,YAAeC,sBAAsBjB,EAAKnG,OAOzDqH,EAASjO,KAAKyL,UAAUsB,EAAKnG,OAC7BsH,EAAgBH,YAAeC,sBAAsBC,OAI3CE,IAAApG,WAAAA,IAAL1I,OACJ0O,YAAeK,eAAe/O,EAAG6O,IACpC7O,EAAEgP,UAGN,UAAQzH,MAAOqH,EAAQhD,MAAM,aAjCDD,oDAuC9B,WACcmC,EACA7B,GAFd,MAGE/B,0BAFYsD,WAAAM,EACAN,UAAAvB,EAHduB,QAAQ,EAKNA,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MAoCxD,OA1C2CvB,eASzC6B,oBAAA,WACE,OAAUvL,KAAKmN,SAASG,8BAOpB/B,iBAAN,yGAME,OADAvL,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAGR7B,uBAAN,4IAGa,gCAAMvL,KAAKmN,SAAS3M,eAA3B,SAAOoH,iBAEP,kBAAK5H,KAAKsL,QAAQgD,aACR1H,MAAO,KAAMqE,MAAM,iDAhCMD,2CA6CzC,WACcmC,EACA1B,GAFd,MAGElC,0BAFYsD,WAAAM,EACAN,YAAApB,IAgChB,OAnCqC/B,eAOnCiC,oBAAA,WACE,OAAU3L,KAAKmN,SAASG,0BAGpB3B,iBAAN,uIACe,SAAM3L,KAAKmN,SAAS3M,eACjC,OADMuM,EAAOnF,UACJqD,SACCrE,MAAO,KAAMqE,MAAM,KAEvB6C,EAAeC,YAAeC,sBAAsBjB,EAAKnG,UAO1C5G,KAAKyL,UAAUsB,EAAKnG,gBAKzC,IALMqH,EAASrG,SACTsG,EAAgBH,YAAeC,sBAAsBC,OAI3CM,IAAAxG,WAAAA,IAAL1I,OACJ0O,YAAeK,eAAe/O,EAAG6O,IACpC7O,EAAEgP,UAGN,UAAQzH,MAAOqH,EAAQhD,MAAM,aAjCID,4CAwDnC,aAAA,MACEzB,0BACAsD,EAAK2B,YAAc,IAAIhF,kBACvBqD,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MAsCxD,OAjDmDvB,eAc3C+E,iBAAN,yGAME,OADAzO,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAiBRqB,uBAAN,4HAIuC,IAA9BzO,KAAKwO,YAAYnO,kBAEXL,KAAK0O,eAAhB,OAAK9G,mBACKhB,MAAO,KAAMqE,MAAM,WAG/B,UAAQrE,MAAO5G,KAAKwO,YAAYG,QAAS1D,MAAM,aA/CAD,0CAmDjD,WACcmC,EACA1B,GAFd,MAGElC,0BAFYsD,WAAAM,EACAN,YAAApB,IAkChB,OArCoC/B,eAOlCmC,oBAAA,WACE,OAAU7L,KAAKmN,SAASG,yBAGpBzB,iBAAN,uIACe,SAAM7L,KAAKmN,SAAS3M,eACjC,IADMuM,EAAOnF,UACJqD,KACP,UAAO,GAeT,IAbM6C,EAAeC,YAAeC,sBAAsBjB,EAAKnG,OAMzDgI,EAAc5O,KAAKyL,UAAUsB,EAAKnG,OAClCsH,EACFH,YAAeC,sBAAsBY,GACzC5O,KAAKwO,YAAYK,QAAQD,OAITE,IAAA/G,WAAAA,IAAL1I,OACJ0O,YAAeK,eAAe/O,EAAG6O,IACpC7O,EAAEgP,UAIN,UAAO,YAnCyBI,+CAyDlC,WACI9D,EACiBH,GAFrB,MAGEjB,0BADmBsD,mBAAArC,EARbqC,WAAuC,KAGvCA,WAA4B,KAOlCA,EAAKkC,cAAgBpE,IAuCzB,OApDwCjB,eAgBtCe,oBAAA,WAEE,MAAUuE,0DAGNvE,iBAAN,8FAEE,OADAzK,KAAKoN,SAAWpN,KAAKiP,cAAcjP,KAAKoN,aACjCpN,KAAKoN,eAGA3C,0BAAd,SAA4B2C,qHAO1B,SAAMA,iBAANxF,SACqB,MAAjB5H,KAAKqM,kBACsBrM,KAAK+O,cAAcvO,eAChD,IADM0O,EAAiBtH,UACJqD,KAEjB,UAAQrE,MAAO,KAAMqE,MAAM,IAE7BjL,KAAKqM,SAAW6C,EAAetI,MACF,MAAzB5G,KAAKwK,mBACPxK,KAAKqM,SAAWrM,KAAKqM,SAAS8C,aAAanP,KAAKwK,oCAGjC,SAAMxK,KAAKqM,SAAS7L,eACvC,OADM4O,EAAaxH,UACJqD,MACbjL,KAAKqM,SAAW,QACTrM,KAAKiP,cAAc7B,QAErBgC,YAlD6BpE,eAsDxC,SAAYH,GACVA,mBACAA,2BACAA,yBAHF,CAAYA,kBAAAA,qBAmCZ,4BAIE,WACuBF,EACAC,gBAAAA,EAAgCC,gBAAgBC,MAFvE,MAGEvB,0BAFqBsD,YAAAlC,EACAkC,eAAAjC,EALfiC,QAAQ,EACRA,iBAA6C,OAsEvD,OAxEiDnD,eAU/CqB,oBAAA,WAEE,MAAO,oDAGKA,sBAAd,SAAwBsE,kDAWtB,SAASC,EAAQC,GACf,OAAIA,aAAqBvE,cAGrBpE,MAFa2I,EAAU/O,OAET+M,KAAK,SAAAtM,GAKjB,OAJAuO,IACIvO,EAAEgK,MACJwE,IAEKxO,EAAE2F,QAEXD,SAAS,IAGHC,MAAO,KAAMD,SAAS,wEArBlC,SAAM0I,UAyBY,OAzBlBzH,SAII4H,EAAe,EACfC,EAAgB,KAoBI9H,mBAAmB3H,KAAK2K,UAAW2E,WAE3D,GAFMrB,EAAYrG,SAEd4H,IAAiBC,EAEnB,UAAQ7I,MAAO,KAAMqE,MAAM,IAE7B,GAAIwE,EAAgB,EAClB,OAAQzP,KAAK4K,cACX,KAAKC,gBAAgBC,KACnB,MAAM,IAAIrE,MACN,qEACyBzG,KAAKoF,WACpC,KAAKyF,gBAAgB6E,SACnB,UAAQ9I,MAAO,KAAMqE,MAAM,IAC7B,KAAKJ,gBAAgB8E,SAOzB,OADA3P,KAAKoF,YACGwB,MAAOqH,EAAQhD,MAAM,UAGzBF,iBAAN,qHAEU,OADR/K,KAAK4P,eAAiB5P,KAAK6P,UAAU7P,KAAK4P,mBAC5B5P,KAAK4P,uBAAnB,SAAQhI,mBAtEqCoD,2CAuF/C,WACcmC,EAAqCX,GADnD,MAEEjD,0BADYsD,WAAAM,EAAqCN,aAAAL,EAEjDK,EAAKiD,OAAS,IAAInH,WAAuC6D,KAyB7D,OA/ByC9C,eASvC+C,oBAAA,WACE,OAAUzM,KAAKmN,SAASG,0BAOhBb,mBAAV,WACE,MAAQzM,KAAK8P,OAAO/G,UAAU,CAC5B,IAAMtH,EAAIzB,KAAKmN,SAAS3M,OACxBR,KAAK8P,OAAO9N,KAAKP,KAIrBgL,iBAAA,WAKE,OAJAzM,KAAK+P,SAIE/P,KAAK8P,OAAOnB,YA7BkB3D,0CAiDvC,WACcmC,EAAqCT,EAC/C/M,GAFJ,MAGE4J,YAAM4D,EAAUT,gBAFJG,WAAAM,EAAqCN,aAAAH,EAH3CG,qBAAoB,EAM1BA,EAAKhH,OAASmK,aAAgBrQ,GAAQ2I,KAAQ2H,MAAM9P,YACpD0M,EAAKO,SAAWpF,QAAQqF,SAASzG,MAAO,KAAMqE,MAAM,MAqCxD,OApDwCvB,eAkBhCiD,iBAAN,yGAME,OADA3M,KAAKoN,SAAWpN,KAAKoN,SAASG,KAAK,WAAM,OAAAV,EAAKW,kBACvCxN,KAAKoN,eAGNT,sBAAR,SAAkBtK,GAChB,OAAOD,KAAKK,MAAMzC,KAAK6F,SAAWxD,IAG1BsK,wBAAV,WACE,OAAO3M,KAAKkQ,UAAUlQ,KAAK8P,OAAOzP,WAG9BsM,uBAAN,6HAEO3M,KAAKmQ,mBACRnQ,KAAK+P,iCAEC/P,KAAK8P,OAAO3G,iBACZiH,EAAcpQ,KAAKqQ,iBACJrQ,KAAK8P,OAAOQ,cAAcF,YAC/C,OADM7O,EAASqG,UACJqD,MACTjL,KAAKmQ,mBAAoB,UAEzBnQ,KAAK+P,YACExO,WAGX,UAAQqF,MAAO,KAAMqE,MAAM,aAlDSwB,qCC1jCxC,aAWWzM,UAAe,KAsc1B,OAxYEuQ,kBAAA,SAAMtE,EAAmBC,GAAzB,wBAAyBA,MACvB,IAAMsE,EAAOxQ,KAkBb,OAjBAsI,KAAQmI,OACJxE,EAAY,EAAG,WAAM,MAAA,oDACrBA,IAeGyE,sBAAsB,kHACnB,SAAMF,EAAKnE,mBAAnB,SAAQzE,SACH+I,iBAAiB1E,EAAWC,EAAgB0E,wBAf/C5Q,KAAK6Q,OAASC,EAAAA,GAAyB,MAAb9Q,KAAK6Q,KAG1B7Q,KAAK6Q,KACH3E,EAGF9J,KAAK2O,KAAK/Q,KAAK6Q,KAAO5E,GAItB7J,KAAKK,MAAMzC,KAAK6Q,KAAO5E,KAsBlCsE,wBAAA,SAAYS,GAAZ,WACQR,EAAOxQ,KAeb,OAAO0Q,sBACH,0HACK,SAAMF,EAAKnE,mBAAwB,OAApCnE,GAAAN,EAACqJ,UAAuBC,eAAkBF,EAAQ3E,mBAAlD,SAAAnE,WAAoC+I,kBAfxCjR,KAAK6Q,OAASC,EAAAA,GAAYE,EAAQH,OAASC,EAAAA,EAGtCA,EAAAA,EACe,MAAb9Q,KAAK6Q,MAAgC,MAAhBG,EAAQH,KAG/B7Q,KAAK6Q,KAAOG,EAAQH,KAIpB,OAuBXN,mBAAA,SAAOnF,GAAP,WACQoF,EAAOxQ,KAUb,OAAO0Q,sBAAsB,kHACnB,SAAMF,EAAKnE,mBAAnB,SAAQzE,SAAuBuJ,OAAO,SAAAlQ,GAAK,OAAAmQ,KAAQ,WAAM,OAAAhG,EAAUnK,cATjEjB,KAAK6Q,OAASC,EAAAA,EAETA,EAAAA,EAIA,OAsBLP,yBAAN,SAAmBnR,6GACT,SAAMY,KAAKqM,mBAAnB,SAAQzE,SAAuByJ,aAAajS,UAIxCmR,oBAAN,SAAcnR,sFAIZ,OAHAkS,gBACI,qGAEGtR,KAAKqR,aAAajS,SAiB3BmR,gBAAA,SAA2B9E,GAA3B,WACQ+E,EAAOxQ,KACb,OAAO0Q,sBAAsB,kHACnB,SAAMF,EAAKnE,mBAAnB,SAAQzE,SAAuBH,IAAI,SAAAxG,GAAK,OAAAmQ,KAAQ,WAAM,OAAA3F,EAAUxK,cAC/DjB,KAAK6Q,OAwBVN,qBAAA,SAAgC9E,GAAhC,WAEQ+E,EAAOxQ,KACb,OAAO0Q,sBAAsB,kHACnB,SAAMF,EAAKnE,mBAAnB,SAAQzE,SAAuB2J,SAAS9F,UACvCzL,KAAK6Q,OAWVN,qBAAA,SAAS/D,GAAT,WACE,GAAkB,MAAdA,EACF,MAAM,IAAI/D,WACN,6DAGN,IAAM+H,EAAOxQ,KACb,OAAO0Q,sBACH,kHAAa,SAAMF,EAAKnE,mBAAZ,SAACzE,SAAuBuD,SAASqB,UAAaxM,KAAK6Q,OAoBrEN,mBAAA,SAAOnL,GAAP,WACQoL,EAAOxQ,KAkBb,OAAO0Q,sBAAsB,sGAG3B,SAAOpG,yBAFkBH,qBACrB,wHAAqB,eAAMqG,EAAKnE,mBAApB,UAAEzE,QAAOM,SAAuBN,QAAM,aACL4J,KAAKpM,UAnBvC,MAAbpF,KAAK6Q,MAAgBzL,EAAQ,EAIxBpF,KAAK6Q,KAAOzL,EACA,IAAVA,EAEF,EACe,MAAbpF,KAAK6Q,YAA2BzH,IAAVhE,GAAuBA,EAAQ,GAGvD0L,EAAAA,EAGA,OAyBXP,iBAAA,SAAKnL,GAAL,WACQoL,EAAOxQ,KAiBb,OAAO0Q,sBACH,kHAAa,SAAMF,EAAKnE,mBAAZ,SAACzE,SAAuB6J,KAAKrM,UAhB5B,MAAbpF,KAAK6Q,MAAgBzL,GAAS,GAAKpF,KAAK6Q,MAAQzL,EAI3CpF,KAAK6Q,KAAOzL,EAEJ,MAAbpF,KAAK6Q,OACJ7Q,KAAK6Q,KAAOzL,QAAmBgE,IAAVhE,GAAuBA,EAAQ,GAGhD,EAGA,OA8BXmL,oBAAA,SAAQ/D,EAAoB7M,EAAe+R,GAA3C,WAEE,gBAFyCA,MAEvB,MAAdlF,GAAsBA,EAAa,EACrC,MAAiB,MAAbxM,KAAK6Q,KACD,IAAIpI,WACN,4DAEE,IAAIA,WACN,+MAGmCzI,KAAK6Q,mBAGhD,IAAML,EAAOxQ,KACP6F,EAASmK,aAAgBrQ,GAAQ2I,KAAQ2H,MAAM9P,YACrD,OAAOuQ,sBAAsB,wHAKnB,OAJJiB,EAAQ9L,EAAOlF,QACf+Q,IACFC,GAAS9L,EAAOlF,YAEJ6P,EAAKnE,mBAAnB,SAAQzE,SAAuBgK,QAAQpF,EAAYmF,EAAMxR,mBACxDH,KAAK6Q,OAmBVN,iBAAA,SAAKnL,GAAL,WACQoL,EAAOxQ,KAcb,OAAO0Q,sBACH,kHAAa,SAAMF,EAAKnE,mBAAZ,SAACzE,SAAuB4J,KAAKpM,UAb5B,MAAbpF,KAAK6Q,MAAgB7Q,KAAK6Q,KAAOzL,EAG5BA,EACe,MAAbpF,KAAK6Q,MAAgB7Q,KAAK6Q,MAAQzL,EAGpCpF,KAAK6Q,KAGL,OAqBLN,oBAAN,qHACE,GAAIvQ,KAAK6Q,OAASC,EAAAA,EAChB,MAAM,IAAIrK,MAAM,kDAEV,SAAMzG,KAAKqM,mBAAnB,SAAQzE,SAAuBiK,iBAc3BtB,2BAAN,qHACE,GAAIvQ,KAAK6Q,OAASC,EAAAA,EAChB,MAAM,IAAIrK,MAAM,kDAEV,SAAMzG,KAAKqM,mBAAnB,SAAQzE,SAAuBkK,wBAvHjBvB,kBAAkB,SAuIpC,SAAgBG,sBACZqB,EACAlB,GACF,oBADEA,QACK,gBAAI,aAAA,qDACThE,OAAOgE,IAST,OAVyBnH,eAOjBsI,qBAAN,8FACE,SAAOD,aARcxB,UA+B3B,SAAgB0B,MAA6BhI,GAA7C,WACE,OAAOyG,sBACH,2FAAY,SAAA1G,kBAAkBC,SAAQA,EAAM5J,QA2ClD,SAAgB6R,IAA2BC,GAA3C,IAMMtB,SAHJ,IAAKhK,WAAWsL,GACd,MAAM,IAAI1L,MAAM,qDAGlB,GAAIM,MAAMC,QAAQmL,GAChB,IAAK,IAAI/R,EAAI,EAAGA,EAAI+R,EAAS9R,OAAQD,IACnCyQ,EAAe,MAARA,EAAgBsB,EAAS/R,GAAkByQ,KAC5BzO,KAAKgQ,IAAIvB,EAAOsB,EAAS/R,GAAkByQ,WAE9D,GAAIsB,aAAoBE,OAC7B,IAAK,IAAMC,KAAMH,EACftB,EAAe,MAARA,EAAgBsB,EAASG,GAAmBzB,KAC7BzO,KAAKgQ,IAAIvB,EAAOsB,EAASG,GAAmBzB,MAGtE,OAAOH,sBAAyB,kHACd,SAAM/I,mBAAmBwK,EAAU,SAAAzQ,GACjD,GAAIA,aAAa6O,QACf,OAAQ3J,MAAOlF,EAAE2K,WAAY1F,SAAS,GACjC,GAAIE,WAAWnF,GACpB,OAAQkF,MAAO,KAAMD,SAAS,GAE9B,MAAM,IAAIF,MACN,uFAIR,SAAOiE,mBAXS9C,SAWsBiD,gBAAgB6E,iBACrDmB,GAWL,SAASD,gBAAgB2B,GACvB,OAAa,OAATA,EACK,KAMLnK,aAFemK,EAAK,KAKd3L,MADM4L,YAAYD,GACX5L,SAAS,IAIlBC,MAAO,KAAMD,SAAS,GAOhC,SAAS6L,YAA+CC,GAEtD,GAAsB,IAAlBA,EAAOpS,OAET,MAAM,IAAIoG,MAAM,wCAGlB,OAAIgM,EAAO,aAActK,OAEhBuK,MAASD,GAGTE,OAAUF,GCrpBrB,gCAME,WAA+BvM,GAA/B,MACEqD,0BAD6BsD,QAAA3G,IAUjC,OAhBqCwD,eAU7BkJ,qBAAN,6HACwB,SAAM5S,KAAKkG,MAAMmG,mBAGvC,OAHMwG,EAAgBjL,SAChBkL,EAAeD,EAAcE,gBACdD,EAAaE,MAAM,gBAbPzC,SCF/B0C,WAAa,IACbC,UAAYC,OAAO,OACnBC,YAAcD,OAAO,SACrBE,YAAcF,OAAO,SACrBG,wBAA0BH,OAAO,mBACjCI,4BAA8BJ,OAAO,uCA2IzC,WAA+BjN,EAAmBsN,GAAlD,MACEjK,0BAD6BsD,QAAA3G,EA3HvB2G,aAAY,EACZA,kBAA4B,KAC5BA,wBAAuB,EACvBA,gBAA+C,KAC/CA,yBAAwB,EACxBA,YAAY,IAwHlBA,EAAK2D,KAAO,IAAIoC,gBAAgB1M,GAC3BsN,IACHA,MAEF3G,EAAK4G,WAAoC,IAAxBD,EAAUC,UAC3B5G,EAAK6G,gBAAkBF,EAAUG,YACjC9G,EAAK+G,cAAgBJ,EAAUI,cAC/B/G,EAAKgH,sBAAwBL,EAAUK,sBACvChH,EAAKiH,UAAYN,EAAUM,UAAYN,EAAUM,UAAY,MA6LjE,OApUgCpK,eAkBxBqK,wBAAN,4HACO/T,KAAKgU,8BACFhU,KAAKiU,yBAAXrM,0BAEF,SAAO5H,KAAK6T,sBAAwBxB,OAAOvK,KAAK9H,KAAK4T,eACjB5T,KAAK0T,uBAW7BK,2BAAd,4IAC8B,SAAM/T,KAAKkU,8BACvC,GADMC,EAAsBjM,UACvBlI,KAAK0T,kBAAoBS,EAE5B,MAAM,IAAI1N,MACN,6DA0BN,GAzBWzG,KAAK0T,iBAAmBS,GAEjCC,KAAK3D,OACD0D,EAAoB9T,SAAWL,KAAK0T,gBAAgBrT,OACpD,WAAM,MAAA,uCACFwM,EAAK6G,gBAAgBrT,OAAOF,WAC5B,kEACWgU,EAAoB9T,OAAOF,WAAa,OAExDH,KAAK0T,kBACR1T,KAAK0T,gBAAkBS,GAGnBE,EAAkCrU,KAAK0T,gBAAgBY,OACzD,SAACC,EAAmCC,GAElC,OADAD,EAASC,GAASD,EAASC,GAAQ,GAAM,EAClCD,OAGPE,EACFpC,OAAOvK,KAAKuM,GAAQlD,OAAO,SAACqD,GAAS,OAACH,EAAOG,GAAQ,IACzDJ,KAAK3D,OACyB,IAA1BgE,EAAepU,OACf,WAAM,MAAA,iCAAmCoU,EAAetU,aAExDH,KAAK4T,cACP,QAAkBhM,EAAAyK,OAAOvK,KAAK9H,KAAK4T,eAAjB7L,WAAAA,IAEhB,GAFStE,QAEM,IADDzD,KAAK0T,gBAAgBgB,QAAQjR,GAEzC,MAAM,IAAIgD,MACN,YAAchD,EACd,uEACYzD,KAAK0T,gBAAgBvT,WAAa,aAIxDH,KAAKgU,sBAAuB,YAGhBD,gCAAd,kIACM/T,KAAKyT,aACYzT,KAAKwQ,KAAKnE,yBACR,SADRzE,SACmBpH,eAChC,IADMmU,EAAe/M,UACJqD,KACf,MAAM,IAAIxE,MAAM,sCAGlB,SAD0BkO,EAAa/N,MACtBoM,MAAMhT,KAAK8T,mBAE5B,SAAO,YAiDLC,qBAAN,yIACO/T,KAAKgU,8BACFhU,KAAKiU,yBAAXrM,0BAEU,SAAM5H,KAAKwQ,KAAKnE,mBAM5B,OANIuI,EAAQhN,SACR5H,KAAKyT,YAGPmB,EAAQA,EAAMnD,KAAK,OAEdmD,EAAMnN,IAAI,SAAAxG,GAAK,OAAA4L,EAAKgI,gBAAgB5T,YAG7C8S,4BAAA,SAAgBe,GAKd,IAJA,IAAM7L,EAASjJ,KAAK+U,SAASD,GACvBE,KACAC,KAEG7U,EAAI,EAAGA,EAAIJ,KAAK0T,gBAAgBrT,OAAQD,IAAK,CACpD,IAAMqD,EAAMzD,KAAK0T,gBAAgBtT,GAC3B8U,EAASlV,KAAK4T,cAAgB5T,KAAK4T,cAAcnQ,GAAO,KAC9D,IAAIzD,KAAK6T,uBAA0BqB,EAAnC,CAIE,IAAMtO,EAAQqC,EAAO7I,GACjB+U,EAAc,KAClB,GAAc,KAAVvO,EAGF,GAAIsO,QAA6B9L,IAAnB8L,EAAOE,QACnBD,EAAcD,EAAOE,YAChB,CAAA,GAAIF,IAAWA,EAAOG,UAAYH,EAAOI,SAC9C,MAAM,IAAI7O,MACN,mBAAmBhD,6BAA8BqR,GAErDK,OAAc/L,MAEX,CAEL,IAAMmM,EAAaC,OAAO5O,GAC1B,GAAI6O,MAAMF,GAINJ,EADED,GAA2B,SAAjBA,EAAOQ,MACL1V,KAAK2V,WAAW/O,GAGhBA,OAEX,GAAKsO,GAAWA,EAAOQ,MAO5B,OAAQR,EAAOQ,OACb,IAAK,UACHP,EAAcI,EACd,MACF,IAAK,QACHJ,EAAc/S,KAAKK,MAAM8S,GACzB,MACF,IAAK,OACHJ,EAAcnV,KAAK2V,WAAW/O,GAC9B,MACF,QACEuO,EAAcI,OAflBJ,EAAcI,EAoBjBL,GAAUA,EAAOI,QAAWL,EAAOxR,GAAO0R,EACdH,EAASvR,GAAO0R,GAKjD,OAAmC,IAA/B9C,OAAOvK,KAAKmN,GAAQ5U,OACf2U,GAGCY,GAAIZ,EAAUa,GAAIZ,IAItBlB,uBAAR,SAAmBnN,GACjB,MAAc,MAAVA,GAAyC,SAAxBA,EAAMkP,cAClB,EAEA,GAKH/B,qBAAR,SAAiBe,GAMf,IALA,IAAMvT,KACFwU,EAAa,EACXC,EAAalB,EAAKzU,OACpB4V,EAAe7C,YAEVhT,EAAI,EAAGA,EAAI4V,EAAY5V,IAC9B,OAAQ6V,GAEN,KAAK/C,UACH,OAAQ4B,EAAKoB,OAAO9V,IAElB,KAAK6S,WACH8C,EAAa3V,EAAI,EACjB6V,EAAe5C,YACf,MAEF,KAAKrT,KAAK8T,UACRvS,EAAOS,KAAK,IACZiU,EAAe/C,UACf6C,EAAa3V,EAAI,EACjB,MAEF,QACE6V,EAAe7C,YACf2C,EAAa3V,EAGjB,MAEF,KAAKgT,YACH,OAAQ0B,EAAKoB,OAAO9V,IAElB,KAAKJ,KAAK8T,UACRvS,EAAOS,KAAK8S,EAAKqB,UAAUJ,EAAY3V,IACvC6V,EAAe/C,UACf6C,EAAa3V,EAAI,EAIrB,MAEF,KAAKiT,YACH,OAAQyB,EAAKoB,OAAO9V,IAElB,KAAK6S,WACHgD,EAAe3C,wBAInB,MAEF,KAAKA,wBACH,OAAQwB,EAAKoB,OAAO9V,IAElB,KAAKJ,KAAK8T,UACRvS,EAAOS,KAAK8S,EAAKqB,UAAUJ,EAAY3V,EAAI,IAC3C6V,EAAe/C,UACf6C,EAAa3V,EAAI,EACjB,MAEF,KAAK6S,WACHgD,EAAe5C,YACf,MAEF,QACE4C,EAAe1C,4BAGnB,MACF,KAAKA,4BACH,OAAQuB,EAAKoB,OAAO9V,IAElB,KAAK6S,WACHgD,EAAe5C,aAczB,OALI4C,IAAiB3C,wBACnB/R,EAAOS,KAAK8S,EAAKqB,UAAUJ,EAAYC,EAAa,IAEpDzU,EAAOS,KAAK8S,EAAKqB,UAAUJ,IAEtBxU,MAlUqBgP,+BCPhC,OAVA,2CCPA,4DAsBA,OAtB6C7G,eAmB3C0M,kBAAA,SAAMC,GACJ,OAAO,IAAIC,cAActW,KAAMqW,OApBUrL,wCAsC3C,WAAsBmC,EAAgCkJ,GAAtD,MACE9M,0BADoBsD,WAAAM,EAEpBN,EAAKnN,KAAO,IAAI6W,kBAAkBpJ,EAAUkJ,KAUhD,OAf4B3M,eAQ1B4M,oBAAA,WACE,OAAOtW,KAAKN,KAAK4N,WAGbgJ,iBAAN,8FACE,SAAOtW,KAAKN,KAAKc,gBAbO4V,8CAqB1B,WACcjJ,EAA0CkJ,GADxD,MAEE9M,0BADYsD,WAAAM,EAA0CN,YAAAwJ,EAHxDxJ,YAAY,KAqCd,OAvCgCnD,eAS9B6M,oBAAA,WACE,OAAUvW,KAAKmN,SAASG,wBAAuBtN,KAAKqW,gBAGhDE,iBAAN,mIACsB,SAAMvW,KAAKmN,SAAS3M,eACxC,IADMgW,EAActO,UACJ+C,KACd,MAAuB,KAAnBjL,KAAKyW,cACA,IAKTzW,KAAKwO,YAAYxM,KAAKhC,KAAKyW,WAC3BzW,KAAKyW,UAAY,OACV,IAQT,KANM7B,EAAQ4B,EAAY5P,MAAMoM,MAAMhT,KAAKqW,YAKrC,GAAKrW,KAAKyW,UAAY7B,EAAM,OACfhN,EAAAgN,EAAMhT,MAAM,GAAI,GAAhBmG,WAAAA,IAAR+M,OACT9U,KAAKwO,YAAYxM,KAAK8S,GAIxB,OAFA9U,KAAKyW,UAAY7B,EAAMA,EAAMvU,OAAS,OAE/B,YArCqBoO,iDClDhC,4DAaA,OAbgD/E,eAU9CgN,uBAAA,WACE,OAAO,IAAIC,aAAa3W,UAXoBgL,uCA6B9C,WAAsBmC,GAAtB,MACE5D,0BADoBsD,WAAAM,EAEpBN,EAAKnN,KAAO,IAAIkX,iBAAiBzJ,KAUrC,OAf2BzD,eAQzBiN,oBAAA,WACE,OAAO3W,KAAKN,KAAK4N,WAGbqJ,iBAAN,8FACE,SAAO3W,KAAKN,KAAKc,gBAbM4V,6CA4CzB,WAA+BjJ,GAA/B,MACE5D,mBACA,GAF6BsD,WAAAM,EAEzB0J,IAAInQ,IAAI,cACVmG,EAAKiK,QAAU,IAAIC,YAAY,aAC1B,CAEG,IAAAC,0CACRnK,EAAKiK,QAAU,IAAIE,EAAc,iBAyBvC,OArC+BtN,eAe7BkN,oBAAA,WACE,OAAU5W,KAAKmN,SAASG,sBAGpBsJ,iBAAN,+HACsB,SAAM5W,KAAKmN,SAAS3M,eAExC,OAFMgW,EAAc5O,UAEJqD,SACL,IAETgM,EAAQT,EAAY5P,MAKpBsQ,EADEL,IAAInQ,IAAI,cACH1G,KAAK8W,QAAQK,OAAOF,GAAQ/L,QAAO,IAEnClL,KAAK8W,QAAQM,MAAMC,OAAOxP,KAAKoP,EAAMnH,SAE9C9P,KAAKwO,YAAYxM,KAAKkV,OACf,aAnCoBzI,iDC9C7B,WACc6I,EACA/T,gBAAAA,MAFd,MAGEgG,0BAFYsD,OAAAyK,EACAzK,UAAAtJ,EAEZ6Q,KAAK3D,OACA6G,aAAgBhT,cACZuS,IAAInQ,IAAI,gBACH4Q,aAAgBC,MAAQD,aAAgBE,MAElD,WAAM,MAAA,yEAEV3K,EAAK4K,OAASlU,EAAQkU,QAAU,EAEhC5K,EAAK6K,UAAYnU,EAAQmU,WAAa,UAuD1C,OAxEuChO,eAoBrCiO,oBAAA,WACE,MAAO,cAAc3X,KAAKsX,MAGtBK,iBAAN,oIACE,OAAI3X,KAAKyX,SAAYzX,KAAKsX,gBAAgBhT,WAClBtE,KAAKsX,KAAKM,WACV5X,KAAKsX,KAAKzG,UACxBjK,MAAO,KAAMqE,MAAM,KAEvBgM,EAAQ,IAAIjP,QAAoB,SAACqF,EAASwK,GAC9C,IAAMhP,EAAMgE,EAAK4K,OAAS5K,EAAK6K,UAC/B,GAAI7K,EAAKyK,gBAAgBhT,WAGvB+I,EAAQ,IAAI/I,WAAWuI,EAAKyK,KAAK1V,MAAMiL,EAAK4K,OAAQ5O,SAC/C,CAKL,IAAMiP,EAAa,IAAIC,WACvBD,EAAWE,OAAS,SAACC,GACnB,IAAI/X,EAAsC4X,EAAWvW,OAOrD,GAHIrB,aAAgBgY,cAClBhY,EAAO,IAAIoE,WAAWpE,MAElBA,aAAgBoE,YACpB,OAAOuT,EAAO,IAAIM,UAAU,sCAE9B9K,EAAQnN,IAEV4X,EAAWM,QAAU,SAACH,GACpB,OAAOJ,EAAO,IAAIpR,MAAM,aAE1BqR,EAAWO,QAAU,SAACJ,GACpB,OAAOJ,EAAO,IAAIpR,MAAMwR,EAAMK,QAIhC,IAAM1W,EAAQiL,EAAKyK,KAAK1V,MAAMiL,EAAK4K,OAAQ5O,GAG3CiP,EAAWS,kBAAkB3W,GAE/BiL,EAAK4K,OAAS5O,YAEMoO,WAAtB,UAAQrP,QAAQM,SAAcN,QAAM,eAtED8O,mBCTvC,SAAsB8B,iBAClBC,EAAkBlV,uBAAAA,4HAEhBsT,IAAInQ,IAAI,iBACOgS,MAAMD,wBAAvBE,EAAW/Q,UACEgR,MACQD,EAASE,qBAC5B,OADMA,EAAOjR,YACN,IAAI+P,kBAAkBkB,EAAMtV,WAEnC,MAAM,IAAIkD,MAAMkS,EAASG,sCAO3B,GADMC,EAAYjT,QAAQ,cACP,iBAAR2S,EACT,MAAM,IAAIhS,MACN,2FAGK,SAAMsS,EAAUN,kBAA3BE,EAAW/Q,UACEgR,MACaD,EAAS7I,uBACjC,OADMkJ,EAAYpR,YACX,IAAI+P,kBAAkBqB,EAAWzV,WAExC,MAAM,IAAIkD,MAAMkS,EAASG,kCCjC/B,SAAgBG,YAAYC,GAC1B,MAA0B,iBAAXA,GAAgD,YAAxBA,EAAOC,OAAO,EAAG,GCO1D,+BASE,WACcjT,EACS3C,gBAAAA,MAFvB,MAGEgG,0BAFYsD,QAAA3G,EACS2G,UAAAtJ,IAczB,OAzBoCmG,eAe5B0P,qBAAN,oGAQE,OAPIH,YAAYjZ,KAAKkG,QAAU2Q,IAAInQ,IAAI,aAE/B2S,EAAKvT,QAAQ,MACnB9F,KAAKkG,MAAQmT,EAAGC,aAActZ,KAAKkG,MAAiBiT,OAAO,QAItD,IAAIxB,kBAAkB3X,KAAKkG,MAAsBlG,KAAKuD,kBAvB7BgW,sCCOlC,WACuBd,EACAe,gBAAAA,MAFvB,MAGEjQ,0BAFqBsD,MAAA4L,EACA5L,cAAA2M,IAgBzB,OA1BmC9P,eAkB3B+P,qBAAN,8FACE,OAAIR,YAAYjZ,KAAKyY,QACZ,IAAKW,eAAepZ,KAAKyY,IAAezY,KAAKwZ,aAC/CnN,eAEEmM,iBAAiBxY,KAAKyY,IAAKzY,KAAKwZ,sBAvBVD,YC2EnC,SAAgBG,IACZR,EAAqB1F,GACvB,oBADuBA,MAChB,IAAIO,WAAW,IAAI0F,cAAcP,GAAS1F,GA2BnD,SAAgBpJ,KACZhL,GADJ,WAEQua,EAAOxP,qBAAqB/K,GAClC,OAAOsR,sBAAsB,2FAAY,SAAAiJ,SAgE3C,SAAgBC,UACZA,GADJ,WAEE,OAAOlJ,sBAAsB,wHACf,SAAMkJ,YAClB,OADMC,EAAMjS,YACLuC,qBAAqB,WAAM,OAAA0P,EAAIrZ,iBCzM1C,IAAMsZ,QAAU"}